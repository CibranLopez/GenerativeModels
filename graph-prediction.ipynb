{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy    as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from pymatgen.core        import Structure\n",
    "from scipy.optimize       import minimize\n",
    "from torch_geometric.data import Batch, Data\n",
    "from libraries.model      import nGCNN, eGCNN, denoise, get_random_graph, add_features_to_graph\n",
    "from libraries.dataset    import revert_standardize_dataset\n",
    "from libraries.graph      import POSCAR_graph_encoding, graph_POSCAR_encoding, find_closest_key, composition_concentration_from_keys\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a88fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From random noise, we generate completely new materials\n",
    "# A target property can be seeked with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder in which all data will be stored\n",
    "is_molecule     = False\n",
    "target_folder   = 'models/Loaded_MP_bandgap-sphere-images/GM_v0'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "\n",
    "# Number of graphs to predict\n",
    "N_predictions = 10\n",
    "\n",
    "# Define target to be generated\n",
    "target_tensor = torch.tensor(1, dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9310c",
   "metadata": {},
   "source": [
    "# Load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24741239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in JSON format to a dictionary\n",
    "with open(f'{target_folder}/model_parameters.json', 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "model_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        model_parameters[key] = torch.tensor(value, device=device)\n",
    "    except:\n",
    "        model_parameters[key] = value\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = model_parameters['n_t_steps']\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = model_parameters['noise_contribution']\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = model_parameters['dropout_node']\n",
    "dropout_edge = model_parameters['dropout_edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23509d8f-998a-4b4e-9f6a-7cd53014900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters['sigma'] = torch.tensor(0.4, dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412622/933877349.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(dataset_name_std)\n"
     ]
    }
   ],
   "source": [
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "labels_name                 = f'{target_folder}/standardized_labels.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset\n",
    "dataset = torch.load(dataset_name_std)\n",
    "\n",
    "# Read the file in JSON format to a dictionary\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "dataset_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        dataset_parameters[key] = torch.tensor(value, device=device)\n",
    "    except:\n",
    "        dataset_parameters[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f409c-3092-41ee-a6ad-0485d67658b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize target_tensor accordingly\n",
    "target_tensor = (target_tensor - dataset_parameters['target_mean']) * dataset_parameters['scale'] / dataset_parameters['target_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066eb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the number of nodes\n",
    "total_nodes = torch.tensor([data.num_nodes for data in dataset])\n",
    "mean_nodes  = torch.mean(total_nodes.float()).item()\n",
    "std_nodes   = torch.std(total_nodes.float()).item()\n",
    "\n",
    "mean_nodes, std_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff10529-1eb5-4d9a-8b5f-c5e5fc08715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = dataset[0].num_node_features\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(dataset[0].y)\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features+1, dropout_node)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features+1, dropout_edge)\n",
    "\n",
    "node_model.load_state_dict(torch.load(node_model_name, map_location=torch.device(device), weights_only=False))\n",
    "edge_model.load_state_dict(torch.load(edge_model_name, map_location=torch.device(device), weights_only=False))\n",
    "node_model.eval()\n",
    "edge_model.eval()\n",
    "\n",
    "# Allow data parallelization among multi-GPU\n",
    "node_model= nn.DataParallel(node_model)\n",
    "edge_model= nn.DataParallel(edge_model)\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a0f21",
   "metadata": {},
   "source": [
    "# Generating new cystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7b29c-a264-40e8-9908-92972731a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constant target tensor once\n",
    "features_tensor = torch.tensor([target_tensor, 0], device=device)\n",
    "\n",
    "# Predicting loop\n",
    "diffused_dataset = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(N_predictions):\n",
    "        # Get a positive random number of nodes using absolute value\n",
    "        n_nodes = int(np.abs(np.random.normal(mean_nodes, std_nodes)))\n",
    "        \n",
    "        # Get random graph, acting as diffused\n",
    "        diffused_graph = get_random_graph(n_nodes, n_node_features)\n",
    "\n",
    "        # Make room for n_graph_features and t_steps in the dataset\n",
    "        diffused_graph = add_features_to_graph(diffused_graph, features_tensor)\n",
    "        \n",
    "        diffused_dataset.append(diffused_graph)\n",
    "            \n",
    "    # Generate batch object and move data to device\n",
    "    diff_batch = Batch.from_data_list(diffused_dataset).to(device)\n",
    "    \n",
    "    # Denoise batch\n",
    "    predicted_dataset = denoise(diff_batch, n_t_steps, node_model, edge_model, alpha_decay=alpha_decay,\n",
    "                                sigma=model_parameters['sigma'], n_features=n_node_features)\n",
    "\n",
    "# From batch object to list\n",
    "predicted_dataset = predicted_dataset.to_data_list()\n",
    "\n",
    "# Remove graph features\n",
    "for graph in predicted_dataset:\n",
    "    graph.x = graph.x[:, :n_node_features]\n",
    "\n",
    "# Revert standardization\n",
    "denoised_graphs = revert_standardize_dataset(predicted_dataset, dataset_parameters)\n",
    "denoised_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c860be9e-aee3-4b3c-af21-a443d10f3a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5564, 4.5564, 4.5564,  ..., 4.5564, 4.5564, 4.5564], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoised_graphs[1].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3388072b-4983-4711-900b-5a0cec29229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5564, 4.5564, 4.5564,  ..., 4.5564, 4.5564, 4.5564], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoised_graphs[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f92263-cb5e-4f16-8f88-c6457ffa3dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564, 4.5564,\n",
       "        4.5564, 4.5564, 4.5564], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoised_graphs[3].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af401f1-89eb-43b9-8f42-2996b590e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = denoised_graphs[0]\n",
    "\n",
    "nodes = temp.x\n",
    "edges = temp.edge_index.detach().cpu().numpy().T\n",
    "weights = temp.edge_attr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110c5c9-d0cf-4056-ada5-5c2206034939",
   "metadata": {},
   "source": [
    "## Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f199e7-5d19-4c52-81f7-25d8dec93263",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_molecule:\n",
    "    # Initial guess for the positions\n",
    "    solution = np.random.rand(len(nodes) * 3)*10  # Initialize all points at origin, 1D array\n",
    "    #solution = coordinates.reshape(-1, 1).ravel()\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            distance = np.linalg.norm(p2 - p1)\n",
    "            errors += np.power(distance - weight, 2)\n",
    "        #print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "                \n",
    "                # Compute error\n",
    "                distance = np.linalg.norm(p2 - p1)\n",
    "\n",
    "                # Append to trial errors for different atom images\n",
    "                trial_error = np.power(distance - weight, 2)\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += trial_error\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66e868-1cf3-483b-b86e-be0530df0704",
   "metadata": {},
   "source": [
    "## Crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a4e28f-83d9-4971-a5fc-3200d0ec4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_molecule:\n",
    "    # Initial guess for the lattice parameters\n",
    "    lattice_vectors = np.array([[10, 0,   0],\n",
    "                                [0,   10, 0],\n",
    "                                [0,   0,   10]])\n",
    "    \n",
    "    # Initial guess for the positions\n",
    "    initial_positions = np.random.rand(len(nodes) * 3)  # Initialize all points at origin, 1D array\n",
    "    #initial_positions = coordinates.reshape(-1, 1).ravel()\n",
    "    solution = np.concatenate([lattice_vectors.ravel(), initial_positions])\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        solution_attempt = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        \n",
    "        lattice_vectors = solution_attempt[:3]\n",
    "        positions       = solution_attempt[3:]\n",
    "        \n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            \n",
    "            trial_errors = [] \n",
    "            for i in [-1, 0, 1]:\n",
    "                for j in [-1, 0, 1]:\n",
    "                    for k in [-1, 0, 1]:\n",
    "                        # i*lattice_vectors[0] + j*lattice_vectors[1] + k*lattice_vectors[2]\n",
    "                        ijk_lattice_vectors = np.sum([i, j, k] * lattice_vectors.T, axis=1)\n",
    "\n",
    "                        # Compute error\n",
    "                        distance = np.linalg.norm(p2 - p1 + ijk_lattice_vectors)\n",
    "\n",
    "                        # Append to trial errors for differente atom images\n",
    "                        trial_errors.append(np.power(distance - weight, 2))\n",
    "            errors += np.min(trial_errors)\n",
    "        #print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        solution_attempt = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        lattice_vectors = solution_attempt[:3]\n",
    "        positions       = solution_attempt[3:]\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "    \n",
    "                trial_errors = []\n",
    "                for i in [-1, 0, 1]:\n",
    "                    for j in [-1, 0, 1]:\n",
    "                        for k in [-1, 0, 1]:\n",
    "                            # i*lattice_vectors[0] + j*lattice_vectors[1] + k*lattice_vectors[2]\n",
    "                            ijk_lattice_vectors = np.sum([i, j, k] * lattice_vectors.T, axis=1)\n",
    "    \n",
    "                            # Compute error\n",
    "                            distance = np.linalg.norm(p2 - p1 + ijk_lattice_vectors)\n",
    "    \n",
    "                            # Append to trial errors for different atom images\n",
    "                            trial_errors.append(np.power(distance - weight, 2))\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += np.min(trial_errors)\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d76ec67-05e8-4242-b25a-2a14bccac3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 0\n",
      "Total: 109.87638410004433 and local 0.7235525572033047 errors\n",
      "\n",
      "Attempt 1\n",
      "Total: 109.94789570923339 and local 0.7267014780141707 errors\n",
      "\n",
      "Attempt 2\n",
      "Total: 109.89995233490441 and local 0.7228179915015176 errors\n",
      "\n",
      "Attempt 3\n",
      "Total: 109.88679338793916 and local 0.7240374708545738 errors\n",
      "\n",
      "Attempt 4\n",
      "Total: 109.93015668648557 and local 0.7261723052134357 errors\n",
      "\n",
      "Attempt 5\n",
      "Total: 109.87679370188496 and local 0.7248992777977689 errors\n",
      "\n",
      "Attempt 6\n",
      "Total: 109.93114896602168 and local 0.72611697416587 errors\n",
      "\n",
      "Attempt 7\n",
      "Total: 109.88875323916463 and local 0.7250824500942431 errors\n",
      "\n",
      "Attempt 8\n",
      "Total: 110.16947113509427 and local 0.7365660549775519 errors\n",
      "\n",
      "Attempt 9\n",
      "Total: 109.92532154678788 and local 0.7252832039795168 errors\n",
      "\n",
      "Attempt 10\n",
      "Total: 109.86127211979566 and local 0.7242764620127184 errors\n",
      "\n",
      "Attempt 11\n",
      "Total: 110.18394283581172 and local 0.7380528973735023 errors\n",
      "\n",
      "Attempt 12\n",
      "Total: 109.93200312362731 and local 0.7250786212841855 errors\n",
      "\n",
      "Attempt 13\n",
      "Total: 109.87418186926864 and local 0.7265666569571022 errors\n",
      "\n",
      "Attempt 14\n",
      "Total: 109.86193074185694 and local 0.7260616698947046 errors\n",
      "\n",
      "Attempt 15\n",
      "Total: 109.85712255576156 and local 0.7253150732279132 errors\n",
      "\n",
      "Attempt 16\n",
      "Total: 109.86699654921269 and local 0.7253925725637488 errors\n",
      "\n",
      "Attempt 17\n",
      "Total: 109.88128590039383 and local 0.7252130193414772 errors\n",
      "\n",
      "Attempt 18\n",
      "Total: 109.85458476528972 and local 0.7248909745118065 errors\n",
      "\n",
      "Attempt 19\n",
      "Total: 109.89692029991134 and local 0.7256102569653317 errors\n",
      "\n",
      "Attempt 20\n",
      "Total: 109.89859451220711 and local 0.7293721722774036 errors\n",
      "\n",
      "Attempt 21\n",
      "Total: 109.87042308345833 and local 0.7255369911953699 errors\n",
      "\n",
      "Attempt 22\n",
      "Total: 109.88172494369954 and local 0.7261495165741634 errors\n",
      "\n",
      "Attempt 23\n",
      "Total: 109.88810062278029 and local 0.7262231137704905 errors\n",
      "\n",
      "Attempt 24\n",
      "Total: 109.89429664694991 and local 0.7249537338774071 errors\n",
      "\n",
      "Attempt 25\n",
      "Total: 109.90111682805392 and local 0.7251287932641899 errors\n",
      "\n",
      "Attempt 26\n",
      "Total: 109.91006568552322 and local 0.7244182951467615 errors\n",
      "\n",
      "Attempt 27\n",
      "Total: 109.90074355537413 and local 0.7233384543501854 errors\n",
      "\n",
      "Attempt 28\n",
      "Total: 109.86249554899952 and local 0.7231846356924996 errors\n",
      "\n",
      "Attempt 29\n",
      "Total: 109.91666195816354 and local 0.7243698430188643 errors\n",
      "\n",
      "Attempt 30\n",
      "Total: 109.91757633612877 and local 0.7244854224248458 errors\n",
      "\n",
      "Attempt 31\n",
      "Total: 109.91919119968011 and local 0.7252576366891347 errors\n",
      "\n",
      "Attempt 32\n",
      "Total: 109.92701767941774 and local 0.726103802244984 errors\n",
      "\n",
      "Attempt 33\n",
      "Total: 109.92568208845786 and local 0.7259251316006577 errors\n",
      "\n",
      "Attempt 34\n",
      "Total: 109.89407976900489 and local 0.7237544244188847 errors\n",
      "\n",
      "Attempt 35\n",
      "Total: 109.88794183901709 and local 0.7250418408217043 errors\n",
      "\n",
      "Attempt 36\n",
      "Total: 109.8638790253961 and local 0.7232554879830533 errors\n",
      "\n",
      "Attempt 37\n",
      "Total: 109.85953201605683 and local 0.723844860507761 errors\n",
      "\n",
      "Attempt 38\n",
      "Total: 109.87031215485196 and local 0.724665228344151 errors\n",
      "\n",
      "Attempt 39\n",
      "Total: 109.87121585250723 and local 0.7257649115597624 errors\n",
      "\n",
      "Attempt 40\n",
      "Total: 109.86859158812133 and local 0.7259734622424682 errors\n",
      "\n",
      "Attempt 41\n",
      "Total: 109.86610479095805 and local 0.725604593469565 errors\n",
      "\n",
      "Attempt 42\n",
      "Total: 109.86236369317979 and local 0.7253375732962347 errors\n",
      "\n",
      "Attempt 43\n",
      "Total: 109.85928171300606 and local 0.7243193452311231 errors\n",
      "\n",
      "Attempt 44\n",
      "Total: 109.9069103441899 and local 0.7247441735564257 errors\n",
      "\n",
      "Attempt 45\n",
      "Total: 109.89644021114229 and local 0.7268800669873752 errors\n",
      "\n",
      "Attempt 46\n",
      "Total: 109.88723520219358 and local 0.7272780532674736 errors\n",
      "\n",
      "Attempt 47\n",
      "Total: 109.8787853250663 and local 0.7263254810975274 errors\n",
      "\n",
      "Attempt 48\n",
      "Total: 109.87195489314783 and local 0.7262445421306569 errors\n",
      "\n",
      "Attempt 49\n",
      "Total: 109.87333028310024 and local 0.7259497213101831 errors\n",
      "\n",
      "Attempt 50\n",
      "Total: 109.90969384326846 and local 0.7255320042438363 errors\n",
      "\n",
      "Attempt 51\n",
      "Total: 109.896748994168 and local 0.7252870552604987 errors\n",
      "\n",
      "Attempt 52\n",
      "Total: 109.8759795386221 and local 0.7259147828936018 errors\n",
      "\n",
      "Attempt 53\n",
      "Total: 109.86292053220674 and local 0.7258294697189946 errors\n",
      "\n",
      "Attempt 54\n",
      "Total: 109.8933343868216 and local 0.7288824644053559 errors\n",
      "\n",
      "Attempt 55\n",
      "Total: 109.86809170357547 and local 0.7278766852560343 errors\n",
      "\n",
      "Attempt 56\n",
      "Total: 109.86246107138082 and local 0.7267900062900213 errors\n",
      "\n",
      "Attempt 57\n",
      "Total: 109.85796750789162 and local 0.7267058317369104 errors\n",
      "\n",
      "Attempt 58\n",
      "Total: 109.85444320941549 and local 0.7265169632237708 errors\n",
      "\n",
      "Attempt 59\n",
      "Total: 109.86598466493439 and local 0.7256558661461946 errors\n",
      "\n",
      "Attempt 60\n",
      "Total: 109.89991528360146 and local 0.7262939236617125 errors\n",
      "\n",
      "Attempt 61\n",
      "Total: 109.87914186924702 and local 0.7243080682451177 errors\n",
      "\n",
      "Attempt 62\n",
      "Total: 109.87516809392099 and local 0.7235188016248897 errors\n",
      "\n",
      "Attempt 63\n",
      "Total: 109.86803027001757 and local 0.7230730815910081 errors\n",
      "\n",
      "Attempt 64\n",
      "Total: 109.93444398375728 and local 0.7257804692001996 errors\n",
      "\n",
      "Attempt 65\n",
      "Total: 109.86495477266092 and local 0.7272934448836647 errors\n",
      "\n",
      "Attempt 66\n",
      "Total: 109.87517743715297 and local 0.725184011359454 errors\n",
      "\n",
      "Attempt 67\n",
      "Total: 109.89850005171544 and local 0.7237262234524386 errors\n",
      "\n",
      "Attempt 68\n",
      "Total: 109.8893252736754 and local 0.7224159158315071 errors\n",
      "\n",
      "Attempt 69\n",
      "Total: 109.92041174274837 and local 0.7265092665316419 errors\n",
      "\n",
      "Attempt 70\n",
      "Total: 109.91466856038738 and local 0.7272175961055882 errors\n",
      "\n",
      "Attempt 71\n",
      "Total: 109.92083249078475 and local 0.7248981658452976 errors\n",
      "\n",
      "Attempt 72\n",
      "Total: 109.9001834237249 and local 0.7247432086081249 errors\n",
      "\n",
      "Attempt 73\n",
      "Total: 109.87834213848384 and local 0.7239322448787723 errors\n",
      "\n",
      "Attempt 74\n",
      "Total: 109.88018182866647 and local 0.7259238738602383 errors\n",
      "\n",
      "Attempt 75\n",
      "Total: 109.87156391187305 and local 0.7256178984613013 errors\n",
      "\n",
      "Attempt 76\n",
      "Total: 109.86431837136537 and local 0.7259126007903991 errors\n",
      "\n",
      "Attempt 77\n",
      "Total: 109.86042909364495 and local 0.7257540055264318 errors\n",
      "\n",
      "Attempt 78\n",
      "Total: 109.86393660965538 and local 0.7252777874234745 errors\n",
      "\n",
      "Attempt 79\n",
      "Total: 109.86020426000773 and local 0.7256794592804217 errors\n",
      "\n",
      "Attempt 80\n",
      "Total: 109.8621957064494 and local 0.725753404650829 errors\n",
      "\n",
      "Attempt 81\n",
      "Total: 109.85614917669488 and local 0.7257670586147081 errors\n",
      "\n",
      "Attempt 82\n",
      "Total: 109.85689619413421 and local 0.7259204014391951 errors\n",
      "\n",
      "Attempt 83\n",
      "Total: 109.85994182519664 and local 0.7260065521891373 errors\n",
      "\n",
      "Attempt 84\n",
      "Total: 109.85656298237092 and local 0.7259196390074498 errors\n",
      "\n",
      "Attempt 85\n",
      "Total: 109.85518256841559 and local 0.7256371346319997 errors\n",
      "\n",
      "Attempt 86\n",
      "Total: 109.86742946052428 and local 0.7243259914326614 errors\n",
      "\n",
      "Attempt 87\n",
      "Total: 109.86112901722993 and local 0.7245365218669785 errors\n",
      "\n",
      "Attempt 88\n",
      "Total: 109.90674191287938 and local 0.7289030089066219 errors\n",
      "\n",
      "Attempt 89\n",
      "Total: 109.9088611187261 and local 0.7262417805176588 errors\n",
      "\n",
      "Attempt 90\n",
      "Total: 109.86518952389056 and local 0.7248738361324527 errors\n",
      "\n",
      "Attempt 91\n",
      "Total: 109.85778689616748 and local 0.7241391106088542 errors\n",
      "\n",
      "Attempt 92\n",
      "Total: 109.88275968874524 and local 0.7256585195666112 errors\n",
      "\n",
      "Attempt 93\n",
      "Total: 109.87005684402553 and local 0.7249607781563475 errors\n",
      "\n",
      "Attempt 94\n",
      "Total: 109.86652303114025 and local 0.7241595763799846 errors\n",
      "\n",
      "Attempt 95\n",
      "Total: 109.86434470785011 and local 0.7244223759953944 errors\n",
      "\n",
      "Attempt 96\n",
      "Total: 109.8603798549618 and local 0.7232287137585792 errors\n",
      "\n",
      "Attempt 97\n",
      "Total: 109.85963919612757 and local 0.7232585736035055 errors\n",
      "\n",
      "Attempt 98\n",
      "Total: 109.85930154423386 and local 0.7228592897047484 errors\n",
      "\n",
      "Attempt 99\n",
      "Total: 109.85830274615256 and local 0.7225428816636081 errors\n",
      "Converged to a solution.\n"
     ]
    }
   ],
   "source": [
    "error_threshold = 1e-5\n",
    "\n",
    "for attempt in range(100):\n",
    "    print()\n",
    "    print(f'Attempt {attempt}')\n",
    "    solution = minimize(objective, solution,\n",
    "                        args=(edges, weights),\n",
    "                        method='Powell')\n",
    "\n",
    "    is_success       = solution.success\n",
    "    solution_message = solution.message\n",
    "    worst_particle, worst_error = worst_identification(edges, weights, solution.x)\n",
    "\n",
    "    attempt_error = objective(solution.x, edges, weights)\n",
    "    print(f'Total: {attempt_error} and local {worst_error} errors')\n",
    "\n",
    "    if attempt_error < error_threshold:\n",
    "        break\n",
    "\n",
    "    solution = solution.x.reshape(-1, 3)  # Reshape to 2D array\n",
    "\n",
    "    # Re-initialize that position\n",
    "    if is_molecule:\n",
    "        solution[worst_particle] = np.random.rand(3)\n",
    "    else:\n",
    "        solution[worst_particle+3] = np.random.rand(3)\n",
    "\n",
    "    solution = solution.flatten()\n",
    "\n",
    "# Check convergence status\n",
    "if is_success:\n",
    "    print('Converged to a solution.')\n",
    "else:\n",
    "    print(f'Failed to converge: {solution_message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "515f6a4d-8542-4695-866c-9734d739f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solution.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c66525-8a75-4c42-9a2a-a9edf135d70c",
   "metadata": {},
   "source": [
    "## Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74512108-9c89-46c6-a60c-a3f1cd5d487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)\n",
    "    \n",
    "    lattice_vectors     = np.array([[10,  0,   0],\n",
    "                                    [0,   10,  0],\n",
    "                                    [0,   0,   10]])\n",
    "    cartesian_positions = solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292e640-09fc-4593-9782-6b022dc1f860",
   "metadata": {},
   "source": [
    "## Crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c905edb6-ba68-41f3-927d-cd0fc21fee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    \n",
    "    lattice_vectors     = solution[:3]\n",
    "    cartesian_positions = solution[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0709aff-bf63-4f99-ab14-e8bcabac3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSCAR_name = None\n",
    "\n",
    "# Get name for the first line of the POSCAR\n",
    "POSCAR_name = POSCAR_name or 'POSCAR from GenerativeModels'\n",
    "\n",
    "# Clone the input graph to preserve the original structure\n",
    "new_graph = temp.clone()\n",
    "\n",
    "# Load and detach embeddings for the graph nodes\n",
    "data_embeddings = new_graph.x.detach().cpu().numpy()\n",
    "\n",
    "# Loading dictionary of available embeddings for atoms\n",
    "available_embeddings = {}\n",
    "with open('../MP/input/atomic_masses.dat', 'r') as atomic_masses_file:\n",
    "    for line in atomic_masses_file:\n",
    "        key, mass, charge, electronegativity, ionization_energy = line.split()\n",
    "\n",
    "        # Check if all information is present\n",
    "        if all(val != 'None' for val in (mass, charge, electronegativity, ionization_energy)):\n",
    "            available_embeddings[key] = np.array([mass, charge, electronegativity, ionization_energy], dtype=float)\n",
    "\n",
    "# Get most similar atoms for each graph node and create a list of keys\n",
    "keys = [find_closest_key(available_embeddings, emb) for emb in data_embeddings]\n",
    "\n",
    "# Get elements' composition, concentration, and positions\n",
    "POSCAR_composition, POSCAR_concentration, POSCAR_positions = composition_concentration_from_keys(keys, cartesian_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4065eb29-e8e9-4530-a68a-dd5bf8d7dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write file\n",
    "with open('CONTCAR', 'w') as POSCAR_file:\n",
    "    # Delete previous data in the file\n",
    "    POSCAR_file.truncate()\n",
    "    \n",
    "    # Write POSCAR's name\n",
    "    POSCAR_file.write(f'{POSCAR_name}\\n')\n",
    "\n",
    "    # Write scaling factor (assumed to be 1.0)\n",
    "    POSCAR_file.write('1.0\\n')\n",
    "\n",
    "    # Write lattice parameters (assumed to be orthogonal)\n",
    "    np.savetxt(POSCAR_file, lattice_vectors, delimiter=' ')\n",
    "\n",
    "    # Write composition (each different species, previously sorted)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_composition], fmt='%s', delimiter=' ')\n",
    "\n",
    "    # Write concentration (number of each of the previous elements)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_concentration], fmt='%d', delimiter=' ')\n",
    "\n",
    "    # Write position in cartesian form\n",
    "    POSCAR_file.write('Cartesian\\n')\n",
    "    np.savetxt(POSCAR_file, POSCAR_positions, delimiter=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
