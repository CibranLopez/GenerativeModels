{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy    as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "from libraries.model      import nGCNN, eGCNN, denoise, get_random_graph, add_features_to_graph\n",
    "from libraries.dataset    import revert_standardize_dataset\n",
    "from libraries.graph      import POSCAR_graph_encoding\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a88fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From random noise, we generate completely new materials\n",
    "# A target property can be seeked with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder in which all data will be stored\n",
    "target_folder    = 'models/Loaded_QM9_gap-sphere-images/GM_v0'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "\n",
    "# Number of graphs to predict\n",
    "N_predictions = 10\n",
    "\n",
    "# Define target to be generated\n",
    "target_tensor = torch.tensor(1, dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9310c",
   "metadata": {},
   "source": [
    "# Load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24741239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in JSON format to a dictionary\n",
    "with open(f'{target_folder}/model_parameters.json', 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "model_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        model_parameters[key] = torch.tensor(value, device=device)\n",
    "    except:\n",
    "        model_parameters[key] = value\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = model_parameters['n_t_steps']\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = model_parameters['noise_contribution']\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = model_parameters['dropout_node']\n",
    "dropout_edge = model_parameters['dropout_edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b95aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143570/933877349.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(dataset_name_std)\n"
     ]
    }
   ],
   "source": [
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "labels_name                 = f'{target_folder}/standardized_labels.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset\n",
    "dataset = torch.load(dataset_name_std)\n",
    "\n",
    "# Read the file in JSON format to a dictionary\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "dataset_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        dataset_parameters[key] = torch.tensor(value, device=device)\n",
    "    except:\n",
    "        dataset_parameters[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339f409c-3092-41ee-a6ad-0485d67658b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize target_tensor accordingly\n",
    "target_tensor = (target_tensor - dataset_parameters['target_mean']) * dataset_parameters['scale'] / dataset_parameters['target_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066eb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.983739852905273, 2.9542582035064697)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the number of nodes\n",
    "total_nodes = torch.tensor([data.num_nodes for data in dataset])\n",
    "mean_nodes  = torch.mean(total_nodes.float()).item()\n",
    "std_nodes   = torch.std(total_nodes.float()).item()\n",
    "\n",
    "mean_nodes, std_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff10529-1eb5-4d9a-8b5f-c5e5fc08715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "DataParallel(\n",
      "  (module): nGCNN(\n",
      "    (conv1): GraphConv(6, 256)\n",
      "    (conv2): GraphConv(256, 512)\n",
      "    (conv3): GraphConv(512, 256)\n",
      "    (conv4): GraphConv(256, 4)\n",
      "    (norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "DataParallel(\n",
      "  (module): eGCNN(\n",
      "    (linear1): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (linear3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (linear4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = dataset[0].num_node_features\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(dataset[0].y)\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features+1, dropout_node)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features+1, dropout_edge)\n",
    "\n",
    "node_model.load_state_dict(torch.load(node_model_name, map_location=torch.device(device), weights_only=False))\n",
    "edge_model.load_state_dict(torch.load(edge_model_name, map_location=torch.device(device), weights_only=False))\n",
    "node_model.eval()\n",
    "edge_model.eval()\n",
    "\n",
    "# Allow data parallelization among multi-GPU\n",
    "node_model= nn.DataParallel(node_model)\n",
    "edge_model= nn.DataParallel(edge_model)\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a0f21",
   "metadata": {},
   "source": [
    "# Generating new cystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e7b29c-a264-40e8-9908-92972731a20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[19, 4], edge_index=[2, 171], edge_attr=[171]),\n",
       " Data(x=[18, 4], edge_index=[2, 153], edge_attr=[153]),\n",
       " Data(x=[15, 4], edge_index=[2, 105], edge_attr=[105]),\n",
       " Data(x=[14, 4], edge_index=[2, 91], edge_attr=[91]),\n",
       " Data(x=[13, 4], edge_index=[2, 78], edge_attr=[78]),\n",
       " Data(x=[18, 4], edge_index=[2, 153], edge_attr=[153]),\n",
       " Data(x=[19, 4], edge_index=[2, 171], edge_attr=[171]),\n",
       " Data(x=[19, 4], edge_index=[2, 171], edge_attr=[171]),\n",
       " Data(x=[22, 4], edge_index=[2, 231], edge_attr=[231]),\n",
       " Data(x=[18, 4], edge_index=[2, 153], edge_attr=[153])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create constant target tensor once\n",
    "features_tensor = torch.tensor([target_tensor, 0], device=device)\n",
    "\n",
    "# Predicting loop\n",
    "diffused_dataset = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(N_predictions):\n",
    "        # Get a positive random number of nodes using absolute value\n",
    "        n_nodes = int(np.random.normal(mean_nodes, std_nodes))\n",
    "        \n",
    "        # Get random graph, acting as diffused\n",
    "        diffused_graph = get_random_graph(n_nodes, n_node_features)\n",
    "\n",
    "        # Make room for n_graph_features and t_steps in the dataset\n",
    "        diffused_graph = add_features_to_graph(diffused_graph, features_tensor)\n",
    "        \n",
    "        diffused_dataset.append(diffused_graph)\n",
    "            \n",
    "    # Generate batch object and move data to device\n",
    "    diff_batch = Batch.from_data_list(diffused_dataset).to(device)\n",
    "    \n",
    "    # Denoise batch\n",
    "    predicted_dataset = denoise(diff_batch, n_t_steps, node_model, edge_model, alpha_decay=alpha_decay,\n",
    "                                sigma=model_parameters['sigma'], n_features=n_node_features)\n",
    "\n",
    "# From batch object to list\n",
    "predicted_dataset = predicted_dataset.to_data_list()\n",
    "\n",
    "# Remove graph features\n",
    "for graph in predicted_dataset:\n",
    "    graph.x = graph.x[:, :n_node_features]\n",
    "\n",
    "# Revert standardization\n",
    "denoised_graphs = revert_standardize_dataset(predicted_dataset, dataset_parameters)\n",
    "denoised_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af401f1-89eb-43b9-8f42-2996b590e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = denoised_graphs[0]\n",
    "\n",
    "nodes = temp.x\n",
    "edges = temp.edge_index.detach().cpu().numpy().T\n",
    "weights = temp.edge_attr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087b15ac-0819-4d19-b613-7c5dbc908885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from scipy.optimize       import minimize\n",
    "from libraries.graph      import graph_POSCAR_encoding, find_closest_key, composition_concentration_from_keys\n",
    "from torch_geometric.data import Data\n",
    "from pymatgen.core        import Structure\n",
    "\n",
    "\n",
    "is_molecule = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110c5c9-d0cf-4056-ada5-5c2206034939",
   "metadata": {},
   "source": [
    "## Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f199e7-5d19-4c52-81f7-25d8dec93263",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_molecule:\n",
    "    # Initial guess for the positions\n",
    "    solution = np.random.rand(len(nodes) * 3)*10  # Initialize all points at origin, 1D array\n",
    "    #solution = coordinates.reshape(-1, 1).ravel()\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            distance = np.linalg.norm(p2 - p1)\n",
    "            errors += np.power(distance - weight, 2)\n",
    "        #print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "                \n",
    "                # Compute error\n",
    "                distance = np.linalg.norm(p2 - p1)\n",
    "\n",
    "                # Append to trial errors for different atom images\n",
    "                trial_error = np.power(distance - weight, 2)\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += trial_error\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66e868-1cf3-483b-b86e-be0530df0704",
   "metadata": {},
   "source": [
    "## Crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a4e28f-83d9-4971-a5fc-3200d0ec4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_molecule:\n",
    "    # Initial guess for the lattice parameters\n",
    "    lattice_vectors = np.array([[10, 0,   0],\n",
    "                                [0,   10, 0],\n",
    "                                [0,   0,   10]])\n",
    "    \n",
    "    # Initial guess for the positions\n",
    "    initial_positions = np.random.rand(len(nodes) * 3)  # Initialize all points at origin, 1D array\n",
    "    #initial_positions = coordinates.reshape(-1, 1).ravel()\n",
    "    solution = np.concatenate([lattice_vectors.ravel(), initial_positions])\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        solution_attempt = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        \n",
    "        lattice_vectors = solution_attempt[:3]\n",
    "        positions       = solution_attempt[3:]\n",
    "        \n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            \n",
    "            trial_errors = [] \n",
    "            for i in [-1, 0, 1]:\n",
    "                for j in [-1, 0, 1]:\n",
    "                    for k in [-1, 0, 1]:\n",
    "                        # i*lattice_vectors[0] + j*lattice_vectors[1] + k*lattice_vectors[2]\n",
    "                        ijk_lattice_vectors = np.sum([i, j, k] * lattice_vectors.T, axis=1)\n",
    "\n",
    "                        # Compute error\n",
    "                        distance = np.linalg.norm(p2 - p1 + ijk_lattice_vectors)\n",
    "\n",
    "                        # Append to trial errors for differente atom images\n",
    "                        trial_errors.append(np.power(distance - weight, 2))\n",
    "            errors += np.min(trial_errors)\n",
    "        #print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        solution_attempt = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        lattice_vectors = solution_attempt[:3]\n",
    "        positions       = solution_attempt[3:]\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "    \n",
    "                trial_errors = []\n",
    "                for i in [-1, 0, 1]:\n",
    "                    for j in [-1, 0, 1]:\n",
    "                        for k in [-1, 0, 1]:\n",
    "                            # i*lattice_vectors[0] + j*lattice_vectors[1] + k*lattice_vectors[2]\n",
    "                            ijk_lattice_vectors = np.sum([i, j, k] * lattice_vectors.T, axis=1)\n",
    "    \n",
    "                            # Compute error\n",
    "                            distance = np.linalg.norm(p2 - p1 + ijk_lattice_vectors)\n",
    "    \n",
    "                            # Append to trial errors for different atom images\n",
    "                            trial_errors.append(np.power(distance - weight, 2))\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += np.min(trial_errors)\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76ec67-05e8-4242-b25a-2a14bccac3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 0\n",
      "Total: 125.74301359596039 and local 0.7476436327115027 errors\n",
      "\n",
      "Attempt 1\n",
      "Total: 125.71236875602945 and local 0.7468679023297331 errors\n",
      "\n",
      "Attempt 2\n",
      "Total: 125.70692815795934 and local 0.7493603011531895 errors\n",
      "\n",
      "Attempt 3\n",
      "Total: 125.7226210572962 and local 0.7482249363311236 errors\n",
      "\n",
      "Attempt 4\n",
      "Total: 125.76260696645463 and local 0.7462730597538276 errors\n",
      "\n",
      "Attempt 5\n",
      "Total: 125.74114983985318 and local 0.7451115367432417 errors\n",
      "\n",
      "Attempt 6\n"
     ]
    }
   ],
   "source": [
    "error_threshold = 1e-5\n",
    "\n",
    "for attempt in range(100):\n",
    "    print()\n",
    "    print(f'Attempt {attempt}')\n",
    "    solution = minimize(objective, solution,\n",
    "                        args=(edges, weights),\n",
    "                        method='Powell')\n",
    "\n",
    "    is_success       = solution.success\n",
    "    solution_message = solution.message\n",
    "    worst_particle, worst_error = worst_identification(edges, weights, solution.x)\n",
    "\n",
    "    attempt_error = objective(solution.x, edges, weights)\n",
    "    print(f'Total: {attempt_error} and local {worst_error} errors')\n",
    "\n",
    "    if attempt_error < error_threshold:\n",
    "        break\n",
    "\n",
    "    solution = solution.x.reshape(-1, 3)  # Reshape to 2D array\n",
    "\n",
    "    # Re-initialize that position\n",
    "    if is_molecule:\n",
    "        solution[worst_particle] = np.random.rand(3)\n",
    "    else:\n",
    "        solution[worst_particle+3] = np.random.rand(3)\n",
    "\n",
    "    solution = solution.flatten()\n",
    "\n",
    "# Check convergence status\n",
    "if is_success:\n",
    "    print('Converged to a solution.')\n",
    "else:\n",
    "    print(f'Failed to converge: {solution_message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f6a4d-8542-4695-866c-9734d739f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solution.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c66525-8a75-4c42-9a2a-a9edf135d70c",
   "metadata": {},
   "source": [
    "## Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74512108-9c89-46c6-a60c-a3f1cd5d487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)\n",
    "    \n",
    "    lattice_vectors     = np.array([[10,  0,   0],\n",
    "                                    [0,   10,  0],\n",
    "                                    [0,   0,   10]])\n",
    "    cartesian_positions = solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292e640-09fc-4593-9782-6b022dc1f860",
   "metadata": {},
   "source": [
    "## Crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905edb6-ba68-41f3-927d-cd0fc21fee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    \n",
    "    lattice_vectors     = solution[:3]\n",
    "    cartesian_positions = solution[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0709aff-bf63-4f99-ab14-e8bcabac3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSCAR_name = None\n",
    "\n",
    "# Get name for the first line of the POSCAR\n",
    "POSCAR_name = POSCAR_name or 'POSCAR from GenerativeModels'\n",
    "\n",
    "# Clone the input graph to preserve the original structure\n",
    "new_graph = temp.clone()\n",
    "\n",
    "# Load and detach embeddings for the graph nodes\n",
    "data_embeddings = new_graph.x.detach().cpu().numpy()\n",
    "\n",
    "# Loading dictionary of available embeddings for atoms\n",
    "available_embeddings = {}\n",
    "with open('../MP/input/atomic_masses.dat', 'r') as atomic_masses_file:\n",
    "    for line in atomic_masses_file:\n",
    "        key, mass, charge, electronegativity, ionization_energy = line.split()\n",
    "\n",
    "        # Check if all information is present\n",
    "        if all(val != 'None' for val in (mass, charge, electronegativity, ionization_energy)):\n",
    "            available_embeddings[key] = np.array([mass, charge, electronegativity, ionization_energy], dtype=float)\n",
    "\n",
    "# Get most similar atoms for each graph node and create a list of keys\n",
    "keys = [find_closest_key(available_embeddings, emb) for emb in data_embeddings]\n",
    "\n",
    "# Get elements' composition, concentration, and positions\n",
    "POSCAR_composition, POSCAR_concentration, POSCAR_positions = composition_concentration_from_keys(keys, cartesian_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4065eb29-e8e9-4530-a68a-dd5bf8d7dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write file\n",
    "with open('CONTCAR', 'w') as POSCAR_file:\n",
    "    # Delete previous data in the file\n",
    "    POSCAR_file.truncate()\n",
    "    \n",
    "    # Write POSCAR's name\n",
    "    POSCAR_file.write(f'{POSCAR_name}\\n')\n",
    "\n",
    "    # Write scaling factor (assumed to be 1.0)\n",
    "    POSCAR_file.write('1.0\\n')\n",
    "\n",
    "    # Write lattice parameters (assumed to be orthogonal)\n",
    "    np.savetxt(POSCAR_file, lattice_vectors, delimiter=' ')\n",
    "\n",
    "    # Write composition (each different species, previously sorted)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_composition], fmt='%s', delimiter=' ')\n",
    "\n",
    "    # Write concentration (number of each of the previous elements)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_concentration], fmt='%d', delimiter=' ')\n",
    "\n",
    "    # Write position in cartesian form\n",
    "    POSCAR_file.write('Cartesian\\n')\n",
    "    np.savetxt(POSCAR_file, POSCAR_positions, delimiter=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
