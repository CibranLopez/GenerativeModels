{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "from libraries.model      import nGCNN, eGCNN, denoise, get_random_graph\n",
    "from libraries.dataset    import revert_standardize_dataset\n",
    "from libraries.graph      import POSCAR_graph_encoding\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a88fe8",
   "metadata": {},
   "source": [
    "# From random noise, we generate completely new materials\n",
    "# A target property can be seeked with this approach"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83ca3ae",
   "metadata": {},
   "source": [
    "# Define folder in which all data will be stored\n",
    "target_folder    = 'models/QM9-dsC7O2H10nsd/GM_v0'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "\n",
    "# Number of graphs to predict\n",
    "N_predictions = 10\n",
    "\n",
    "# Define target to be generated\n",
    "target_tensor = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "29f9310c",
   "metadata": {},
   "source": [
    "# Load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24741239",
   "metadata": {},
   "source": [
    "# Read the file in JSON format to a dictionary\n",
    "with open(f'{target_folder}/model_parameters.json', 'r') as json_file:\n",
    "    model_parameters = json.load(json_file)\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = model_parameters['n_t_steps']\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = model_parameters['noise_contribution']\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = model_parameters['dropout_node']\n",
    "dropout_edge = model_parameters['dropout_edge']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b95aa1f",
   "metadata": {},
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset, with corresponding labels and parameters\n",
    "dataset = torch.load(dataset_name_std)\n",
    "labels  = torch.load(labels_name)\n",
    "\n",
    "# Read the file in JSON format to a dictionary\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    dataset_parameters = json.load(json_file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066eb85e",
   "metadata": {},
   "source": [
    "# Calculate the mean and standard deviation of the number of nodes\n",
    "total_nodes = torch.tensor([data.num_nodes for data in dataset])\n",
    "mean_nodes  = torch.mean(total_nodes.float()).item()\n",
    "std_nodes   = torch.std(total_nodes.float()).item()\n",
    "\n",
    "mean_nodes, std_nodes"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419e5cec-b09f-422c-af85-33b41dbe23c5",
   "metadata": {},
   "source": [
    "for graph in dataset:\n",
    "    #graph.y = torch.tensor([graph.y], dtype=torch.float)\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1aa0ba5",
   "metadata": {},
   "source": [
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(dataset[0].y)\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "node_model.load_state_dict(torch.load(node_model_name))\n",
    "node_model.eval()\n",
    "\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "edge_model.eval()\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "963a0f21",
   "metadata": {},
   "source": [
    "# Generating new cystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd3de58-8827-4fde-8a83-f40f51f18ffb",
   "metadata": {},
   "source": [
    "# Predicting loop\n",
    "diffused_batch = []\n",
    "with torch.no_grad():\n",
    "    for i in range(N_predictions):\n",
    "        # Get random number of nodes\n",
    "        n_nodes = int(np.random.normal(mean_nodes, std_nodes))\n",
    "        \n",
    "        # Get random graph, acting as diffused\n",
    "        diffused_graph = get_random_graph(n_nodes, n_node_features-1)\n",
    "        diffused_graph.y = torch.tensor([0], dtype=torch.float)\n",
    "        diffused_batch.append(diffused_graph)\n",
    "\n",
    "    # Generate batch objects\n",
    "    diffused_batch = Batch.from_data_list(diffused_batch)\n",
    "    \n",
    "    # Move data to device\n",
    "    diffused_batch = diffused_batch.to(device)\n",
    "    \n",
    "    # Denoise batch of diffused graphs\n",
    "    #print(f'Denoising...')\n",
    "    predicted_dataset = denoise(diffused_batch, n_t_steps, node_model, edge_model, n_graph_features,\n",
    "                                s=alpha_decay, sigma=model_parameters['sigma'])\n",
    "\n",
    "# From batch object to list\n",
    "predicted_dataset = predicted_dataset.to_data_list()\n",
    "\n",
    "# Revert stardadization\n",
    "denoised_graphs = revert_standardize_dataset(predicted_dataset, dataset_parameters)\n",
    "denoised_graphs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af401f1-89eb-43b9-8f42-2996b590e6e2",
   "metadata": {},
   "source": [
    "temp = denoised_graphs[0]\n",
    "\n",
    "nodes = temp.x\n",
    "edges = temp.edge_index.detach().cpu().numpy().T\n",
    "attributes = temp.edge_attr.detach().cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087b15ac-0819-4d19-b613-7c5dbc908885",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from scipy.optimize       import minimize\n",
    "from libraries.graph      import graph_POSCAR_encoding, find_closest_key, composition_concentration_from_keys\n",
    "from torch_geometric.data import Data\n",
    "from pymatgen.core        import Structure\n",
    "\n",
    "\n",
    "is_molecule = True\n",
    "\n",
    "\n",
    "if is_molecule:\n",
    "    # Initial guess for the positions\n",
    "    solution = np.random.rand(len(nodes) * 3)*10  # Initialize all points at origin, 1D array\n",
    "    #solution = coordinates.reshape(-1, 1).ravel()\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            distance = np.linalg.norm(p2 - p1)\n",
    "            errors += np.power(distance - weight, 2)\n",
    "        print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "                \n",
    "                # Compute error\n",
    "                distance = np.linalg.norm(p2 - p1)\n",
    "\n",
    "                # Append to trial errors for different atom images\n",
    "                trial_error = np.power(distance - weight, 2)\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += trial_error\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74111279-8e3c-4c14-8209-b0792b81eb62",
   "metadata": {},
   "source": [
    "error_threshold = 1e-5\n",
    "\n",
    "n_attempts = 1\n",
    "for attempt in range(n_attempts):\n",
    "    print()\n",
    "    print(f'Attempt {attempt}')\n",
    "    solution = minimize(objective, solution,\n",
    "                        args=(edges, attributes),\n",
    "                        method='Powell')\n",
    "\n",
    "    is_success       = solution.success\n",
    "    solution_message = solution.message\n",
    "    worst_particle, worst_error = worst_identification(edges, attributes, solution.x)\n",
    "\n",
    "    attempt_error = objective(solution.x, edges, attributes)\n",
    "    print(f'Total: {attempt_error} and local {worst_error} errors')\n",
    "\n",
    "    if (attempt_error < error_threshold) or (attempt == n_attempts-1):\n",
    "        break\n",
    "    \n",
    "    solution = solution.x.reshape(-1, 3)  # Reshape to 2D array\n",
    "\n",
    "    # Re-initialize that position\n",
    "    solution[worst_particle+3] = np.random.rand(3)\n",
    "\n",
    "    solution = solution.flatten()\n",
    "\n",
    "# Check convergence status\n",
    "if is_success:\n",
    "    print('Converged to a solution.')\n",
    "else:\n",
    "    print(f'Failed to converge: {solution_message}')\n",
    "\n",
    "\n",
    "solution = solution.x.reshape(-1, 3)  # Reshape to 2D array"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25d44f47-620b-4820-a1b6-cdd95fd92465",
   "metadata": {},
   "source": [
    "if is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)\n",
    "    \n",
    "    lattice_vectors     = np.array([[100, 0,   0],\n",
    "                                    [0,   100, 0],\n",
    "                                    [0,   0,   100]])\n",
    "    cartesian_positions = solution\n",
    "\n",
    "\n",
    "POSCAR_name = None\n",
    "\n",
    "# Get name for the first line of the POSCAR\n",
    "POSCAR_name = POSCAR_name or 'POSCAR from GenerativeModels'\n",
    "\n",
    "# Clone the input graph to preserve the original structure\n",
    "new_graph = temp.clone()\n",
    "\n",
    "# Load and detach embeddings for the graph nodes\n",
    "data_embeddings = new_graph.x.detach().cpu().numpy()\n",
    "\n",
    "# Loading dictionary of available embeddings for atoms\n",
    "available_embeddings = {}\n",
    "with open('../MP/input/atomic_masses.dat', 'r') as atomic_masses_file:\n",
    "    for line in atomic_masses_file:\n",
    "        key, mass, charge, electronegativity, ionization_energy = line.split()\n",
    "\n",
    "        # Check if all information is present\n",
    "        if all(val != 'None' for val in (mass, charge, electronegativity, ionization_energy)):\n",
    "            available_embeddings[key] = np.array([mass, charge, electronegativity, ionization_energy], dtype=float)\n",
    "\n",
    "# Get most similar atoms for each graph node and create a list of keys\n",
    "keys = [find_closest_key(available_embeddings, emb) for emb in data_embeddings]\n",
    "\n",
    "# Get elements' composition, concentration, and positions\n",
    "POSCAR_composition, POSCAR_concentration, POSCAR_positions = composition_concentration_from_keys(keys, cartesian_positions)\n",
    "\n",
    "\n",
    "# Write file\n",
    "with open('CONTCAR', 'w') as POSCAR_file:\n",
    "    # Delete previous data in the file\n",
    "    POSCAR_file.truncate()\n",
    "    \n",
    "    # Write POSCAR's name\n",
    "    POSCAR_file.write(f'{POSCAR_name}\\n')\n",
    "\n",
    "    # Write scaling factor (assumed to be 1.0)\n",
    "    POSCAR_file.write('1.0\\n')\n",
    "\n",
    "    # Write lattice parameters (assumed to be orthogonal)\n",
    "    np.savetxt(POSCAR_file, lattice_vectors, delimiter=' ')\n",
    "\n",
    "    # Write composition (each different species, previously sorted)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_composition], fmt='%s', delimiter=' ')\n",
    "\n",
    "    # Write concentration (number of each of the previous elements)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_concentration], fmt='%d', delimiter=' ')\n",
    "\n",
    "    # Write position in cartesian form\n",
    "    POSCAR_file.write('Cartesian\\n')\n",
    "    np.savetxt(POSCAR_file, POSCAR_positions, delimiter=' ')"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
