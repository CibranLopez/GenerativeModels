{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import torch.nn          as nn\n",
    "import libraries.dataset as gld\n",
    "import libraries.model   as glm\n",
    "import libraries.graph   as glg\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from pymatgen.core        import Structure\n",
    "from scipy.optimize       import minimize\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a88fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From random noise, we generate completely new materials\n",
    "# A target property can be seeked with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder in which all data will be stored\n",
    "is_molecule     = True\n",
    "target_folder   = 'models/QM9-all-linked/GM_v0'\n",
    "model_name = f'{target_folder}/model.pt'\n",
    "\n",
    "# Number of graphs to predict\n",
    "N_predictions = 10\n",
    "\n",
    "# Define target to be generated\n",
    "target_tensor = torch.tensor(0, dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9310c",
   "metadata": {},
   "source": [
    "# Load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24741239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in JSON format to a dictionary\n",
    "with open(f'{target_folder}/model_parameters.json', 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "model_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        model_parameters[key] = torch.tensor(value, device=device)\n",
    "    except:\n",
    "        model_parameters[key] = value\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = model_parameters['n_t_steps']\n",
    "\n",
    "model_parameters['alpha_decay'] = torch.tensor(0.4, device=device)\n",
    "\n",
    "# Decay of parameter alpha\n",
    "alpha_decay = model_parameters['alpha_decay']\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "pdropout_node = model_parameters['dropout_node']\n",
    "pdropout_edge = model_parameters['dropout_edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b95aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "labels_name                 = f'{target_folder}/standardized_labels.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset\n",
    "dataset = torch.load(dataset_name_std, weights_only=False)\n",
    "\n",
    "# Read the file in JSON format to a dictionary\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "dataset_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        dataset_parameters[key] = torch.tensor(value, device=device)\n",
    "    except:\n",
    "        dataset_parameters[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339f409c-3092-41ee-a6ad-0485d67658b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize target_tensor accordingly\n",
    "target_tensor = (target_tensor - dataset_parameters['target_mean']) * dataset_parameters['scale'] / dataset_parameters['target_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066eb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.983739852905273, 2.9542582035064697)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the number of nodes\n",
    "total_nodes = torch.tensor([data.num_nodes for data in dataset])\n",
    "mean_nodes  = torch.mean(total_nodes.float()).item()\n",
    "std_nodes   = torch.std(total_nodes.float()).item()\n",
    "\n",
    "mean_nodes, std_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff10529-1eb5-4d9a-8b5f-c5e5fc08715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GCNN:\n",
      "DataParallel(\n",
      "  (module): GNN(\n",
      "    (node_conv1): GraphConv(8, 32)\n",
      "    (node_conv2): GraphConv(32, 64)\n",
      "    (node_conv3): GraphConv(64, 4)\n",
      "    (edge_linear_f1): Linear(in_features=17, out_features=32, bias=True)\n",
      "    (edge_linear_r1): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (edge_linear_f2): Linear(in_features=65, out_features=32, bias=True)\n",
      "    (edge_linear_r2): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (edge_linear_f3): Linear(in_features=129, out_features=16, bias=True)\n",
      "    (edge_linear_r3): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (node_norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (edge_norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = dataset[0].num_node_features\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(dataset[0].y)\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "model = glm.GNN(n_node_features, n_graph_features, pdropout_node, pdropout_edge).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_name, map_location=torch.device(device), weights_only=False))\n",
    "model.eval()\n",
    "\n",
    "# Allow data parallelization among multi-GPU\n",
    "model= nn.DataParallel(model)\n",
    "\n",
    "print('\\nGCNN:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a0f21",
   "metadata": {},
   "source": [
    "# Generating new cystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e13c537c-c31e-4817-89b8-5e60a1281b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[174, 8], edge_index=[2, 1469], edge_attr=[1469], batch=[174], ptr=[11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Data(x=[18, 4], edge_index=[2, 153], edge_attr=[153]),\n",
       " Data(x=[13, 4], edge_index=[2, 78], edge_attr=[78]),\n",
       " Data(x=[18, 4], edge_index=[2, 153], edge_attr=[153]),\n",
       " Data(x=[17, 4], edge_index=[2, 136], edge_attr=[136]),\n",
       " Data(x=[14, 4], edge_index=[2, 91], edge_attr=[91]),\n",
       " Data(x=[21, 4], edge_index=[2, 210], edge_attr=[210]),\n",
       " Data(x=[17, 4], edge_index=[2, 136], edge_attr=[136]),\n",
       " Data(x=[14, 4], edge_index=[2, 91], edge_attr=[91]),\n",
       " Data(x=[20, 4], edge_index=[2, 190], edge_attr=[190]),\n",
       " Data(x=[22, 4], edge_index=[2, 231], edge_attr=[231])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create constant target tensor once\n",
    "features_tensor = torch.cat([target_tensor, torch.tensor([0], device=device, dtype=target_tensor.dtype)])\n",
    "\n",
    "# Predicting loop\n",
    "diffused_dataset = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(N_predictions):\n",
    "        # Get a positive random number of nodes using absolute value\n",
    "        n_nodes = int(np.abs(np.random.normal(mean_nodes, std_nodes)))\n",
    "        \n",
    "        # Get random graph, acting as diffused\n",
    "        diffused_graph = glm.get_random_graph(n_nodes, n_node_features)\n",
    "\n",
    "        # Make room for n_graph_features and t_steps in the dataset\n",
    "        diffused_graph = glm.add_features_to_graph(diffused_graph, features_tensor)\n",
    "        \n",
    "        diffused_dataset.append(diffused_graph)\n",
    "            \n",
    "    # Generate batch object and move data to device\n",
    "    diff_batch = Batch.from_data_list(diffused_dataset).to(device)\n",
    "    print(diff_batch)\n",
    "    # Denoise batch\n",
    "    predicted_dataset = glm.denoise(diff_batch,\n",
    "                                    n_t_steps, alpha_decay,\n",
    "                                    model,\n",
    "                                    n_features=n_node_features)\n",
    "\n",
    "# From batch object to list\n",
    "predicted_dataset = predicted_dataset.to_data_list()\n",
    "\n",
    "# Remove graph features\n",
    "for graph in predicted_dataset:\n",
    "    graph.x = graph.x[:, :n_node_features]\n",
    "\n",
    "# Revert standardization\n",
    "denoised_graphs = gld.revert_standardize_dataset(predicted_dataset, dataset_parameters)\n",
    "denoised_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8af401f1-89eb-43b9-8f42-2996b590e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = denoised_graphs[0]\n",
    "\n",
    "nodes = temp.x\n",
    "edges = temp.edge_index.detach().cpu().numpy().T\n",
    "weights = temp.edge_attr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110c5c9-d0cf-4056-ada5-5c2206034939",
   "metadata": {},
   "source": [
    "## Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32f199e7-5d19-4c52-81f7-25d8dec93263",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_molecule:\n",
    "    # Initial guess for the positions\n",
    "    solution = np.random.rand(len(nodes) * 3)*10  # Initialize all points at origin, 1D array\n",
    "    #solution = coordinates.reshape(-1, 1).ravel()\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            distance = np.linalg.norm(p2 - p1)\n",
    "            errors += np.power(distance - weight, 2)\n",
    "        #print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        positions = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "                \n",
    "                # Compute error\n",
    "                distance = np.linalg.norm(p2 - p1)\n",
    "\n",
    "                # Append to trial errors for different atom images\n",
    "                trial_error = np.power(distance - weight, 2)\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += trial_error\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66e868-1cf3-483b-b86e-be0530df0704",
   "metadata": {},
   "source": [
    "## Crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a4e28f-83d9-4971-a5fc-3200d0ec4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_molecule:\n",
    "    # Initial guess for the lattice parameters\n",
    "    lattice_vectors = np.array([[12.6111574162109861, 0.0000011161086378, 0.0000448983002823],\n",
    "                                [0.0000017328561662, 17.1582865432904406, -0.0000025255958988],\n",
    "                                [0.0000367952301136, -0.0000015683987433, 10.2820259071568429]])\n",
    "    \n",
    "    # Initial guess for the positions\n",
    "    #initial_positions = np.random.rand(len(nodes) * 3)  # Initialize all points at origin, 1D array\n",
    "    initial_positions = coordinates\n",
    "    solution = np.concatenate([lattice_vectors.ravel(), initial_positions])\n",
    "    \n",
    "    # Function to calculate the squared difference between distances and weights\n",
    "    def objective(solution_attempt, edges, weights):\n",
    "        solution_attempt = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "        \n",
    "        lattice_vectors = solution_attempt[:3]\n",
    "        positions       = solution_attempt[3:]\n",
    "        \n",
    "        errors = 0\n",
    "        for edge, weight in zip(edges, weights):\n",
    "            p1 = positions[edge[0]]\n",
    "            p2 = positions[edge[1]]\n",
    "            \n",
    "            trial_errors = [] \n",
    "            for i in [-1, 0, 1]:\n",
    "                for j in [-1, 0, 1]:\n",
    "                    for k in [-1, 0, 1]:\n",
    "                        # i*lattice_vectors[0] + j*lattice_vectors[1] + k*lattice_vectors[2]\n",
    "                        ijk_lattice_vectors = np.sum([i, j, k] * lattice_vectors.T, axis=1)\n",
    "\n",
    "                        # Compute error\n",
    "                        distance = np.linalg.norm(p2 - p1 + ijk_lattice_vectors)\n",
    "\n",
    "                        # Append to trial errors for differente atom images\n",
    "                        trial_errors.append(np.power(distance - weight, 2))\n",
    "            errors += np.min(trial_errors)\n",
    "        #print(errors)\n",
    "        return errors\n",
    "    \n",
    "    def worst_identification(edges, attributes, solution_attempt):\n",
    "        solution_attempt = solution_attempt.reshape(-1, 3)  # Reshape to 2D array\n",
    "    \n",
    "        lattice_vectors = solution_attempt[:3]\n",
    "        positions       = solution_attempt[3:]\n",
    "    \n",
    "        particle_errors = []\n",
    "        for particle in np.unique(edges):\n",
    "            # Get those edge indexes where particle has a connection\n",
    "            particle_connections = np.where((edges[:, 0] == particle) | (edges[:, 1] == particle))\n",
    "    \n",
    "            particle_error = 0\n",
    "            for idx in particle_connections[0]:\n",
    "                # Load indexes in edge\n",
    "                edge = edges[idx]\n",
    "    \n",
    "                # Load expected attribute\n",
    "                p1 = positions[edge[0]]\n",
    "                p2 = positions[edge[1]]\n",
    "    \n",
    "                # Load reference attribute\n",
    "                weight = attributes[idx].item()\n",
    "    \n",
    "                trial_errors = []\n",
    "                for i in [-1, 0, 1]:\n",
    "                    for j in [-1, 0, 1]:\n",
    "                        for k in [-1, 0, 1]:\n",
    "                            # i*lattice_vectors[0] + j*lattice_vectors[1] + k*lattice_vectors[2]\n",
    "                            ijk_lattice_vectors = np.sum([i, j, k] * lattice_vectors.T, axis=1)\n",
    "    \n",
    "                            # Compute error\n",
    "                            distance = np.linalg.norm(p2 - p1 + ijk_lattice_vectors)\n",
    "    \n",
    "                            # Append to trial errors for different atom images\n",
    "                            trial_errors.append(np.power(distance - weight, 2))\n",
    "    \n",
    "                # Add error\n",
    "                particle_error += np.min(trial_errors)\n",
    "    \n",
    "            # Average over the connection of the node\n",
    "            particle_error /= len(particle_connections[0])\n",
    "    \n",
    "            # Append particle error\n",
    "            particle_errors.append(particle_error)\n",
    "    \n",
    "        return np.argmax(particle_errors), np.max(particle_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d76ec67-05e8-4242-b25a-2a14bccac3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 0\n",
      "Total: 77.48523182259045 and local 0.6329999255632265 errors\n",
      "\n",
      "Attempt 1\n",
      "Total: 77.20066500671535 and local 0.6216631390758556 errors\n",
      "\n",
      "Attempt 2\n",
      "Total: 77.48800982346968 and local 0.623685722475042 errors\n",
      "\n",
      "Attempt 3\n",
      "Total: 77.654806713137 and local 0.6384355329665493 errors\n",
      "\n",
      "Attempt 4\n",
      "Total: 77.71685800819633 and local 0.6358518108203721 errors\n",
      "\n",
      "Attempt 5\n",
      "Total: 77.66796971437712 and local 0.6345372153555331 errors\n",
      "\n",
      "Attempt 6\n",
      "Total: 77.71496010016631 and local 0.6370539971236819 errors\n",
      "\n",
      "Attempt 7\n",
      "Total: 77.64962428337947 and local 0.6351606462728148 errors\n",
      "\n",
      "Attempt 8\n",
      "Total: 77.7106664910225 and local 0.634845927850386 errors\n",
      "\n",
      "Attempt 9\n",
      "Total: 77.64965815162219 and local 0.6351494340008969 errors\n",
      "\n",
      "Attempt 10\n",
      "Total: 77.70850764448971 and local 0.634938034672591 errors\n",
      "\n",
      "Attempt 11\n",
      "Total: 77.6499193477986 and local 0.6351016986188665 errors\n",
      "\n",
      "Attempt 12\n",
      "Total: 77.7068970956085 and local 0.6349929058173133 errors\n",
      "\n",
      "Attempt 13\n",
      "Total: 77.65581856607565 and local 0.6346937472312821 errors\n",
      "\n",
      "Attempt 14\n",
      "Total: 77.23364927745676 and local 0.6209936516148377 errors\n",
      "\n",
      "Attempt 15\n",
      "Total: 77.66435773149001 and local 0.6364145552603998 errors\n",
      "\n",
      "Attempt 16\n",
      "Total: 77.22761084989668 and local 0.621539816943178 errors\n",
      "\n",
      "Attempt 17\n",
      "Total: 77.78581441099747 and local 0.6388987079819121 errors\n",
      "\n",
      "Attempt 18\n",
      "Total: 77.23216567942568 and local 0.6215146635677613 errors\n",
      "\n",
      "Attempt 19\n",
      "Total: 77.75203461509551 and local 0.6376262241248407 errors\n",
      "\n",
      "Attempt 20\n",
      "Total: 77.23075070320508 and local 0.6203575155572507 errors\n",
      "\n",
      "Attempt 21\n",
      "Total: 77.23996822319292 and local 0.6209818349634041 errors\n",
      "\n",
      "Attempt 22\n",
      "Total: 77.23397805770344 and local 0.6223164258021417 errors\n",
      "\n",
      "Attempt 23\n",
      "Total: 77.23897741185175 and local 0.623082679694964 errors\n",
      "\n",
      "Attempt 24\n",
      "Total: 77.23974310405632 and local 0.6231457045274585 errors\n",
      "\n",
      "Attempt 25\n",
      "Total: 77.23583021783786 and local 0.6229258422624862 errors\n",
      "\n",
      "Attempt 26\n",
      "Total: 77.23198012327607 and local 0.6227096394374554 errors\n",
      "\n",
      "Attempt 27\n",
      "Total: 77.2315432009366 and local 0.622671647303743 errors\n",
      "\n",
      "Attempt 28\n",
      "Total: 77.22861487115952 and local 0.6224306492617382 errors\n",
      "\n",
      "Attempt 29\n",
      "Total: 77.22656793346543 and local 0.622315030957789 errors\n",
      "\n",
      "Attempt 30\n",
      "Total: 77.22721901500068 and local 0.6227007539471656 errors\n",
      "\n",
      "Attempt 31\n",
      "Total: 77.2261886575452 and local 0.6225852057329649 errors\n",
      "\n",
      "Attempt 32\n",
      "Total: 77.22304927697967 and local 0.6221522630634376 errors\n",
      "\n",
      "Attempt 33\n",
      "Total: 77.23692395524164 and local 0.6245030112806813 errors\n",
      "\n",
      "Attempt 34\n",
      "Total: 77.22334284856228 and local 0.6224411122415814 errors\n",
      "\n",
      "Attempt 35\n",
      "Total: 77.23146273369557 and local 0.6240352977544067 errors\n",
      "\n",
      "Attempt 36\n",
      "Total: 77.22263696647606 and local 0.622149725404234 errors\n",
      "\n",
      "Attempt 37\n",
      "Total: 77.23126643319165 and local 0.6239984038172447 errors\n",
      "\n",
      "Attempt 38\n",
      "Total: 77.23333338955325 and local 0.6242288248470649 errors\n",
      "\n",
      "Attempt 39\n",
      "Total: 77.23342969831214 and local 0.6242541590857638 errors\n",
      "\n",
      "Attempt 40\n",
      "Total: 77.22736866445618 and local 0.6221322063931828 errors\n",
      "\n",
      "Attempt 41\n",
      "Total: 77.22973551874351 and local 0.6239249987517842 errors\n",
      "\n",
      "Attempt 42\n",
      "Total: 77.22707222782454 and local 0.6223764968813933 errors\n",
      "\n",
      "Attempt 43\n",
      "Total: 77.22715594015442 and local 0.6236654645765014 errors\n",
      "\n",
      "Attempt 44\n",
      "Total: 77.22452141589541 and local 0.6221625649978655 errors\n",
      "\n",
      "Attempt 45\n",
      "Total: 77.22485545860056 and local 0.6234102193095769 errors\n",
      "\n",
      "Attempt 46\n",
      "Total: 77.2224823794066 and local 0.6220255511614992 errors\n",
      "\n",
      "Attempt 47\n",
      "Total: 77.22269076988127 and local 0.6231270612506286 errors\n",
      "\n",
      "Attempt 48\n",
      "Total: 77.22492157963356 and local 0.6233843365320885 errors\n",
      "\n",
      "Attempt 49\n",
      "Total: 77.22408018520898 and local 0.6231257540780064 errors\n",
      "\n",
      "Attempt 50\n",
      "Total: 77.22400529166596 and local 0.6231556021644495 errors\n",
      "\n",
      "Attempt 51\n",
      "Total: 77.22340102023131 and local 0.6233209722339974 errors\n",
      "\n",
      "Attempt 52\n",
      "Total: 77.22260518283785 and local 0.623160770542293 errors\n",
      "\n",
      "Attempt 53\n",
      "Total: 77.22065264294983 and local 0.6228777086227894 errors\n",
      "\n",
      "Attempt 54\n",
      "Total: 77.22039111005138 and local 0.6226058922887392 errors\n",
      "\n",
      "Attempt 55\n",
      "Total: 77.21959160999526 and local 0.6227113245256203 errors\n",
      "\n",
      "Attempt 56\n",
      "Total: 77.22040052776514 and local 0.6228274416697906 errors\n",
      "\n",
      "Attempt 57\n",
      "Total: 77.2202095638207 and local 0.6227024508780431 errors\n",
      "\n",
      "Attempt 58\n",
      "Total: 77.2204760158913 and local 0.6227353784082863 errors\n",
      "\n",
      "Attempt 59\n",
      "Total: 77.21919197943247 and local 0.6225417603919607 errors\n",
      "\n",
      "Attempt 60\n",
      "Total: 77.21864717191812 and local 0.6223875452546291 errors\n",
      "\n",
      "Attempt 61\n",
      "Total: 77.21935715527364 and local 0.622761979958062 errors\n",
      "\n",
      "Attempt 62\n",
      "Total: 77.2181505417686 and local 0.6223437743920672 errors\n",
      "\n",
      "Attempt 63\n",
      "Total: 77.21781408705904 and local 0.6223108483736874 errors\n",
      "\n",
      "Attempt 64\n",
      "Total: 77.21695967374463 and local 0.6222761057178423 errors\n",
      "\n",
      "Attempt 65\n",
      "Total: 77.21684465987656 and local 0.622263346486675 errors\n",
      "\n",
      "Attempt 66\n",
      "Total: 77.2159591043942 and local 0.6221033808712598 errors\n",
      "\n",
      "Attempt 67\n",
      "Total: 77.21591301251969 and local 0.6218891823881926 errors\n",
      "\n",
      "Attempt 68\n",
      "Total: 77.21582480426166 and local 0.622108032183879 errors\n",
      "\n",
      "Attempt 69\n",
      "Total: 77.21534460202176 and local 0.6220078622564479 errors\n",
      "\n",
      "Attempt 70\n",
      "Total: 77.21556244103613 and local 0.6220652291287596 errors\n",
      "\n",
      "Attempt 71\n",
      "Total: 77.2156590246594 and local 0.6219598553686105 errors\n",
      "\n",
      "Attempt 72\n",
      "Total: 77.21528296819538 and local 0.6218204564863017 errors\n",
      "\n",
      "Attempt 73\n",
      "Total: 77.2147707785198 and local 0.6217803535592794 errors\n",
      "\n",
      "Attempt 74\n",
      "Total: 77.2147492468932 and local 0.6218652082254417 errors\n",
      "\n",
      "Attempt 75\n",
      "Total: 77.21466986292097 and local 0.6218577927287688 errors\n",
      "\n",
      "Attempt 76\n",
      "Total: 77.21457468764835 and local 0.6219001099509338 errors\n",
      "\n",
      "Attempt 77\n",
      "Total: 77.21425660172179 and local 0.6217822739258342 errors\n",
      "\n",
      "Attempt 78\n",
      "Total: 77.21410485849263 and local 0.6217185842298046 errors\n",
      "\n",
      "Attempt 79\n",
      "Total: 77.21404686488773 and local 0.621771723669538 errors\n",
      "\n",
      "Attempt 80\n",
      "Total: 77.21371843257302 and local 0.6216930789450487 errors\n",
      "\n",
      "Attempt 81\n",
      "Total: 77.2134985359525 and local 0.6215128206618032 errors\n",
      "\n",
      "Attempt 82\n",
      "Total: 77.21341204442922 and local 0.6213504305815242 errors\n",
      "\n",
      "Attempt 83\n",
      "Total: 77.21344849686702 and local 0.6215238778415726 errors\n",
      "\n",
      "Attempt 84\n",
      "Total: 77.2127944079336 and local 0.6212821726932323 errors\n",
      "\n",
      "Attempt 85\n",
      "Total: 77.21360698391469 and local 0.6214764206048949 errors\n",
      "\n",
      "Attempt 86\n",
      "Total: 77.21303176043763 and local 0.6213510680314298 errors\n",
      "\n",
      "Attempt 87\n",
      "Total: 77.21301540081551 and local 0.6213368333277173 errors\n",
      "\n",
      "Attempt 88\n",
      "Total: 77.21299529138088 and local 0.6214098885417673 errors\n",
      "\n",
      "Attempt 89\n",
      "Total: 77.21264191183472 and local 0.6212158017164611 errors\n",
      "\n",
      "Attempt 90\n",
      "Total: 77.21277168535588 and local 0.6213907359206094 errors\n",
      "\n",
      "Attempt 91\n",
      "Total: 77.21259006056192 and local 0.621356678217842 errors\n",
      "\n",
      "Attempt 92\n",
      "Total: 77.21290375767256 and local 0.6213202989162935 errors\n",
      "\n",
      "Attempt 93\n",
      "Total: 77.21247169735784 and local 0.621096024990938 errors\n",
      "\n",
      "Attempt 94\n",
      "Total: 77.21247867711689 and local 0.6212598393172937 errors\n",
      "\n",
      "Attempt 95\n",
      "Total: 77.21245505696905 and local 0.6213342222916813 errors\n",
      "\n",
      "Attempt 96\n",
      "Total: 77.21219786168912 and local 0.6212008646842926 errors\n",
      "\n",
      "Attempt 97\n",
      "Total: 77.2121340230923 and local 0.6208908567244557 errors\n",
      "\n",
      "Attempt 98\n",
      "Total: 77.21220544643236 and local 0.62112195651289 errors\n",
      "\n",
      "Attempt 99\n",
      "Total: 77.21222632191025 and local 0.6212138512075989 errors\n",
      "Converged to a solution.\n"
     ]
    }
   ],
   "source": [
    "error_threshold = 1e-5\n",
    "\n",
    "for attempt in range(100):\n",
    "    print()\n",
    "    print(f'Attempt {attempt}')\n",
    "    solution = minimize(objective, solution,\n",
    "                        args=(edges, weights),\n",
    "                        method='Powell')\n",
    "\n",
    "    is_success       = solution.success\n",
    "    solution_message = solution.message\n",
    "    worst_particle, worst_error = worst_identification(edges, weights, solution.x)\n",
    "\n",
    "    attempt_error = objective(solution.x, edges, weights)\n",
    "    print(f'Total: {attempt_error} and local {worst_error} errors')\n",
    "\n",
    "    if attempt_error < error_threshold:\n",
    "        break\n",
    "\n",
    "    solution = solution.x.reshape(-1, 3)  # Reshape to 2D array\n",
    "\n",
    "    # Re-initialize that position\n",
    "    if is_molecule:\n",
    "        solution[worst_particle] = np.random.rand(3)\n",
    "    else:\n",
    "        solution[worst_particle+3] = np.random.rand(3)\n",
    "\n",
    "    solution = solution.flatten()\n",
    "\n",
    "# Check convergence status\n",
    "if is_success:\n",
    "    print('Converged to a solution.')\n",
    "else:\n",
    "    print(f'Failed to converge: {solution_message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "515f6a4d-8542-4695-866c-9734d739f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solution.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c66525-8a75-4c42-9a2a-a9edf135d70c",
   "metadata": {},
   "source": [
    "## Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74512108-9c89-46c6-a60c-a3f1cd5d487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)\n",
    "    \n",
    "    lattice_vectors     = np.array([[10,  0,   0],\n",
    "                                    [0,   10,  0],\n",
    "                                    [0,   0,   10]])\n",
    "    cartesian_positions = solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292e640-09fc-4593-9782-6b022dc1f860",
   "metadata": {},
   "source": [
    "## Crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c905edb6-ba68-41f3-927d-cd0fc21fee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_molecule:\n",
    "    # Get the position of each atom in direct coordinates\n",
    "    #direct_positions = graph_to_cartesian_positions(graph)\n",
    "    #cartesian_positions = solution.x.reshape(-1, 3)*mw\n",
    "    \n",
    "    lattice_vectors     = solution[:3]\n",
    "    cartesian_positions = solution[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8caa7ae4-9a1d-4580-8e6e-9b5135dbe260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.1365,  3.8249,  2.4776, 12.7807],\n",
       "        [ 7.0138,  3.7843,  2.4891, 12.8251],\n",
       "        [ 6.9057,  3.7339,  2.4783, 12.8256],\n",
       "        [ 7.0122,  3.7737,  2.4817, 12.7812],\n",
       "        [ 6.8408,  3.7007,  2.4785, 12.8339],\n",
       "        [ 6.8943,  3.7361,  2.4786, 12.8249],\n",
       "        [ 6.8747,  3.7351,  2.4760, 12.8615],\n",
       "        [ 6.9799,  3.7713,  2.4803, 12.8099],\n",
       "        [ 7.0315,  3.7640,  2.4816, 12.7987],\n",
       "        [ 7.0216,  3.7837,  2.4838, 12.8142],\n",
       "        [ 6.8890,  3.7358,  2.4774, 12.8242],\n",
       "        [ 6.9740,  3.7701,  2.4817, 12.8139],\n",
       "        [ 6.8881,  3.7048,  2.4775, 12.8413],\n",
       "        [ 6.9214,  3.7429,  2.4763, 12.8331],\n",
       "        [ 6.9967,  3.7504,  2.4828, 12.8152],\n",
       "        [ 6.9247,  3.7348,  2.4798, 12.8254],\n",
       "        [ 7.0200,  3.7788,  2.4806, 12.8384],\n",
       "        [ 6.9106,  3.7369,  2.4812, 12.8187]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_graph.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0709aff-bf63-4f99-ab14-e8bcabac3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSCAR_name = None\n",
    "\n",
    "# Get name for the first line of the POSCAR\n",
    "POSCAR_name = POSCAR_name or 'POSCAR from GenerativeModels'\n",
    "\n",
    "# Clone the input graph to preserve the original structure\n",
    "new_graph = temp.clone()\n",
    "\n",
    "# Load and detach embeddings for the graph nodes\n",
    "data_embeddings = new_graph.x.detach().cpu().numpy()\n",
    "\n",
    "# Loading dictionary of available embeddings for atoms\n",
    "available_embeddings = {}\n",
    "with open('../MP/input/atomic_masses.dat', 'r') as atomic_masses_file:\n",
    "    for line in atomic_masses_file:\n",
    "        key, mass, charge, electronegativity, ionization_energy = line.split()\n",
    "\n",
    "        # Check if all information is present\n",
    "        if all(val != 'None' for val in (mass, charge, electronegativity, ionization_energy)):\n",
    "            available_embeddings[key] = np.array([mass, charge, electronegativity, ionization_energy], dtype=float)\n",
    "\n",
    "# Get most similar atoms for each graph node and create a list of keys\n",
    "keys = [glg.find_closest_key(available_embeddings, emb) for emb in data_embeddings]\n",
    "\n",
    "# Get elements' composition, concentration, and positions\n",
    "POSCAR_composition, POSCAR_concentration, POSCAR_positions = glg.composition_concentration_from_keys(keys, cartesian_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4065eb29-e8e9-4530-a68a-dd5bf8d7dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write file\n",
    "with open('CONTCAR', 'w') as POSCAR_file:\n",
    "    # Delete previous data in the file\n",
    "    POSCAR_file.truncate()\n",
    "    \n",
    "    # Write POSCAR's name\n",
    "    POSCAR_file.write(f'{POSCAR_name}\\n')\n",
    "\n",
    "    # Write scaling factor (assumed to be 1.0)\n",
    "    POSCAR_file.write('1.0\\n')\n",
    "\n",
    "    # Write lattice parameters (assumed to be orthogonal)\n",
    "    np.savetxt(POSCAR_file, lattice_vectors, delimiter=' ')\n",
    "\n",
    "    # Write composition (each different species, previously sorted)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_composition], fmt='%s', delimiter=' ')\n",
    "\n",
    "    # Write concentration (number of each of the previous elements)\n",
    "    np.savetxt(POSCAR_file, [POSCAR_concentration], fmt='%d', delimiter=' ')\n",
    "\n",
    "    # Write position in cartesian form\n",
    "    POSCAR_file.write('Cartesian\\n')\n",
    "    np.savetxt(POSCAR_file, POSCAR_positions, delimiter=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
