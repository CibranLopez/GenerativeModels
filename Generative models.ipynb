{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import networkx    as nx\n",
    "import GM_library  as GML\n",
    "import torch\n",
    "\n",
    "from torch.utils.data       import random_split\n",
    "from torch_geometric.utils  import convert\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn     import GraphConv, Linear\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import MP.MP_library as MPL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a93ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell, composition, concentration, positions = MPL.information_from_VASPfile('../MP/Loaded_PHONON/mp-3332-20180417',\n",
    "                                                                            'POSCAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92863dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [15, 15, 15]\n",
    "particle_types, atomic_masses, charges, electronegativities, ionization_energies = GML.graph_POSCAR_encoding(cell, composition, concentration, positions, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf372c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GM_library' from '/Users/cibran/Work/UPC/GenerativeModels/GM_library.py'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de656247",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes, all_positions = GML.get_atoms_in_box(particle_types,\n",
    "                                                composition,\n",
    "                                                cell,\n",
    "                                                atomic_masses,\n",
    "                                                charges,\n",
    "                                                electronegativities,\n",
    "                                                ionization_energies,\n",
    "                                                positions,\n",
    "                                                L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes, all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, attributes = GML.get_edges_in_box(nodes, all_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7e0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs      = 1000\n",
    "batch_size    = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Number of diffusing and denoising steps, which can be different\n",
    "n_diffusing_steps = 10\n",
    "n_denoising_steps = 10\n",
    "\n",
    "# Dropouts for node and edge models (work independently)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Target to generate new crystals\n",
    "target = 'GM_D'\n",
    "\n",
    "input_folder  = 'models'\n",
    "target_folder = f'{input_folder}/{target}'\n",
    "model_name    = f'{target_folder}/model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d59a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "if path.exists(dataset_name_std) and path.exists(labels_name) and path.exists(parameters_name_std):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "\n",
    "elif path.exists(dataset_name) and path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)\n",
    "\n",
    "else:\n",
    "    # Generate the raw dataset from scratch, and standardize it\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c0cbf",
   "metadata": {},
   "source": [
    "# Generation of diffusing and denoising Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a554a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GM-library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449ee41",
   "metadata": {},
   "source": [
    "# Generation of Graph Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bb07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GM-library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e59715",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859c2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 171\n",
      "Number of testing  graphs: 43\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1aa0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nodes:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(5, 512)\n",
      "  (conv2): GraphConv(512, 512)\n",
      "  (linconv): Linear(512, 16, bias=True)\n",
      "  (lin): Linear(16, 1, bias=True)\n",
      ")\n",
      "\n",
      "Edges:\n",
      "eGCNN(\n",
      "  (linear1): Linear(100, 64, bias=True)\n",
      "  (linear2): Linear(64, 64, bias=True)\n",
      "  (linear3): Linear(64, 100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset\n",
    "n_features = dataset[0].num_node_features\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = MPL.nGCNN(n_features, dropout_node).to(device)\n",
    "edge_model = MPL.eGCNN(n_features, dropout_edge).to(device)\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_node = nn.MSELoss()\n",
    "criterion_edge = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    for graph in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Diffuse the graph with some noise\n",
    "        diffused_graph = MPL.diffuse(graph, n_diffusing_steps)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        denoised_graph = diffused_graph.clone()\n",
    "        for t in range(n_denoising_steps):\n",
    "            # Perform a single forward pass for predicting node features\n",
    "            out_x = node_model(diffused_graph.x, \n",
    "                               diffused_graph.edge_index,\n",
    "                               diffused_graph.edge_attr)\n",
    "            \n",
    "            # Define edges\n",
    "            # DEFINE x_i and x_j as connected nodes\n",
    "            \n",
    "            # Perform a single forward pass for predicting edge attributes\n",
    "            out_attr = edge_model(x_i, \n",
    "                                  x_j)\n",
    "\n",
    "            # Construct noise graph\n",
    "            noise_graph = Data(x=out_x, edge_index=diffused_graph.edge_index, edge_attr=out_attr)\n",
    "\n",
    "            # Denoise the graph with the predicted noise\n",
    "            denoised_graph = MPL.denoising_step(diffused_graph, noise_graph, t, n_denoising_steps)\n",
    "\n",
    "        # Calculate the loss for node features\n",
    "        loss_node = criterion_node(graph.x, denoised_graph.x)\n",
    "\n",
    "        # Calculate the loss for edge attributes\n",
    "        loss_edge = criterion_edge(graph.edge_attr, denoised_graph.edge_attr)\n",
    "        \n",
    "        \n",
    "        ### I would independtly check node and edge losses\n",
    "        \n",
    "        \n",
    "        # Accumulate the total training loss\n",
    "        loss = loss_node + loss_edge\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
