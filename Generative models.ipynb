{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data       import random_split\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn     import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs      = 1000\n",
    "batch_size    = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "target = 'D'\n",
    "\n",
    "input_folder  = 'models'\n",
    "target_folder = f'{input_folder}/{target}'\n",
    "model_name    = f'{target_folder}/model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d59a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "try:    \n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "except FileNotFoundError:\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    ### Santadirizing properties\n",
    "\n",
    "    # Compute means and standard deviations\n",
    "\n",
    "    target_list = torch.tensor([])\n",
    "    edge_list   = torch.tensor([])\n",
    "\n",
    "    for data in dataset:\n",
    "        target_list = torch.cat((target_list, data.y),         0)\n",
    "        edge_list   = torch.cat((edge_list,   data.edge_attr), 0)\n",
    "\n",
    "    scale = 1e0\n",
    "\n",
    "    target_mean = torch.mean(target_list)\n",
    "    target_std  = torch.std(target_list)\n",
    "\n",
    "    edge_mean = torch.mean(edge_list)\n",
    "    edge_std  = torch.std(edge_list)\n",
    "\n",
    "    target_factor = target_std / scale\n",
    "    edge_factor   = edge_std   / scale\n",
    "\n",
    "    # Update normalized values into the database\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y         = (data.y         - target_mean) / target_factor\n",
    "        data.edge_attr = (data.edge_attr - edge_mean)   / edge_factor\n",
    "\n",
    "    # Same for the node features\n",
    "\n",
    "    feat_mean = torch.tensor([])\n",
    "    feat_std  = torch.tensor([])\n",
    "\n",
    "    for feat_index in range(dataset[0].num_node_features):\n",
    "        feat_list = torch.tensor([])\n",
    "\n",
    "        for data in dataset:\n",
    "            feat_list = torch.cat((feat_list, data.x[:, feat_index]), 0)\n",
    "\n",
    "        feat_mean = torch.cat((feat_mean, torch.tensor([torch.mean(feat_list)])), 0)\n",
    "        feat_std  = torch.cat((feat_std,  torch.tensor([torch.std(feat_list)])),  0)\n",
    "\n",
    "        for data in dataset:\n",
    "            data.x[:, feat_index] = (data.x[:, feat_index] - feat_mean[feat_index]) * scale / feat_std[feat_index]\n",
    "    \n",
    "    parameters = [target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale]\n",
    "    \n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c0cbf",
   "metadata": {},
   "source": [
    "# Generation of Graph Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27732360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffusion(torch.nn.Module):  # Diffusion\n",
    "    \"\"\"Graph generative denoising neural network with edge diffusion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_channels, pdropout):\n",
    "        \"\"\"Instantiate constants for the class.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(diffusion, self).__init__()\n",
    "        \n",
    "        self.conv1 = GraphConv(features_channels, 64)\n",
    "        self.conv2 = GraphConv(64, 64)\n",
    "        self.conv3 = GraphConv(64, features_channels)\n",
    "        \n",
    "        self.pdropout = pdropout\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, alpha_t):\n",
    "        \"\"\"Diffusion process: applies a Markov chain to the input graph, to diffuse it in T steps.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate random t, epsilon\n",
    "        \n",
    "        epsilon = random graph\n",
    "        # Define sigma parameter\n",
    "        sigma_t = torch.sqrt(1 - alpha_t**2)\n",
    "        \n",
    "        return x, edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3bd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(torch.nn.Module):  # Denoising\n",
    "    \"\"\"Graph generative denoising neural network with edge diffusion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_channels, pdropout):\n",
    "        \"\"\"Instantiate constants for the class.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DDPM, self).__init__()\n",
    "        \n",
    "        self.conv1 = GraphConv(features_channels, 64)\n",
    "        self.conv2 = GraphConv(64, 64)\n",
    "        self.conv3 = GraphConv(64, features_channels)\n",
    "        \n",
    "        self.pdropout = pdropout\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"Denoising process: predicts the original graph from the noised one.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Denoising process through deep mesh\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        return x, edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b6d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 204\n",
      "Number of testing  graphs: 23\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85adc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,        shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de3f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset and data loader\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = DDPM(features_channels=dataset[0].num_node_features,\n",
    "             pdropout=0.4)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394a76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 10347380.0000, Test Loss: 6194468.0000\n",
      "Epoch: 2, Train Loss: 7515795.7500, Test Loss: 4521819.5000\n",
      "Epoch: 3, Train Loss: 5812716.5000, Test Loss: 3742563.2500\n",
      "Epoch: 4, Train Loss: 4134142.1250, Test Loss: 3624987.5000\n",
      "Epoch: 5, Train Loss: 3188670.2500, Test Loss: 3933710.0000\n",
      "Epoch: 6, Train Loss: 2333513.5000, Test Loss: 4425003.0000\n",
      "Epoch: 7, Train Loss: 1979343.3750, Test Loss: 4904834.0000\n",
      "Epoch: 8, Train Loss: 1802134.3750, Test Loss: 5150466.5000\n",
      "Epoch: 9, Train Loss: 1519395.8750, Test Loss: 5098913.5000\n",
      "Epoch: 10, Train Loss: 1401079.4375, Test Loss: 4901725.5000\n",
      "Epoch: 11, Train Loss: 1358810.5938, Test Loss: 4481043.0000\n",
      "Epoch: 12, Train Loss: 1070170.9375, Test Loss: 3909091.0000\n",
      "Epoch: 13, Train Loss: 934936.2812, Test Loss: 3336404.5000\n",
      "Epoch: 14, Train Loss: 832634.3125, Test Loss: 2781900.2500\n",
      "Epoch: 15, Train Loss: 744776.2188, Test Loss: 2278548.7500\n",
      "Epoch: 16, Train Loss: 739187.5781, Test Loss: 1844393.3750\n",
      "Epoch: 17, Train Loss: 578549.1875, Test Loss: 1463000.1250\n",
      "Epoch: 18, Train Loss: 478588.4219, Test Loss: 1169982.3750\n",
      "Epoch: 19, Train Loss: 473721.6562, Test Loss: 942550.9375\n",
      "Epoch: 20, Train Loss: 359918.3594, Test Loss: 766315.3125\n",
      "Epoch: 21, Train Loss: 365967.6094, Test Loss: 636049.6875\n",
      "Epoch: 22, Train Loss: 252715.6406, Test Loss: 542019.1875\n",
      "Epoch: 23, Train Loss: 247669.9531, Test Loss: 473619.0312\n",
      "Epoch: 24, Train Loss: 225864.5703, Test Loss: 421670.0625\n",
      "Epoch: 25, Train Loss: 223579.8828, Test Loss: 387418.9375\n",
      "Epoch: 26, Train Loss: 185836.1562, Test Loss: 359991.4688\n",
      "Epoch: 27, Train Loss: 167680.1094, Test Loss: 337918.3438\n",
      "Epoch: 28, Train Loss: 161086.0078, Test Loss: 316330.3125\n",
      "Epoch: 29, Train Loss: 135160.1914, Test Loss: 290834.1250\n",
      "Epoch: 30, Train Loss: 137873.6875, Test Loss: 268730.3438\n",
      "Epoch: 31, Train Loss: 124851.6484, Test Loss: 244938.6250\n",
      "Epoch: 32, Train Loss: 118592.8438, Test Loss: 225074.0781\n",
      "Epoch: 33, Train Loss: 104065.1289, Test Loss: 204776.2812\n",
      "Epoch: 34, Train Loss: 94989.9102, Test Loss: 188773.7500\n",
      "Epoch: 35, Train Loss: 94946.2852, Test Loss: 176197.1875\n",
      "Epoch: 36, Train Loss: 92471.8516, Test Loss: 166694.6094\n",
      "Epoch: 37, Train Loss: 81340.4746, Test Loss: 158158.1719\n",
      "Epoch: 38, Train Loss: 87066.1074, Test Loss: 151040.4688\n",
      "Epoch: 39, Train Loss: 81648.3418, Test Loss: 145090.6719\n",
      "Epoch: 40, Train Loss: 74248.8555, Test Loss: 138934.9219\n",
      "Epoch: 41, Train Loss: 76597.4492, Test Loss: 133523.1406\n",
      "Epoch: 42, Train Loss: 69404.8750, Test Loss: 128436.9453\n",
      "Epoch: 43, Train Loss: 73816.3320, Test Loss: 123548.2500\n",
      "Epoch: 44, Train Loss: 58691.1582, Test Loss: 118773.1250\n",
      "Epoch: 45, Train Loss: 59503.9062, Test Loss: 114479.7500\n",
      "Epoch: 46, Train Loss: 62052.9902, Test Loss: 110839.4141\n",
      "Epoch: 47, Train Loss: 60987.4102, Test Loss: 107668.2109\n",
      "Epoch: 48, Train Loss: 55067.7148, Test Loss: 104881.9688\n",
      "Epoch: 49, Train Loss: 56206.3203, Test Loss: 102698.4844\n",
      "Epoch: 50, Train Loss: 53451.3242, Test Loss: 100721.4375\n",
      "Epoch: 51, Train Loss: 56546.5859, Test Loss: 99179.2188\n",
      "Epoch: 52, Train Loss: 52748.4297, Test Loss: 97545.2891\n",
      "Epoch: 53, Train Loss: 49922.9258, Test Loss: 96126.8672\n",
      "Epoch: 54, Train Loss: 50670.5547, Test Loss: 94883.8516\n",
      "Epoch: 55, Train Loss: 47889.9199, Test Loss: 93348.8750\n",
      "Epoch: 56, Train Loss: 46670.2246, Test Loss: 92045.5703\n",
      "Epoch: 57, Train Loss: 48580.4805, Test Loss: 90969.8594\n",
      "Epoch: 58, Train Loss: 46182.2109, Test Loss: 89756.7109\n",
      "Epoch: 59, Train Loss: 43970.9902, Test Loss: 88305.3203\n",
      "Epoch: 60, Train Loss: 44611.9531, Test Loss: 87136.5391\n",
      "Epoch: 61, Train Loss: 41004.7188, Test Loss: 85830.0000\n",
      "Epoch: 62, Train Loss: 41352.5527, Test Loss: 84670.0078\n",
      "Epoch: 63, Train Loss: 43278.2383, Test Loss: 83641.3906\n",
      "Epoch: 64, Train Loss: 45317.3564, Test Loss: 82431.7656\n",
      "Epoch: 65, Train Loss: 40034.7695, Test Loss: 81125.6953\n",
      "Epoch: 66, Train Loss: 38165.1973, Test Loss: 79685.9062\n",
      "Epoch: 67, Train Loss: 35161.7754, Test Loss: 78475.5859\n",
      "Epoch: 68, Train Loss: 36755.9746, Test Loss: 77427.6328\n",
      "Epoch: 69, Train Loss: 35599.1211, Test Loss: 76578.7734\n",
      "Epoch: 70, Train Loss: 34710.9941, Test Loss: 75787.8672\n",
      "Epoch: 71, Train Loss: 33561.7412, Test Loss: 75114.6641\n",
      "Epoch: 72, Train Loss: 39001.1064, Test Loss: 74607.6484\n",
      "Epoch: 73, Train Loss: 35139.5400, Test Loss: 73861.1016\n",
      "Epoch: 74, Train Loss: 33610.5967, Test Loss: 72839.4062\n",
      "Epoch: 75, Train Loss: 31224.4023, Test Loss: 71983.7188\n",
      "Epoch: 76, Train Loss: 29084.2061, Test Loss: 71244.8516\n",
      "Epoch: 77, Train Loss: 33113.3047, Test Loss: 70608.3203\n",
      "Epoch: 78, Train Loss: 29950.7295, Test Loss: 69682.4219\n",
      "Epoch: 79, Train Loss: 29420.9893, Test Loss: 68888.7500\n",
      "Epoch: 80, Train Loss: 29765.5850, Test Loss: 68221.2344\n",
      "Epoch: 81, Train Loss: 28129.0918, Test Loss: 67582.7500\n",
      "Epoch: 82, Train Loss: 32282.4395, Test Loss: 67073.1406\n",
      "Epoch: 83, Train Loss: 28830.4736, Test Loss: 66285.5156\n",
      "Epoch: 84, Train Loss: 27618.0703, Test Loss: 65274.5078\n",
      "Epoch: 85, Train Loss: 25310.0928, Test Loss: 64412.8633\n",
      "Epoch: 86, Train Loss: 24847.0156, Test Loss: 63660.1133\n",
      "Epoch: 87, Train Loss: 25843.0957, Test Loss: 62927.3789\n",
      "Epoch: 88, Train Loss: 26622.1025, Test Loss: 62332.5273\n",
      "Epoch: 89, Train Loss: 25376.2031, Test Loss: 61832.8945\n",
      "Epoch: 90, Train Loss: 28971.2012, Test Loss: 61464.5000\n",
      "Epoch: 91, Train Loss: 25257.6211, Test Loss: 60796.2227\n",
      "Epoch: 92, Train Loss: 22767.0767, Test Loss: 60205.4766\n",
      "Epoch: 93, Train Loss: 24072.4932, Test Loss: 59608.2695\n",
      "Epoch: 94, Train Loss: 25666.4258, Test Loss: 59146.9844\n",
      "Epoch: 95, Train Loss: 25338.4463, Test Loss: 58415.5156\n",
      "Epoch: 96, Train Loss: 23302.5029, Test Loss: 57374.1250\n",
      "Epoch: 97, Train Loss: 23061.3535, Test Loss: 56486.6758\n",
      "Epoch: 98, Train Loss: 23924.3984, Test Loss: 55840.2031\n",
      "Epoch: 99, Train Loss: 22323.0186, Test Loss: 55082.7148\n",
      "Epoch: 100, Train Loss: 23427.2275, Test Loss: 54576.0664\n",
      "Epoch: 101, Train Loss: 20061.6377, Test Loss: 53865.2734\n",
      "Epoch: 102, Train Loss: 21921.7021, Test Loss: 53223.2344\n",
      "Epoch: 103, Train Loss: 19628.8320, Test Loss: 52734.7578\n",
      "Epoch: 104, Train Loss: 21616.0996, Test Loss: 52297.8672\n",
      "Epoch: 105, Train Loss: 20273.6665, Test Loss: 51931.9766\n",
      "Epoch: 106, Train Loss: 20225.7676, Test Loss: 51600.2773\n",
      "Epoch: 107, Train Loss: 19950.2041, Test Loss: 51339.1445\n",
      "Epoch: 108, Train Loss: 19868.9180, Test Loss: 51028.5391\n",
      "Epoch: 109, Train Loss: 19989.2109, Test Loss: 50764.1797\n",
      "Epoch: 110, Train Loss: 19595.2573, Test Loss: 50458.7812\n",
      "Epoch: 111, Train Loss: 19529.2158, Test Loss: 50130.1289\n",
      "Epoch: 112, Train Loss: 21070.0234, Test Loss: 49854.9414\n",
      "Epoch: 113, Train Loss: 21112.6924, Test Loss: 49274.0820\n",
      "Epoch: 114, Train Loss: 18311.8403, Test Loss: 48448.0078\n",
      "Epoch: 115, Train Loss: 18152.0908, Test Loss: 47780.0352\n",
      "Epoch: 116, Train Loss: 18025.0752, Test Loss: 47181.2422\n",
      "Epoch: 117, Train Loss: 20269.8921, Test Loss: 46731.8789\n",
      "Epoch: 118, Train Loss: 20336.9189, Test Loss: 46096.9727\n",
      "Epoch: 119, Train Loss: 17949.8896, Test Loss: 45321.0586\n",
      "Epoch: 120, Train Loss: 17727.0117, Test Loss: 44726.3047\n",
      "Epoch: 121, Train Loss: 19054.6040, Test Loss: 44335.9609\n",
      "Epoch: 122, Train Loss: 19106.1655, Test Loss: 43865.0312\n",
      "Epoch: 123, Train Loss: 17154.9004, Test Loss: 43310.5977\n",
      "Epoch: 124, Train Loss: 19110.0913, Test Loss: 42906.2500\n",
      "Epoch: 125, Train Loss: 16873.5015, Test Loss: 42368.8594\n",
      "Epoch: 126, Train Loss: 16296.7827, Test Loss: 41970.2461\n",
      "Epoch: 127, Train Loss: 18120.3706, Test Loss: 41721.2617\n",
      "Epoch: 128, Train Loss: 18152.7354, Test Loss: 41344.3750\n",
      "Epoch: 129, Train Loss: 18287.1387, Test Loss: 40776.6094\n",
      "Epoch: 130, Train Loss: 17092.5024, Test Loss: 40184.2422\n",
      "Epoch: 131, Train Loss: 15784.7266, Test Loss: 39408.1562\n",
      "Epoch: 132, Train Loss: 16268.0005, Test Loss: 38887.0312\n",
      "Epoch: 133, Train Loss: 16676.8198, Test Loss: 38577.8789\n",
      "Epoch: 134, Train Loss: 15405.8892, Test Loss: 38429.2930\n",
      "Epoch: 135, Train Loss: 15129.2700, Test Loss: 38387.0820\n",
      "Epoch: 136, Train Loss: 15240.3452, Test Loss: 38365.9883\n",
      "Epoch: 137, Train Loss: 14493.5435, Test Loss: 38362.7461\n",
      "Epoch: 138, Train Loss: 15204.2549, Test Loss: 38293.2539\n",
      "Epoch: 139, Train Loss: 15153.9541, Test Loss: 38245.4102\n",
      "Epoch: 140, Train Loss: 16167.9287, Test Loss: 38203.2578\n",
      "Epoch: 141, Train Loss: 14439.1318, Test Loss: 37866.1992\n",
      "Epoch: 142, Train Loss: 14614.3481, Test Loss: 37538.4492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143, Train Loss: 14665.8042, Test Loss: 37237.3672\n",
      "Epoch: 144, Train Loss: 15557.9761, Test Loss: 36983.5547\n",
      "Epoch: 145, Train Loss: 16041.5518, Test Loss: 36472.1523\n",
      "Epoch: 146, Train Loss: 15424.4644, Test Loss: 35834.0898\n",
      "Epoch: 147, Train Loss: 13697.2568, Test Loss: 35087.1992\n",
      "Epoch: 148, Train Loss: 14228.5654, Test Loss: 34495.5352\n",
      "Epoch: 149, Train Loss: 13074.4077, Test Loss: 34110.6250\n",
      "Epoch: 150, Train Loss: 14353.8286, Test Loss: 33882.7383\n",
      "Epoch: 151, Train Loss: 14137.3423, Test Loss: 33809.7578\n",
      "Epoch: 152, Train Loss: 15180.3550, Test Loss: 33797.1250\n",
      "Epoch: 153, Train Loss: 13191.0513, Test Loss: 33627.9258\n",
      "Epoch: 154, Train Loss: 14356.9785, Test Loss: 33528.6953\n",
      "Epoch: 155, Train Loss: 14366.8325, Test Loss: 33263.0938\n",
      "Epoch: 156, Train Loss: 12965.3374, Test Loss: 32773.1289\n",
      "Epoch: 157, Train Loss: 14821.7407, Test Loss: 32379.7129\n",
      "Epoch: 158, Train Loss: 13592.6338, Test Loss: 31861.0684\n",
      "Epoch: 159, Train Loss: 14102.2642, Test Loss: 31233.1250\n",
      "Epoch: 160, Train Loss: 12594.6943, Test Loss: 30564.3789\n",
      "Epoch: 161, Train Loss: 14371.9585, Test Loss: 30096.7383\n",
      "Epoch: 162, Train Loss: 13257.0444, Test Loss: 29675.9805\n",
      "Epoch: 163, Train Loss: 12400.0547, Test Loss: 29203.8086\n",
      "Epoch: 164, Train Loss: 12265.7637, Test Loss: 28945.6172\n",
      "Epoch: 165, Train Loss: 12690.5566, Test Loss: 28791.0762\n",
      "Epoch: 166, Train Loss: 12876.8696, Test Loss: 28803.6211\n",
      "Epoch: 167, Train Loss: 12729.3799, Test Loss: 28766.4082\n",
      "Epoch: 168, Train Loss: 12539.9209, Test Loss: 28598.2266\n",
      "Epoch: 169, Train Loss: 12124.3232, Test Loss: 28507.6504\n",
      "Epoch: 170, Train Loss: 11926.8330, Test Loss: 28474.7207\n",
      "Epoch: 171, Train Loss: 12447.4404, Test Loss: 28538.1855\n",
      "Epoch: 172, Train Loss: 11991.4722, Test Loss: 28353.0898\n",
      "Epoch: 173, Train Loss: 11518.0737, Test Loss: 28240.4570\n",
      "Epoch: 174, Train Loss: 12896.6987, Test Loss: 28158.2773\n",
      "Epoch: 175, Train Loss: 11151.1724, Test Loss: 27904.0156\n",
      "Epoch: 176, Train Loss: 11312.5171, Test Loss: 27679.9863\n",
      "Epoch: 177, Train Loss: 11921.2822, Test Loss: 27509.0332\n",
      "Epoch: 178, Train Loss: 12217.2007, Test Loss: 27419.1992\n",
      "Epoch: 179, Train Loss: 11220.6562, Test Loss: 27124.2598\n",
      "Epoch: 180, Train Loss: 11723.1211, Test Loss: 26956.8809\n",
      "Epoch: 181, Train Loss: 10398.7068, Test Loss: 26629.7207\n",
      "Epoch: 182, Train Loss: 11799.6377, Test Loss: 26389.0703\n",
      "Epoch: 183, Train Loss: 10988.9824, Test Loss: 26037.1035\n",
      "Epoch: 184, Train Loss: 11566.7769, Test Loss: 25828.8223\n",
      "Epoch: 185, Train Loss: 10752.4448, Test Loss: 25520.5586\n",
      "Epoch: 186, Train Loss: 10546.0669, Test Loss: 25310.4785\n",
      "Epoch: 187, Train Loss: 11464.4099, Test Loss: 25192.1973\n",
      "Epoch: 188, Train Loss: 11442.4922, Test Loss: 25013.6875\n",
      "Epoch: 189, Train Loss: 11073.3955, Test Loss: 24730.8652\n",
      "Epoch: 190, Train Loss: 11204.0132, Test Loss: 24365.3105\n",
      "Epoch: 191, Train Loss: 10958.6782, Test Loss: 23958.1426\n",
      "Epoch: 192, Train Loss: 9962.4331, Test Loss: 23512.8027\n",
      "Epoch: 193, Train Loss: 9599.6680, Test Loss: 23217.2344\n",
      "Epoch: 194, Train Loss: 11080.7900, Test Loss: 23034.4414\n",
      "Epoch: 195, Train Loss: 9892.5127, Test Loss: 22803.0586\n",
      "Epoch: 196, Train Loss: 9538.4160, Test Loss: 22695.1406\n",
      "Epoch: 197, Train Loss: 9534.0940, Test Loss: 22650.0117\n",
      "Epoch: 198, Train Loss: 9518.0354, Test Loss: 22655.9297\n",
      "Epoch: 199, Train Loss: 10663.5635, Test Loss: 22719.6465\n",
      "Epoch: 200, Train Loss: 10143.7124, Test Loss: 22669.1758\n",
      "Epoch: 201, Train Loss: 9292.0344, Test Loss: 22463.8457\n",
      "Epoch: 202, Train Loss: 9482.7114, Test Loss: 22328.0703\n",
      "Epoch: 203, Train Loss: 9113.3059, Test Loss: 22250.8926\n",
      "Epoch: 204, Train Loss: 10101.9863, Test Loss: 22234.8574\n",
      "Epoch: 205, Train Loss: 9157.3616, Test Loss: 22088.3066\n",
      "Epoch: 206, Train Loss: 9063.7827, Test Loss: 21982.5859\n",
      "Epoch: 207, Train Loss: 8794.8652, Test Loss: 21904.9805\n",
      "Epoch: 208, Train Loss: 8970.3596, Test Loss: 21829.6230\n",
      "Epoch: 209, Train Loss: 8674.3530, Test Loss: 21766.3340\n",
      "Epoch: 210, Train Loss: 8788.5271, Test Loss: 21675.1680\n",
      "Epoch: 211, Train Loss: 9490.1863, Test Loss: 21579.0215\n",
      "Epoch: 212, Train Loss: 9118.2515, Test Loss: 21316.2930\n",
      "Epoch: 213, Train Loss: 8436.4160, Test Loss: 21113.4043\n",
      "Epoch: 214, Train Loss: 9451.8796, Test Loss: 20954.9004\n",
      "Epoch: 215, Train Loss: 9304.3635, Test Loss: 20687.3145\n",
      "Epoch: 216, Train Loss: 9388.0256, Test Loss: 20343.5391\n",
      "Epoch: 217, Train Loss: 9120.4509, Test Loss: 19969.8809\n",
      "Epoch: 218, Train Loss: 9071.9478, Test Loss: 19593.6875\n",
      "Epoch: 219, Train Loss: 9142.7039, Test Loss: 19237.2402\n",
      "Epoch: 220, Train Loss: 8282.2219, Test Loss: 18882.7969\n",
      "Epoch: 221, Train Loss: 8329.2202, Test Loss: 18665.4941\n",
      "Epoch: 222, Train Loss: 8243.6270, Test Loss: 18603.5879\n",
      "Epoch: 223, Train Loss: 8771.4250, Test Loss: 18615.0098\n",
      "Epoch: 224, Train Loss: 8703.0088, Test Loss: 18588.0762\n",
      "Epoch: 225, Train Loss: 7643.1289, Test Loss: 18470.3008\n",
      "Epoch: 226, Train Loss: 7774.3926, Test Loss: 18367.8711\n",
      "Epoch: 227, Train Loss: 7797.9387, Test Loss: 18307.1074\n",
      "Epoch: 228, Train Loss: 7846.2605, Test Loss: 18276.6211\n",
      "Epoch: 229, Train Loss: 7357.4646, Test Loss: 18282.1641\n",
      "Epoch: 230, Train Loss: 8176.3796, Test Loss: 18291.2129\n",
      "Epoch: 231, Train Loss: 7686.4414, Test Loss: 18143.6895\n",
      "Epoch: 232, Train Loss: 7954.0928, Test Loss: 18050.2676\n",
      "Epoch: 233, Train Loss: 8488.8184, Test Loss: 18048.3809\n",
      "Epoch: 234, Train Loss: 7774.7505, Test Loss: 17913.2754\n",
      "Epoch: 235, Train Loss: 7952.5557, Test Loss: 17870.5566\n",
      "Epoch: 236, Train Loss: 7375.6096, Test Loss: 17705.8516\n",
      "Epoch: 237, Train Loss: 7948.4531, Test Loss: 17603.8438\n",
      "Epoch: 238, Train Loss: 7979.3098, Test Loss: 17416.6484\n",
      "Epoch: 239, Train Loss: 7486.7656, Test Loss: 17172.5527\n",
      "Epoch: 240, Train Loss: 7375.9756, Test Loss: 17016.4121\n",
      "Epoch: 241, Train Loss: 6857.5815, Test Loss: 16922.1680\n",
      "Epoch: 242, Train Loss: 7176.0039, Test Loss: 16864.2051\n",
      "Epoch: 243, Train Loss: 7929.0012, Test Loss: 16833.0762\n",
      "Epoch: 244, Train Loss: 7490.9958, Test Loss: 16736.7793\n",
      "Epoch: 245, Train Loss: 6882.0459, Test Loss: 16543.3223\n",
      "Epoch: 246, Train Loss: 6878.7844, Test Loss: 16378.1641\n",
      "Epoch: 247, Train Loss: 7520.4355, Test Loss: 16243.3750\n",
      "Epoch: 248, Train Loss: 7301.1533, Test Loss: 16028.2676\n",
      "Epoch: 249, Train Loss: 7425.3726, Test Loss: 15763.2939\n",
      "Epoch: 250, Train Loss: 7154.4460, Test Loss: 15496.7207\n",
      "Epoch: 251, Train Loss: 7214.2053, Test Loss: 15237.5566\n",
      "Epoch: 252, Train Loss: 7068.5925, Test Loss: 15010.0928\n",
      "Epoch: 253, Train Loss: 6639.7957, Test Loss: 14791.2041\n",
      "Epoch: 254, Train Loss: 6438.7930, Test Loss: 14663.7559\n",
      "Epoch: 255, Train Loss: 6725.4092, Test Loss: 14606.1973\n",
      "Epoch: 256, Train Loss: 7076.5645, Test Loss: 14646.1006\n",
      "Epoch: 257, Train Loss: 6244.7808, Test Loss: 14634.9727\n",
      "Epoch: 258, Train Loss: 6795.9050, Test Loss: 14673.5459\n",
      "Epoch: 259, Train Loss: 6645.8140, Test Loss: 14609.0625\n",
      "Epoch: 260, Train Loss: 6031.3093, Test Loss: 14585.7109\n",
      "Epoch: 261, Train Loss: 6625.9858, Test Loss: 14553.6934\n",
      "Epoch: 262, Train Loss: 6210.9670, Test Loss: 14407.3926\n",
      "Epoch: 263, Train Loss: 6150.8591, Test Loss: 14307.7500\n",
      "Epoch: 264, Train Loss: 6895.0059, Test Loss: 14257.0264\n",
      "Epoch: 265, Train Loss: 6171.7317, Test Loss: 14144.1602\n",
      "Epoch: 266, Train Loss: 6151.3406, Test Loss: 14086.8496\n",
      "Epoch: 267, Train Loss: 6256.7512, Test Loss: 14057.7354\n",
      "Epoch: 268, Train Loss: 5869.2048, Test Loss: 14066.3740\n",
      "Epoch: 269, Train Loss: 5991.8562, Test Loss: 14088.9004\n",
      "Epoch: 270, Train Loss: 6319.4558, Test Loss: 14112.3115\n",
      "Epoch: 271, Train Loss: 5835.5784, Test Loss: 14002.6484\n",
      "Epoch: 272, Train Loss: 5892.9407, Test Loss: 13896.6973\n",
      "Epoch: 273, Train Loss: 5907.4482, Test Loss: 13820.0605\n",
      "Epoch: 274, Train Loss: 5883.4062, Test Loss: 13765.1943\n",
      "Epoch: 275, Train Loss: 5841.9756, Test Loss: 13728.2090\n",
      "Epoch: 276, Train Loss: 5777.7019, Test Loss: 13678.4746\n",
      "Epoch: 277, Train Loss: 5822.4246, Test Loss: 13631.1807\n",
      "Epoch: 278, Train Loss: 5580.8992, Test Loss: 13603.2217\n",
      "Epoch: 279, Train Loss: 5866.0776, Test Loss: 13584.0576\n",
      "Epoch: 280, Train Loss: 5629.5659, Test Loss: 13451.4443\n",
      "Epoch: 281, Train Loss: 5520.1807, Test Loss: 13345.0850\n",
      "Epoch: 282, Train Loss: 5955.6277, Test Loss: 13262.2158\n",
      "Epoch: 283, Train Loss: 5495.2612, Test Loss: 13081.6455\n",
      "Epoch: 284, Train Loss: 5932.2739, Test Loss: 12955.7500\n",
      "Epoch: 285, Train Loss: 5268.9298, Test Loss: 12757.0332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286, Train Loss: 5624.4607, Test Loss: 12606.2168\n",
      "Epoch: 287, Train Loss: 5532.1125, Test Loss: 12528.4678\n",
      "Epoch: 288, Train Loss: 5613.3853, Test Loss: 12504.1143\n",
      "Epoch: 289, Train Loss: 5640.6748, Test Loss: 12397.6279\n",
      "Epoch: 290, Train Loss: 5262.3425, Test Loss: 12216.8945\n",
      "Epoch: 291, Train Loss: 5910.5524, Test Loss: 12091.8271\n",
      "Epoch: 292, Train Loss: 5433.1592, Test Loss: 11946.8428\n",
      "Epoch: 293, Train Loss: 5513.3962, Test Loss: 11882.5898\n",
      "Epoch: 294, Train Loss: 5100.6589, Test Loss: 11765.3604\n",
      "Epoch: 295, Train Loss: 5176.5527, Test Loss: 11697.0693\n",
      "Epoch: 296, Train Loss: 5376.4724, Test Loss: 11694.0791\n",
      "Epoch: 297, Train Loss: 5170.6787, Test Loss: 11626.7246\n",
      "Epoch: 298, Train Loss: 5299.3689, Test Loss: 11565.5137\n",
      "Epoch: 299, Train Loss: 5465.7830, Test Loss: 11429.9717\n",
      "Epoch: 300, Train Loss: 4865.5807, Test Loss: 11280.8701\n",
      "Epoch: 301, Train Loss: 5000.4966, Test Loss: 11171.8750\n",
      "Epoch: 302, Train Loss: 5161.4041, Test Loss: 11100.3877\n",
      "Epoch: 303, Train Loss: 5320.3228, Test Loss: 11090.1445\n",
      "Epoch: 304, Train Loss: 4730.5438, Test Loss: 11029.0215\n",
      "Epoch: 305, Train Loss: 4953.3796, Test Loss: 10977.9561\n",
      "Epoch: 306, Train Loss: 4965.6233, Test Loss: 10947.7402\n",
      "Epoch: 307, Train Loss: 4842.5085, Test Loss: 10951.7793\n",
      "Epoch: 308, Train Loss: 4714.7223, Test Loss: 10968.8467\n",
      "Epoch: 309, Train Loss: 4893.0166, Test Loss: 10980.3545\n",
      "Epoch: 310, Train Loss: 4879.0103, Test Loss: 11005.2090\n",
      "Epoch: 311, Train Loss: 4636.4294, Test Loss: 10908.5820\n",
      "Epoch: 312, Train Loss: 5053.6705, Test Loss: 10851.1152\n",
      "Epoch: 313, Train Loss: 4637.6746, Test Loss: 10717.7197\n",
      "Epoch: 314, Train Loss: 4921.2417, Test Loss: 10620.5889\n",
      "Epoch: 315, Train Loss: 4532.3239, Test Loss: 10455.9365\n",
      "Epoch: 316, Train Loss: 4389.1808, Test Loss: 10335.7275\n",
      "Epoch: 317, Train Loss: 4427.3423, Test Loss: 10257.0420\n",
      "Epoch: 318, Train Loss: 4375.6605, Test Loss: 10208.0693\n",
      "Epoch: 319, Train Loss: 4700.5164, Test Loss: 10194.0869\n",
      "Epoch: 320, Train Loss: 4410.3101, Test Loss: 10101.2646\n",
      "Epoch: 321, Train Loss: 4401.0244, Test Loss: 10051.1240\n",
      "Epoch: 322, Train Loss: 4521.4985, Test Loss: 10005.2705\n",
      "Epoch: 323, Train Loss: 4344.2588, Test Loss: 9975.2881\n",
      "Epoch: 324, Train Loss: 4173.2278, Test Loss: 9956.7617\n",
      "Epoch: 325, Train Loss: 4184.3833, Test Loss: 9937.9307\n",
      "Epoch: 326, Train Loss: 4169.2714, Test Loss: 9920.9834\n",
      "Epoch: 327, Train Loss: 4551.8628, Test Loss: 9904.8867\n",
      "Epoch: 328, Train Loss: 4438.2976, Test Loss: 9840.5156\n",
      "Epoch: 329, Train Loss: 4544.4858, Test Loss: 9724.8037\n",
      "Epoch: 330, Train Loss: 4168.3640, Test Loss: 9557.5840\n",
      "Epoch: 331, Train Loss: 4252.3672, Test Loss: 9455.4473\n",
      "Epoch: 332, Train Loss: 4517.9634, Test Loss: 9336.0205\n",
      "Epoch: 333, Train Loss: 4202.8027, Test Loss: 9204.7500\n",
      "Epoch: 334, Train Loss: 4354.3456, Test Loss: 9130.9297\n",
      "Epoch: 335, Train Loss: 4134.3921, Test Loss: 9041.5684\n",
      "Epoch: 336, Train Loss: 4182.1287, Test Loss: 9021.1592\n",
      "Epoch: 337, Train Loss: 4071.1392, Test Loss: 8950.5020\n",
      "Epoch: 338, Train Loss: 4395.6268, Test Loss: 8913.0400\n",
      "Epoch: 339, Train Loss: 4191.7538, Test Loss: 8822.9014\n",
      "Epoch: 340, Train Loss: 3838.3832, Test Loss: 8710.4795\n",
      "Epoch: 341, Train Loss: 4120.6312, Test Loss: 8653.0020\n",
      "Epoch: 342, Train Loss: 4056.3264, Test Loss: 8576.6230\n",
      "Epoch: 343, Train Loss: 3906.5846, Test Loss: 8551.9189\n",
      "Epoch: 344, Train Loss: 3975.5559, Test Loss: 8539.0059\n",
      "Epoch: 345, Train Loss: 3888.9136, Test Loss: 8539.1318\n",
      "Epoch: 346, Train Loss: 3857.0955, Test Loss: 8552.1338\n",
      "Epoch: 347, Train Loss: 3811.8330, Test Loss: 8575.0322\n",
      "Epoch: 348, Train Loss: 3685.9424, Test Loss: 8594.6416\n",
      "Epoch: 349, Train Loss: 3712.8749, Test Loss: 8602.1035\n",
      "Epoch: 350, Train Loss: 4067.1355, Test Loss: 8592.5010\n",
      "Epoch: 351, Train Loss: 3730.0325, Test Loss: 8514.9033\n",
      "Epoch: 352, Train Loss: 3698.6185, Test Loss: 8441.4922\n",
      "Epoch: 353, Train Loss: 3925.6254, Test Loss: 8399.8047\n",
      "Epoch: 354, Train Loss: 3572.2869, Test Loss: 8316.0850\n",
      "Epoch: 355, Train Loss: 3775.5573, Test Loss: 8249.0547\n",
      "Epoch: 356, Train Loss: 3676.9072, Test Loss: 8208.7812\n",
      "Epoch: 357, Train Loss: 3463.1953, Test Loss: 8184.7441\n",
      "Epoch: 358, Train Loss: 3729.8108, Test Loss: 8169.6089\n",
      "Epoch: 359, Train Loss: 3760.0363, Test Loss: 8181.9053\n",
      "Epoch: 360, Train Loss: 3632.8901, Test Loss: 8138.7310\n",
      "Epoch: 361, Train Loss: 3683.2156, Test Loss: 8102.6255\n",
      "Epoch: 362, Train Loss: 3455.1144, Test Loss: 8072.5962\n",
      "Epoch: 363, Train Loss: 3483.1735, Test Loss: 8033.8525\n",
      "Epoch: 364, Train Loss: 3647.1783, Test Loss: 7989.9409\n",
      "Epoch: 365, Train Loss: 3615.8018, Test Loss: 7955.9390\n",
      "Epoch: 366, Train Loss: 3561.1538, Test Loss: 7864.5391\n",
      "Epoch: 367, Train Loss: 3677.0793, Test Loss: 7802.0986\n",
      "Epoch: 368, Train Loss: 3316.0981, Test Loss: 7709.5073\n",
      "Epoch: 369, Train Loss: 3351.7560, Test Loss: 7642.9922\n",
      "Epoch: 370, Train Loss: 3306.3871, Test Loss: 7593.0903\n",
      "Epoch: 371, Train Loss: 3293.3386, Test Loss: 7531.4990\n",
      "Epoch: 372, Train Loss: 3325.0642, Test Loss: 7483.8120\n",
      "Epoch: 373, Train Loss: 3302.5974, Test Loss: 7444.3945\n",
      "Epoch: 374, Train Loss: 3515.7755, Test Loss: 7425.4902\n",
      "Epoch: 375, Train Loss: 3466.2284, Test Loss: 7378.7964\n",
      "Epoch: 376, Train Loss: 3552.7773, Test Loss: 7303.7119\n",
      "Epoch: 377, Train Loss: 3507.5496, Test Loss: 7205.1201\n",
      "Epoch: 378, Train Loss: 3385.1150, Test Loss: 7104.8286\n",
      "Epoch: 379, Train Loss: 3263.7810, Test Loss: 6994.8501\n",
      "Epoch: 380, Train Loss: 3289.0842, Test Loss: 6938.2407\n",
      "Epoch: 381, Train Loss: 3443.5626, Test Loss: 6911.9224\n",
      "Epoch: 382, Train Loss: 3245.6185, Test Loss: 6891.0801\n",
      "Epoch: 383, Train Loss: 3157.1874, Test Loss: 6890.4546\n",
      "Epoch: 384, Train Loss: 3261.8831, Test Loss: 6897.5273\n",
      "Epoch: 385, Train Loss: 3130.7324, Test Loss: 6899.3755\n",
      "Epoch: 386, Train Loss: 3097.1345, Test Loss: 6904.3843\n",
      "Epoch: 387, Train Loss: 3247.1899, Test Loss: 6916.1392\n",
      "Epoch: 388, Train Loss: 3152.0681, Test Loss: 6887.5659\n",
      "Epoch: 389, Train Loss: 3292.6686, Test Loss: 6811.5806\n",
      "Epoch: 390, Train Loss: 3053.1356, Test Loss: 6721.9971\n",
      "Epoch: 391, Train Loss: 2959.4084, Test Loss: 6664.2661\n",
      "Epoch: 392, Train Loss: 3184.8351, Test Loss: 6626.9580\n",
      "Epoch: 393, Train Loss: 2929.2993, Test Loss: 6554.8584\n",
      "Epoch: 394, Train Loss: 2985.2889, Test Loss: 6498.7251\n",
      "Epoch: 395, Train Loss: 3112.0763, Test Loss: 6470.8540\n",
      "Epoch: 396, Train Loss: 3088.0294, Test Loss: 6425.8086\n",
      "Epoch: 397, Train Loss: 2900.9108, Test Loss: 6365.3906\n",
      "Epoch: 398, Train Loss: 2882.6274, Test Loss: 6327.2280\n",
      "Epoch: 399, Train Loss: 2824.8656, Test Loss: 6294.9614\n",
      "Epoch: 400, Train Loss: 3102.1005, Test Loss: 6275.1196\n",
      "Epoch: 401, Train Loss: 3024.0393, Test Loss: 6220.7485\n",
      "Epoch: 402, Train Loss: 2979.0684, Test Loss: 6199.0356\n",
      "Epoch: 403, Train Loss: 2909.4209, Test Loss: 6214.2651\n",
      "Epoch: 404, Train Loss: 2827.2770, Test Loss: 6230.3955\n",
      "Epoch: 405, Train Loss: 2786.5591, Test Loss: 6243.9775\n",
      "Epoch: 406, Train Loss: 3070.9155, Test Loss: 6259.3379\n",
      "Epoch: 407, Train Loss: 2741.9142, Test Loss: 6237.1724\n",
      "Epoch: 408, Train Loss: 2856.0621, Test Loss: 6214.9785\n",
      "Epoch: 409, Train Loss: 2864.7235, Test Loss: 6198.9761\n",
      "Epoch: 410, Train Loss: 2970.0927, Test Loss: 6166.9448\n",
      "Epoch: 411, Train Loss: 2756.5518, Test Loss: 6081.3130\n",
      "Epoch: 412, Train Loss: 2838.7518, Test Loss: 6021.8271\n",
      "Epoch: 413, Train Loss: 2808.7268, Test Loss: 6007.4023\n",
      "Epoch: 414, Train Loss: 2723.6719, Test Loss: 5957.0830\n",
      "Epoch: 415, Train Loss: 2956.9547, Test Loss: 5920.9834\n",
      "Epoch: 416, Train Loss: 2885.1870, Test Loss: 5866.5278\n",
      "Epoch: 417, Train Loss: 2655.9873, Test Loss: 5792.8442\n",
      "Epoch: 418, Train Loss: 2892.5406, Test Loss: 5748.6826\n",
      "Epoch: 419, Train Loss: 2668.3081, Test Loss: 5695.1821\n",
      "Epoch: 420, Train Loss: 2739.4492, Test Loss: 5674.3320\n",
      "Epoch: 421, Train Loss: 2809.3805, Test Loss: 5679.0044\n",
      "Epoch: 422, Train Loss: 2735.6770, Test Loss: 5654.8770\n",
      "Epoch: 423, Train Loss: 2773.0984, Test Loss: 5610.1470\n",
      "Epoch: 424, Train Loss: 2514.4925, Test Loss: 5565.5938\n",
      "Epoch: 425, Train Loss: 2657.2675, Test Loss: 5539.1084\n",
      "Epoch: 426, Train Loss: 2631.1920, Test Loss: 5533.6235\n",
      "Epoch: 427, Train Loss: 2506.2941, Test Loss: 5532.0601\n",
      "Epoch: 428, Train Loss: 2741.4940, Test Loss: 5533.9102\n",
      "Epoch: 429, Train Loss: 2599.2467, Test Loss: 5518.1885\n",
      "Epoch: 430, Train Loss: 2584.5079, Test Loss: 5509.2773\n",
      "Epoch: 431, Train Loss: 2679.5984, Test Loss: 5488.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 432, Train Loss: 2487.8512, Test Loss: 5422.0254\n",
      "Epoch: 433, Train Loss: 2503.2804, Test Loss: 5367.6035\n",
      "Epoch: 434, Train Loss: 2663.1072, Test Loss: 5325.8999\n",
      "Epoch: 435, Train Loss: 2623.8007, Test Loss: 5266.3062\n",
      "Epoch: 436, Train Loss: 2400.4097, Test Loss: 5199.9053\n",
      "Epoch: 437, Train Loss: 2404.8075, Test Loss: 5147.8911\n",
      "Epoch: 438, Train Loss: 2431.1703, Test Loss: 5117.1846\n",
      "Epoch: 439, Train Loss: 2586.3077, Test Loss: 5096.0898\n",
      "Epoch: 440, Train Loss: 2538.1165, Test Loss: 5098.8037\n",
      "Epoch: 441, Train Loss: 2497.2756, Test Loss: 5083.8521\n",
      "Epoch: 442, Train Loss: 2346.6804, Test Loss: 5072.4385\n",
      "Epoch: 443, Train Loss: 2238.2199, Test Loss: 5068.0103\n",
      "Epoch: 444, Train Loss: 2558.0328, Test Loss: 5068.1235\n",
      "Epoch: 445, Train Loss: 2503.2020, Test Loss: 5045.7539\n",
      "Epoch: 446, Train Loss: 2296.5625, Test Loss: 5017.0195\n",
      "Epoch: 447, Train Loss: 2324.8244, Test Loss: 4995.6714\n",
      "Epoch: 448, Train Loss: 2497.3394, Test Loss: 4988.0835\n",
      "Epoch: 449, Train Loss: 2383.5073, Test Loss: 4952.4097\n",
      "Epoch: 450, Train Loss: 2421.9849, Test Loss: 4925.1143\n",
      "Epoch: 451, Train Loss: 2435.8712, Test Loss: 4884.3047\n",
      "Epoch: 452, Train Loss: 2234.7305, Test Loss: 4840.5381\n",
      "Epoch: 453, Train Loss: 2396.8843, Test Loss: 4807.8154\n",
      "Epoch: 454, Train Loss: 2418.4091, Test Loss: 4785.1318\n",
      "Epoch: 455, Train Loss: 2242.0846, Test Loss: 4750.6948\n",
      "Epoch: 456, Train Loss: 2234.3546, Test Loss: 4733.5913\n",
      "Epoch: 457, Train Loss: 2271.8428, Test Loss: 4721.7944\n",
      "Epoch: 458, Train Loss: 2239.6124, Test Loss: 4729.5171\n",
      "Epoch: 459, Train Loss: 2220.3099, Test Loss: 4720.8174\n",
      "Epoch: 460, Train Loss: 2244.7284, Test Loss: 4713.7939\n",
      "Epoch: 461, Train Loss: 2282.3264, Test Loss: 4700.0435\n",
      "Epoch: 462, Train Loss: 2333.6257, Test Loss: 4651.6211\n",
      "Epoch: 463, Train Loss: 2065.2437, Test Loss: 4582.8677\n",
      "Epoch: 464, Train Loss: 2151.0804, Test Loss: 4538.9248\n",
      "Epoch: 465, Train Loss: 2252.6945, Test Loss: 4520.4922\n",
      "Epoch: 466, Train Loss: 2158.4130, Test Loss: 4514.3789\n",
      "Epoch: 467, Train Loss: 2228.6754, Test Loss: 4520.4355\n",
      "Epoch: 468, Train Loss: 2134.0224, Test Loss: 4514.3276\n",
      "Epoch: 469, Train Loss: 2247.1503, Test Loss: 4505.1738\n",
      "Epoch: 470, Train Loss: 2281.3290, Test Loss: 4503.2422\n",
      "Epoch: 471, Train Loss: 2252.7276, Test Loss: 4483.2861\n",
      "Epoch: 472, Train Loss: 2074.9958, Test Loss: 4448.1831\n",
      "Epoch: 473, Train Loss: 2098.8499, Test Loss: 4434.7456\n",
      "Epoch: 474, Train Loss: 2100.5016, Test Loss: 4432.9385\n",
      "Epoch: 475, Train Loss: 2189.5602, Test Loss: 4436.1133\n",
      "Epoch: 476, Train Loss: 2209.9117, Test Loss: 4433.4834\n",
      "Epoch: 477, Train Loss: 2064.4868, Test Loss: 4394.1284\n",
      "Epoch: 478, Train Loss: 2140.1298, Test Loss: 4366.6304\n",
      "Epoch: 479, Train Loss: 2136.5850, Test Loss: 4347.1953\n",
      "Epoch: 480, Train Loss: 2235.2746, Test Loss: 4307.1841\n",
      "Epoch: 481, Train Loss: 2026.3919, Test Loss: 4271.5117\n",
      "Epoch: 482, Train Loss: 2040.8270, Test Loss: 4254.7427\n",
      "Epoch: 483, Train Loss: 2035.4212, Test Loss: 4240.4697\n",
      "Epoch: 484, Train Loss: 2044.0743, Test Loss: 4241.9248\n",
      "Epoch: 485, Train Loss: 1992.1575, Test Loss: 4242.3154\n",
      "Epoch: 486, Train Loss: 1977.7978, Test Loss: 4232.5352\n",
      "Epoch: 487, Train Loss: 1892.7241, Test Loss: 4224.5430\n",
      "Epoch: 488, Train Loss: 1988.2751, Test Loss: 4211.2144\n",
      "Epoch: 489, Train Loss: 1981.6215, Test Loss: 4201.1675\n",
      "Epoch: 490, Train Loss: 2023.8717, Test Loss: 4188.4722\n",
      "Epoch: 491, Train Loss: 2010.1350, Test Loss: 4148.0405\n",
      "Epoch: 492, Train Loss: 1986.6747, Test Loss: 4115.1792\n",
      "Epoch: 493, Train Loss: 2085.3475, Test Loss: 4102.6079\n",
      "Epoch: 494, Train Loss: 1949.6507, Test Loss: 4091.0479\n",
      "Epoch: 495, Train Loss: 1945.3148, Test Loss: 4081.2498\n",
      "Epoch: 496, Train Loss: 1893.6682, Test Loss: 4069.4548\n",
      "Epoch: 497, Train Loss: 1974.7464, Test Loss: 4060.3525\n",
      "Epoch: 498, Train Loss: 1953.6544, Test Loss: 4056.1570\n",
      "Epoch: 499, Train Loss: 1962.8810, Test Loss: 4040.4485\n",
      "Epoch: 500, Train Loss: 1934.3016, Test Loss: 4008.7668\n",
      "Epoch: 501, Train Loss: 1963.0493, Test Loss: 3972.5820\n",
      "Epoch: 502, Train Loss: 1836.2058, Test Loss: 3942.6218\n",
      "Epoch: 503, Train Loss: 1845.4847, Test Loss: 3929.0237\n",
      "Epoch: 504, Train Loss: 1854.6877, Test Loss: 3929.4419\n",
      "Epoch: 505, Train Loss: 1856.8366, Test Loss: 3914.7461\n",
      "Epoch: 506, Train Loss: 1921.2253, Test Loss: 3909.4160\n",
      "Epoch: 507, Train Loss: 1897.9453, Test Loss: 3892.0085\n",
      "Epoch: 508, Train Loss: 1798.5925, Test Loss: 3865.7195\n",
      "Epoch: 509, Train Loss: 1880.1602, Test Loss: 3849.9590\n",
      "Epoch: 510, Train Loss: 1918.9927, Test Loss: 3817.9006\n",
      "Epoch: 511, Train Loss: 1784.8311, Test Loss: 3794.3625\n",
      "Epoch: 512, Train Loss: 2014.8348, Test Loss: 3772.8962\n",
      "Epoch: 513, Train Loss: 1903.6434, Test Loss: 3748.0442\n",
      "Epoch: 514, Train Loss: 1832.7327, Test Loss: 3732.1692\n",
      "Epoch: 515, Train Loss: 1738.1690, Test Loss: 3726.6001\n",
      "Epoch: 516, Train Loss: 1798.7240, Test Loss: 3726.4683\n",
      "Epoch: 517, Train Loss: 1793.7520, Test Loss: 3726.0820\n",
      "Epoch: 518, Train Loss: 1764.6384, Test Loss: 3716.1345\n",
      "Epoch: 519, Train Loss: 1944.2960, Test Loss: 3712.7954\n",
      "Epoch: 520, Train Loss: 1787.2455, Test Loss: 3705.8982\n",
      "Epoch: 521, Train Loss: 1776.4109, Test Loss: 3694.9016\n",
      "Epoch: 522, Train Loss: 1755.1450, Test Loss: 3675.5205\n",
      "Epoch: 523, Train Loss: 1774.3925, Test Loss: 3659.9497\n",
      "Epoch: 524, Train Loss: 1750.0178, Test Loss: 3638.8708\n",
      "Epoch: 525, Train Loss: 1771.8389, Test Loss: 3632.1328\n",
      "Epoch: 526, Train Loss: 1779.7971, Test Loss: 3628.4893\n",
      "Epoch: 527, Train Loss: 1667.6407, Test Loss: 3627.1204\n",
      "Epoch: 528, Train Loss: 1754.8170, Test Loss: 3615.2749\n",
      "Epoch: 529, Train Loss: 1766.8572, Test Loss: 3603.3320\n",
      "Epoch: 530, Train Loss: 1807.4406, Test Loss: 3603.0474\n",
      "Epoch: 531, Train Loss: 1761.0051, Test Loss: 3576.3652\n",
      "Epoch: 532, Train Loss: 1727.9307, Test Loss: 3527.3369\n",
      "Epoch: 533, Train Loss: 1654.5988, Test Loss: 3505.3760\n",
      "Epoch: 534, Train Loss: 1750.9173, Test Loss: 3496.8801\n",
      "Epoch: 535, Train Loss: 1739.1690, Test Loss: 3484.2717\n",
      "Epoch: 536, Train Loss: 1753.0157, Test Loss: 3453.4707\n",
      "Epoch: 537, Train Loss: 1704.0145, Test Loss: 3398.7261\n",
      "Epoch: 538, Train Loss: 1703.3303, Test Loss: 3365.1687\n",
      "Epoch: 539, Train Loss: 1730.1481, Test Loss: 3345.6038\n",
      "Epoch: 540, Train Loss: 1625.8080, Test Loss: 3354.6743\n",
      "Epoch: 541, Train Loss: 1672.0886, Test Loss: 3358.8430\n",
      "Epoch: 542, Train Loss: 1694.8520, Test Loss: 3341.3726\n",
      "Epoch: 543, Train Loss: 1712.2286, Test Loss: 3338.8564\n",
      "Epoch: 544, Train Loss: 1637.0533, Test Loss: 3354.0149\n",
      "Epoch: 545, Train Loss: 1641.8624, Test Loss: 3357.8760\n",
      "Epoch: 546, Train Loss: 1606.9628, Test Loss: 3334.6814\n",
      "Epoch: 547, Train Loss: 1577.8561, Test Loss: 3317.2134\n",
      "Epoch: 548, Train Loss: 1688.0581, Test Loss: 3315.8418\n",
      "Epoch: 549, Train Loss: 1560.6000, Test Loss: 3321.8223\n",
      "Epoch: 550, Train Loss: 1527.1360, Test Loss: 3306.4468\n",
      "Epoch: 551, Train Loss: 1545.6329, Test Loss: 3282.2793\n",
      "Epoch: 552, Train Loss: 1592.5364, Test Loss: 3270.6758\n",
      "Epoch: 553, Train Loss: 1604.6177, Test Loss: 3271.8455\n",
      "Epoch: 554, Train Loss: 1580.7042, Test Loss: 3267.2063\n",
      "Epoch: 555, Train Loss: 1590.2737, Test Loss: 3254.7388\n",
      "Epoch: 556, Train Loss: 1539.0554, Test Loss: 3228.7764\n",
      "Epoch: 557, Train Loss: 1550.3461, Test Loss: 3193.7646\n",
      "Epoch: 558, Train Loss: 1553.7200, Test Loss: 3182.6184\n",
      "Epoch: 559, Train Loss: 1614.3493, Test Loss: 3186.2822\n",
      "Epoch: 560, Train Loss: 1537.6246, Test Loss: 3178.5903\n",
      "Epoch: 561, Train Loss: 1598.4684, Test Loss: 3166.4331\n",
      "Epoch: 562, Train Loss: 1493.2335, Test Loss: 3137.1614\n",
      "Epoch: 563, Train Loss: 1548.1539, Test Loss: 3122.7637\n",
      "Epoch: 564, Train Loss: 1484.9460, Test Loss: 3111.1191\n",
      "Epoch: 565, Train Loss: 1662.2721, Test Loss: 3109.8896\n",
      "Epoch: 566, Train Loss: 1554.5778, Test Loss: 3114.4851\n",
      "Epoch: 567, Train Loss: 1523.0240, Test Loss: 3099.4519\n",
      "Epoch: 568, Train Loss: 1466.7181, Test Loss: 3076.3809\n",
      "Epoch: 569, Train Loss: 1549.4910, Test Loss: 3064.3508\n",
      "Epoch: 570, Train Loss: 1516.5115, Test Loss: 3050.5112\n",
      "Epoch: 571, Train Loss: 1521.9352, Test Loss: 3037.1506\n",
      "Epoch: 572, Train Loss: 1514.6037, Test Loss: 3041.3101\n",
      "Epoch: 573, Train Loss: 1583.3714, Test Loss: 3046.8230\n",
      "Epoch: 574, Train Loss: 1484.1219, Test Loss: 3021.8879\n",
      "Epoch: 575, Train Loss: 1523.6257, Test Loss: 2994.3955\n",
      "Epoch: 576, Train Loss: 1468.3020, Test Loss: 2979.7771\n",
      "Epoch: 577, Train Loss: 1486.9566, Test Loss: 2984.6208\n",
      "Epoch: 578, Train Loss: 1485.5186, Test Loss: 2985.0430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 579, Train Loss: 1506.0051, Test Loss: 2976.0596\n",
      "Epoch: 580, Train Loss: 1415.3485, Test Loss: 2957.6133\n",
      "Epoch: 581, Train Loss: 1465.5713, Test Loss: 2947.6399\n",
      "Epoch: 582, Train Loss: 1491.4724, Test Loss: 2949.7993\n",
      "Epoch: 583, Train Loss: 1434.4515, Test Loss: 2951.1780\n",
      "Epoch: 584, Train Loss: 1452.7490, Test Loss: 2939.5085\n",
      "Epoch: 585, Train Loss: 1487.0314, Test Loss: 2912.7354\n",
      "Epoch: 586, Train Loss: 1374.8000, Test Loss: 2886.0190\n",
      "Epoch: 587, Train Loss: 1518.7537, Test Loss: 2876.2378\n",
      "Epoch: 588, Train Loss: 1487.3509, Test Loss: 2870.0525\n",
      "Epoch: 589, Train Loss: 1450.7213, Test Loss: 2879.7861\n",
      "Epoch: 590, Train Loss: 1395.3582, Test Loss: 2866.0750\n",
      "Epoch: 591, Train Loss: 1446.2165, Test Loss: 2841.2844\n",
      "Epoch: 592, Train Loss: 1429.6632, Test Loss: 2823.0696\n",
      "Epoch: 593, Train Loss: 1387.5179, Test Loss: 2816.9351\n",
      "Epoch: 594, Train Loss: 1443.6310, Test Loss: 2821.9211\n",
      "Epoch: 595, Train Loss: 1400.1238, Test Loss: 2831.7161\n",
      "Epoch: 596, Train Loss: 1370.1614, Test Loss: 2800.4241\n",
      "Epoch: 597, Train Loss: 1429.4321, Test Loss: 2784.8591\n",
      "Epoch: 598, Train Loss: 1479.2149, Test Loss: 2780.5095\n",
      "Epoch: 599, Train Loss: 1404.9421, Test Loss: 2757.8259\n",
      "Epoch: 600, Train Loss: 1433.2725, Test Loss: 2751.4292\n",
      "Epoch: 601, Train Loss: 1411.4313, Test Loss: 2743.9695\n",
      "Epoch: 602, Train Loss: 1403.8150, Test Loss: 2727.6965\n",
      "Epoch: 603, Train Loss: 1366.8074, Test Loss: 2697.6816\n",
      "Epoch: 604, Train Loss: 1320.5111, Test Loss: 2687.0576\n",
      "Epoch: 605, Train Loss: 1330.5507, Test Loss: 2688.1216\n",
      "Epoch: 606, Train Loss: 1345.3222, Test Loss: 2690.7380\n",
      "Epoch: 607, Train Loss: 1331.4531, Test Loss: 2675.6028\n",
      "Epoch: 608, Train Loss: 1381.8048, Test Loss: 2662.5745\n",
      "Epoch: 609, Train Loss: 1385.5817, Test Loss: 2668.8489\n",
      "Epoch: 610, Train Loss: 1397.0191, Test Loss: 2678.7695\n",
      "Epoch: 611, Train Loss: 1314.2889, Test Loss: 2662.5730\n",
      "Epoch: 612, Train Loss: 1325.8922, Test Loss: 2658.6726\n",
      "Epoch: 613, Train Loss: 1383.2118, Test Loss: 2669.9475\n",
      "Epoch: 614, Train Loss: 1359.8754, Test Loss: 2665.5730\n",
      "Epoch: 615, Train Loss: 1369.1772, Test Loss: 2662.6194\n",
      "Epoch: 616, Train Loss: 1359.7373, Test Loss: 2660.4395\n",
      "Epoch: 617, Train Loss: 1290.3229, Test Loss: 2644.5366\n",
      "Epoch: 618, Train Loss: 1304.8887, Test Loss: 2617.2415\n",
      "Epoch: 619, Train Loss: 1291.7864, Test Loss: 2606.0300\n",
      "Epoch: 620, Train Loss: 1330.6375, Test Loss: 2594.9243\n",
      "Epoch: 621, Train Loss: 1317.5760, Test Loss: 2589.6404\n",
      "Epoch: 622, Train Loss: 1296.6005, Test Loss: 2579.1338\n",
      "Epoch: 623, Train Loss: 1323.9911, Test Loss: 2561.6162\n",
      "Epoch: 624, Train Loss: 1307.9036, Test Loss: 2552.5779\n",
      "Epoch: 625, Train Loss: 1299.1658, Test Loss: 2548.5115\n",
      "Epoch: 626, Train Loss: 1341.4409, Test Loss: 2551.2214\n",
      "Epoch: 627, Train Loss: 1340.6066, Test Loss: 2542.9802\n",
      "Epoch: 628, Train Loss: 1211.9158, Test Loss: 2531.3601\n",
      "Epoch: 629, Train Loss: 1294.7418, Test Loss: 2526.6187\n",
      "Epoch: 630, Train Loss: 1262.3973, Test Loss: 2507.6997\n",
      "Epoch: 631, Train Loss: 1252.1360, Test Loss: 2492.3323\n",
      "Epoch: 632, Train Loss: 1291.1728, Test Loss: 2493.9875\n",
      "Epoch: 633, Train Loss: 1233.9855, Test Loss: 2505.2683\n",
      "Epoch: 634, Train Loss: 1304.7220, Test Loss: 2511.1516\n",
      "Epoch: 635, Train Loss: 1250.2515, Test Loss: 2490.8486\n",
      "Epoch: 636, Train Loss: 1274.6387, Test Loss: 2496.6899\n",
      "Epoch: 637, Train Loss: 1345.8275, Test Loss: 2497.3689\n",
      "Epoch: 638, Train Loss: 1252.0720, Test Loss: 2486.7627\n",
      "Epoch: 639, Train Loss: 1253.9512, Test Loss: 2456.2107\n",
      "Epoch: 640, Train Loss: 1193.9977, Test Loss: 2441.5942\n",
      "Epoch: 641, Train Loss: 1227.6002, Test Loss: 2426.2554\n",
      "Epoch: 642, Train Loss: 1267.0732, Test Loss: 2421.8928\n",
      "Epoch: 643, Train Loss: 1209.9772, Test Loss: 2432.9053\n",
      "Epoch: 644, Train Loss: 1239.9036, Test Loss: 2420.6616\n",
      "Epoch: 645, Train Loss: 1270.4689, Test Loss: 2401.6011\n",
      "Epoch: 646, Train Loss: 1195.4450, Test Loss: 2392.5303\n",
      "Epoch: 647, Train Loss: 1183.4766, Test Loss: 2379.4680\n",
      "Epoch: 648, Train Loss: 1130.8430, Test Loss: 2383.2112\n",
      "Epoch: 649, Train Loss: 1257.4807, Test Loss: 2384.5645\n",
      "Epoch: 650, Train Loss: 1208.6737, Test Loss: 2360.0576\n",
      "Epoch: 651, Train Loss: 1274.1199, Test Loss: 2348.7424\n",
      "Epoch: 652, Train Loss: 1197.3085, Test Loss: 2350.7925\n",
      "Epoch: 653, Train Loss: 1225.7195, Test Loss: 2357.0142\n",
      "Epoch: 654, Train Loss: 1154.5881, Test Loss: 2339.3960\n",
      "Epoch: 655, Train Loss: 1230.0958, Test Loss: 2321.0874\n",
      "Epoch: 656, Train Loss: 1168.1696, Test Loss: 2304.9856\n",
      "Epoch: 657, Train Loss: 1176.3063, Test Loss: 2293.9719\n",
      "Epoch: 658, Train Loss: 1144.9929, Test Loss: 2280.7812\n",
      "Epoch: 659, Train Loss: 1161.1813, Test Loss: 2282.6882\n",
      "Epoch: 660, Train Loss: 1177.4007, Test Loss: 2280.0564\n",
      "Epoch: 661, Train Loss: 1170.7546, Test Loss: 2275.9060\n",
      "Epoch: 662, Train Loss: 1228.4896, Test Loss: 2267.7117\n",
      "Epoch: 663, Train Loss: 1130.2721, Test Loss: 2263.3198\n",
      "Epoch: 664, Train Loss: 1150.0764, Test Loss: 2257.0061\n",
      "Epoch: 665, Train Loss: 1148.3837, Test Loss: 2264.2483\n",
      "Epoch: 666, Train Loss: 1134.0212, Test Loss: 2263.3923\n",
      "Epoch: 667, Train Loss: 1152.0642, Test Loss: 2235.8765\n",
      "Epoch: 668, Train Loss: 1184.9189, Test Loss: 2226.9878\n",
      "Epoch: 669, Train Loss: 1108.4651, Test Loss: 2235.8074\n",
      "Epoch: 670, Train Loss: 1097.7386, Test Loss: 2239.8943\n",
      "Epoch: 671, Train Loss: 1144.1511, Test Loss: 2229.8879\n",
      "Epoch: 672, Train Loss: 1125.2399, Test Loss: 2230.0625\n",
      "Epoch: 673, Train Loss: 1157.1599, Test Loss: 2230.4407\n",
      "Epoch: 674, Train Loss: 1218.6211, Test Loss: 2233.3147\n",
      "Epoch: 675, Train Loss: 1090.4706, Test Loss: 2233.2363\n",
      "Epoch: 676, Train Loss: 1093.9365, Test Loss: 2219.5342\n",
      "Epoch: 677, Train Loss: 1139.7576, Test Loss: 2200.3579\n",
      "Epoch: 678, Train Loss: 1133.2502, Test Loss: 2190.1650\n",
      "Epoch: 679, Train Loss: 1138.5759, Test Loss: 2169.8208\n",
      "Epoch: 680, Train Loss: 1149.6268, Test Loss: 2164.9573\n",
      "Epoch: 681, Train Loss: 1136.3519, Test Loss: 2156.1797\n",
      "Epoch: 682, Train Loss: 1114.4705, Test Loss: 2152.0781\n",
      "Epoch: 683, Train Loss: 1089.9606, Test Loss: 2158.2393\n",
      "Epoch: 684, Train Loss: 1124.1136, Test Loss: 2142.8311\n",
      "Epoch: 685, Train Loss: 1101.2299, Test Loss: 2123.4365\n",
      "Epoch: 686, Train Loss: 1076.7395, Test Loss: 2115.3523\n",
      "Epoch: 687, Train Loss: 1109.7346, Test Loss: 2108.2920\n",
      "Epoch: 688, Train Loss: 1102.5709, Test Loss: 2102.3694\n",
      "Epoch: 689, Train Loss: 1087.9235, Test Loss: 2101.7671\n",
      "Epoch: 690, Train Loss: 1092.0025, Test Loss: 2094.4814\n",
      "Epoch: 691, Train Loss: 1084.2611, Test Loss: 2081.6694\n",
      "Epoch: 692, Train Loss: 1116.6006, Test Loss: 2076.7869\n",
      "Epoch: 693, Train Loss: 1090.3051, Test Loss: 2075.6943\n",
      "Epoch: 694, Train Loss: 1089.4181, Test Loss: 2077.9382\n",
      "Epoch: 695, Train Loss: 1096.7697, Test Loss: 2077.7271\n",
      "Epoch: 696, Train Loss: 1134.1457, Test Loss: 2068.5190\n",
      "Epoch: 697, Train Loss: 1068.4120, Test Loss: 2060.2559\n",
      "Epoch: 698, Train Loss: 1074.8556, Test Loss: 2070.9929\n",
      "Epoch: 699, Train Loss: 1031.2967, Test Loss: 2075.8970\n",
      "Epoch: 700, Train Loss: 1057.7483, Test Loss: 2067.2549\n",
      "Epoch: 701, Train Loss: 1090.6725, Test Loss: 2065.7158\n",
      "Epoch: 702, Train Loss: 1047.3041, Test Loss: 2068.7498\n",
      "Epoch: 703, Train Loss: 1062.5332, Test Loss: 2045.1006\n",
      "Epoch: 704, Train Loss: 1040.2770, Test Loss: 2024.5682\n",
      "Epoch: 705, Train Loss: 1080.9046, Test Loss: 2021.6073\n",
      "Epoch: 706, Train Loss: 1065.2833, Test Loss: 2024.0200\n",
      "Epoch: 707, Train Loss: 1079.6914, Test Loss: 2008.2263\n",
      "Epoch: 708, Train Loss: 1035.2813, Test Loss: 1993.8110\n",
      "Epoch: 709, Train Loss: 1063.2419, Test Loss: 1980.0568\n",
      "Epoch: 710, Train Loss: 1030.7618, Test Loss: 1979.6760\n",
      "Epoch: 711, Train Loss: 1077.2634, Test Loss: 1978.2092\n",
      "Epoch: 712, Train Loss: 1022.6551, Test Loss: 1963.0548\n",
      "Epoch: 713, Train Loss: 1014.5871, Test Loss: 1961.2849\n",
      "Epoch: 714, Train Loss: 1007.5352, Test Loss: 1976.2355\n",
      "Epoch: 715, Train Loss: 1069.9440, Test Loss: 1980.9305\n",
      "Epoch: 716, Train Loss: 1017.5439, Test Loss: 1962.2399\n",
      "Epoch: 717, Train Loss: 1040.6120, Test Loss: 1954.5923\n",
      "Epoch: 718, Train Loss: 988.0413, Test Loss: 1942.0587\n",
      "Epoch: 719, Train Loss: 1011.9502, Test Loss: 1944.2842\n",
      "Epoch: 720, Train Loss: 1004.6297, Test Loss: 1954.8528\n",
      "Epoch: 721, Train Loss: 984.4513, Test Loss: 1946.7708\n",
      "Epoch: 722, Train Loss: 1006.3658, Test Loss: 1925.2084\n",
      "Epoch: 723, Train Loss: 1031.7303, Test Loss: 1908.0200\n",
      "Epoch: 724, Train Loss: 1005.2807, Test Loss: 1902.5045\n",
      "Epoch: 725, Train Loss: 1006.4146, Test Loss: 1909.8582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 726, Train Loss: 978.3255, Test Loss: 1914.7642\n",
      "Epoch: 727, Train Loss: 998.6888, Test Loss: 1892.5447\n",
      "Epoch: 728, Train Loss: 961.8082, Test Loss: 1896.9071\n",
      "Epoch: 729, Train Loss: 971.5745, Test Loss: 1917.9449\n",
      "Epoch: 730, Train Loss: 980.0600, Test Loss: 1920.4180\n",
      "Epoch: 731, Train Loss: 967.2055, Test Loss: 1895.3136\n",
      "Epoch: 732, Train Loss: 977.5005, Test Loss: 1894.3568\n",
      "Epoch: 733, Train Loss: 1007.2693, Test Loss: 1903.4102\n",
      "Epoch: 734, Train Loss: 956.2083, Test Loss: 1904.0927\n",
      "Epoch: 735, Train Loss: 1024.3697, Test Loss: 1894.6455\n",
      "Epoch: 736, Train Loss: 957.0377, Test Loss: 1877.7943\n",
      "Epoch: 737, Train Loss: 958.0572, Test Loss: 1871.5897\n",
      "Epoch: 738, Train Loss: 991.2341, Test Loss: 1865.2242\n",
      "Epoch: 739, Train Loss: 963.3066, Test Loss: 1868.6370\n",
      "Epoch: 740, Train Loss: 966.6685, Test Loss: 1862.4282\n",
      "Epoch: 741, Train Loss: 952.8315, Test Loss: 1856.3542\n",
      "Epoch: 742, Train Loss: 922.0622, Test Loss: 1851.7928\n",
      "Epoch: 743, Train Loss: 968.7503, Test Loss: 1845.4056\n",
      "Epoch: 744, Train Loss: 944.7455, Test Loss: 1827.8186\n",
      "Epoch: 745, Train Loss: 982.8517, Test Loss: 1812.5579\n",
      "Epoch: 746, Train Loss: 981.0037, Test Loss: 1806.9592\n",
      "Epoch: 747, Train Loss: 944.9848, Test Loss: 1802.0990\n",
      "Epoch: 748, Train Loss: 950.5128, Test Loss: 1804.5828\n",
      "Epoch: 749, Train Loss: 946.5917, Test Loss: 1790.0426\n",
      "Epoch: 750, Train Loss: 983.5284, Test Loss: 1775.6625\n",
      "Epoch: 751, Train Loss: 904.6935, Test Loss: 1772.2992\n",
      "Epoch: 752, Train Loss: 942.1438, Test Loss: 1778.3845\n",
      "Epoch: 753, Train Loss: 912.9759, Test Loss: 1774.3522\n",
      "Epoch: 754, Train Loss: 969.3182, Test Loss: 1775.5569\n",
      "Epoch: 755, Train Loss: 951.2887, Test Loss: 1775.9382\n",
      "Epoch: 756, Train Loss: 909.8048, Test Loss: 1792.2683\n",
      "Epoch: 757, Train Loss: 941.4815, Test Loss: 1782.1224\n",
      "Epoch: 758, Train Loss: 908.6104, Test Loss: 1752.2338\n",
      "Epoch: 759, Train Loss: 924.0127, Test Loss: 1740.8038\n",
      "Epoch: 760, Train Loss: 954.2703, Test Loss: 1743.5637\n",
      "Epoch: 761, Train Loss: 920.5372, Test Loss: 1744.1600\n",
      "Epoch: 762, Train Loss: 942.5349, Test Loss: 1721.4802\n",
      "Epoch: 763, Train Loss: 893.6049, Test Loss: 1716.8958\n",
      "Epoch: 764, Train Loss: 926.4510, Test Loss: 1714.8115\n",
      "Epoch: 765, Train Loss: 928.7112, Test Loss: 1726.5825\n",
      "Epoch: 766, Train Loss: 887.0269, Test Loss: 1745.3568\n",
      "Epoch: 767, Train Loss: 931.5085, Test Loss: 1737.2592\n",
      "Epoch: 768, Train Loss: 893.1427, Test Loss: 1729.1315\n",
      "Epoch: 769, Train Loss: 903.2064, Test Loss: 1722.7269\n",
      "Epoch: 770, Train Loss: 896.6226, Test Loss: 1728.4644\n",
      "Epoch: 771, Train Loss: 902.4861, Test Loss: 1724.9731\n",
      "Epoch: 772, Train Loss: 908.0249, Test Loss: 1722.3417\n",
      "Epoch: 773, Train Loss: 918.2538, Test Loss: 1735.1989\n",
      "Epoch: 774, Train Loss: 903.3764, Test Loss: 1732.4988\n",
      "Epoch: 775, Train Loss: 904.4737, Test Loss: 1720.8641\n",
      "Epoch: 776, Train Loss: 860.3354, Test Loss: 1715.8975\n",
      "Epoch: 777, Train Loss: 880.9254, Test Loss: 1701.6136\n",
      "Epoch: 778, Train Loss: 856.7592, Test Loss: 1691.7924\n",
      "Epoch: 779, Train Loss: 879.8349, Test Loss: 1674.2294\n",
      "Epoch: 780, Train Loss: 847.8967, Test Loss: 1668.5986\n",
      "Epoch: 781, Train Loss: 852.3361, Test Loss: 1680.2726\n",
      "Epoch: 782, Train Loss: 880.8989, Test Loss: 1683.6451\n",
      "Epoch: 783, Train Loss: 877.2459, Test Loss: 1684.2318\n",
      "Epoch: 784, Train Loss: 859.6310, Test Loss: 1683.5183\n",
      "Epoch: 785, Train Loss: 870.5384, Test Loss: 1671.6805\n",
      "Epoch: 786, Train Loss: 882.2554, Test Loss: 1657.9208\n",
      "Epoch: 787, Train Loss: 867.0040, Test Loss: 1654.4194\n",
      "Epoch: 788, Train Loss: 854.6789, Test Loss: 1645.6903\n",
      "Epoch: 789, Train Loss: 876.2426, Test Loss: 1649.5487\n",
      "Epoch: 790, Train Loss: 862.4617, Test Loss: 1656.4232\n",
      "Epoch: 791, Train Loss: 880.9355, Test Loss: 1657.3965\n",
      "Epoch: 792, Train Loss: 857.9225, Test Loss: 1647.7910\n",
      "Epoch: 793, Train Loss: 829.5857, Test Loss: 1635.0092\n",
      "Epoch: 794, Train Loss: 845.8169, Test Loss: 1635.0474\n",
      "Epoch: 795, Train Loss: 871.6010, Test Loss: 1637.5577\n",
      "Epoch: 796, Train Loss: 884.3232, Test Loss: 1640.3917\n",
      "Epoch: 797, Train Loss: 839.1342, Test Loss: 1641.0435\n",
      "Epoch: 798, Train Loss: 845.5472, Test Loss: 1642.2471\n",
      "Epoch: 799, Train Loss: 814.9282, Test Loss: 1627.8582\n",
      "Epoch: 800, Train Loss: 849.0797, Test Loss: 1619.6584\n",
      "Epoch: 801, Train Loss: 839.7130, Test Loss: 1618.8340\n",
      "Epoch: 802, Train Loss: 869.0799, Test Loss: 1620.1726\n",
      "Epoch: 803, Train Loss: 876.9847, Test Loss: 1614.9858\n",
      "Epoch: 804, Train Loss: 824.6696, Test Loss: 1598.1294\n",
      "Epoch: 805, Train Loss: 860.7629, Test Loss: 1594.9977\n",
      "Epoch: 806, Train Loss: 813.3069, Test Loss: 1591.1814\n",
      "Epoch: 807, Train Loss: 847.4133, Test Loss: 1595.7699\n",
      "Epoch: 808, Train Loss: 877.0803, Test Loss: 1582.1091\n",
      "Epoch: 809, Train Loss: 812.9344, Test Loss: 1558.6483\n",
      "Epoch: 810, Train Loss: 822.7667, Test Loss: 1553.6694\n",
      "Epoch: 811, Train Loss: 857.3002, Test Loss: 1559.4005\n",
      "Epoch: 812, Train Loss: 814.1790, Test Loss: 1582.6594\n",
      "Epoch: 813, Train Loss: 848.0345, Test Loss: 1568.8285\n",
      "Epoch: 814, Train Loss: 840.1622, Test Loss: 1560.0330\n",
      "Epoch: 815, Train Loss: 816.8070, Test Loss: 1552.5090\n",
      "Epoch: 816, Train Loss: 851.1796, Test Loss: 1546.2076\n",
      "Epoch: 817, Train Loss: 836.3942, Test Loss: 1523.7640\n",
      "Epoch: 818, Train Loss: 820.3226, Test Loss: 1519.2715\n",
      "Epoch: 819, Train Loss: 833.5717, Test Loss: 1521.9382\n",
      "Epoch: 820, Train Loss: 831.9519, Test Loss: 1532.0531\n",
      "Epoch: 821, Train Loss: 797.1880, Test Loss: 1527.9886\n",
      "Epoch: 822, Train Loss: 810.8855, Test Loss: 1516.2935\n",
      "Epoch: 823, Train Loss: 825.7962, Test Loss: 1514.9475\n",
      "Epoch: 824, Train Loss: 832.8502, Test Loss: 1517.3339\n",
      "Epoch: 825, Train Loss: 818.3883, Test Loss: 1532.9948\n",
      "Epoch: 826, Train Loss: 794.3532, Test Loss: 1534.3000\n",
      "Epoch: 827, Train Loss: 828.0931, Test Loss: 1516.3705\n",
      "Epoch: 828, Train Loss: 794.5870, Test Loss: 1500.3536\n",
      "Epoch: 829, Train Loss: 781.0759, Test Loss: 1505.6418\n",
      "Epoch: 830, Train Loss: 786.0518, Test Loss: 1499.0856\n",
      "Epoch: 831, Train Loss: 751.0285, Test Loss: 1493.9702\n",
      "Epoch: 832, Train Loss: 801.6544, Test Loss: 1492.2543\n",
      "Epoch: 833, Train Loss: 794.9684, Test Loss: 1490.2914\n",
      "Epoch: 834, Train Loss: 839.4963, Test Loss: 1472.4813\n",
      "Epoch: 835, Train Loss: 815.7255, Test Loss: 1467.5530\n",
      "Epoch: 836, Train Loss: 809.8054, Test Loss: 1462.7233\n",
      "Epoch: 837, Train Loss: 798.1228, Test Loss: 1465.6040\n",
      "Epoch: 838, Train Loss: 768.6388, Test Loss: 1457.6609\n",
      "Epoch: 839, Train Loss: 770.0871, Test Loss: 1453.9065\n",
      "Epoch: 840, Train Loss: 790.2579, Test Loss: 1469.6323\n",
      "Epoch: 841, Train Loss: 784.8638, Test Loss: 1481.6489\n",
      "Epoch: 842, Train Loss: 792.4935, Test Loss: 1478.9407\n",
      "Epoch: 843, Train Loss: 811.4738, Test Loss: 1469.4480\n",
      "Epoch: 844, Train Loss: 770.0043, Test Loss: 1461.3972\n",
      "Epoch: 845, Train Loss: 770.0495, Test Loss: 1471.9282\n",
      "Epoch: 846, Train Loss: 792.4343, Test Loss: 1455.8569\n",
      "Epoch: 847, Train Loss: 763.2946, Test Loss: 1430.4805\n",
      "Epoch: 848, Train Loss: 815.0309, Test Loss: 1426.8168\n",
      "Epoch: 849, Train Loss: 807.6342, Test Loss: 1441.0902\n",
      "Epoch: 850, Train Loss: 780.1990, Test Loss: 1468.4553\n",
      "Epoch: 851, Train Loss: 776.3823, Test Loss: 1450.1080\n",
      "Epoch: 852, Train Loss: 768.2877, Test Loss: 1422.6281\n",
      "Epoch: 853, Train Loss: 759.3819, Test Loss: 1399.1163\n",
      "Epoch: 854, Train Loss: 754.4633, Test Loss: 1402.3431\n",
      "Epoch: 855, Train Loss: 794.1018, Test Loss: 1417.9766\n",
      "Epoch: 856, Train Loss: 762.3932, Test Loss: 1421.7932\n",
      "Epoch: 857, Train Loss: 752.5992, Test Loss: 1414.3865\n",
      "Epoch: 858, Train Loss: 756.1274, Test Loss: 1417.9860\n",
      "Epoch: 859, Train Loss: 776.0901, Test Loss: 1423.1897\n",
      "Epoch: 860, Train Loss: 724.6246, Test Loss: 1408.1267\n",
      "Epoch: 861, Train Loss: 751.8847, Test Loss: 1389.0618\n",
      "Epoch: 862, Train Loss: 749.6968, Test Loss: 1384.3453\n",
      "Epoch: 863, Train Loss: 757.6912, Test Loss: 1385.6167\n",
      "Epoch: 864, Train Loss: 766.0987, Test Loss: 1384.6118\n",
      "Epoch: 865, Train Loss: 749.4351, Test Loss: 1379.5137\n",
      "Epoch: 866, Train Loss: 750.9728, Test Loss: 1393.3861\n",
      "Epoch: 867, Train Loss: 736.6524, Test Loss: 1414.3235\n",
      "Epoch: 868, Train Loss: 734.7440, Test Loss: 1408.0319\n",
      "Epoch: 869, Train Loss: 713.8436, Test Loss: 1389.8795\n",
      "Epoch: 870, Train Loss: 765.8517, Test Loss: 1367.2635\n",
      "Epoch: 871, Train Loss: 739.4136, Test Loss: 1377.6761\n",
      "Epoch: 872, Train Loss: 771.8397, Test Loss: 1398.7782\n",
      "Epoch: 873, Train Loss: 723.8910, Test Loss: 1388.0192\n",
      "Epoch: 874, Train Loss: 730.5142, Test Loss: 1371.2058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 875, Train Loss: 722.7395, Test Loss: 1379.2299\n",
      "Epoch: 876, Train Loss: 753.9919, Test Loss: 1384.6583\n",
      "Epoch: 877, Train Loss: 743.3348, Test Loss: 1356.7975\n",
      "Epoch: 878, Train Loss: 726.9947, Test Loss: 1338.5338\n",
      "Epoch: 879, Train Loss: 712.6666, Test Loss: 1325.5828\n",
      "Epoch: 880, Train Loss: 697.6516, Test Loss: 1337.0350\n",
      "Epoch: 881, Train Loss: 718.6653, Test Loss: 1346.8932\n",
      "Epoch: 882, Train Loss: 709.8927, Test Loss: 1348.7992\n",
      "Epoch: 883, Train Loss: 714.2797, Test Loss: 1353.4418\n",
      "Epoch: 884, Train Loss: 703.5414, Test Loss: 1344.8022\n",
      "Epoch: 885, Train Loss: 717.1600, Test Loss: 1330.5546\n",
      "Epoch: 886, Train Loss: 704.0805, Test Loss: 1322.5448\n",
      "Epoch: 887, Train Loss: 702.8260, Test Loss: 1317.6239\n",
      "Epoch: 888, Train Loss: 729.3225, Test Loss: 1311.9430\n",
      "Epoch: 889, Train Loss: 740.7264, Test Loss: 1314.4785\n",
      "Epoch: 890, Train Loss: 744.5918, Test Loss: 1325.1644\n",
      "Epoch: 891, Train Loss: 709.9610, Test Loss: 1322.8639\n",
      "Epoch: 892, Train Loss: 709.9890, Test Loss: 1304.5620\n",
      "Epoch: 893, Train Loss: 683.6227, Test Loss: 1294.9805\n",
      "Epoch: 894, Train Loss: 695.1860, Test Loss: 1300.2039\n",
      "Epoch: 895, Train Loss: 702.1391, Test Loss: 1310.7507\n",
      "Epoch: 896, Train Loss: 708.5399, Test Loss: 1299.0793\n",
      "Epoch: 897, Train Loss: 716.3554, Test Loss: 1304.5015\n",
      "Epoch: 898, Train Loss: 712.3282, Test Loss: 1293.6520\n",
      "Epoch: 899, Train Loss: 703.9060, Test Loss: 1291.1279\n",
      "Epoch: 900, Train Loss: 681.2376, Test Loss: 1281.7264\n",
      "Epoch: 901, Train Loss: 700.8632, Test Loss: 1274.4534\n",
      "Epoch: 902, Train Loss: 747.0510, Test Loss: 1262.9664\n",
      "Epoch: 903, Train Loss: 700.6516, Test Loss: 1268.0190\n",
      "Epoch: 904, Train Loss: 698.6685, Test Loss: 1267.4259\n",
      "Epoch: 905, Train Loss: 688.7596, Test Loss: 1265.7104\n",
      "Epoch: 906, Train Loss: 690.4759, Test Loss: 1258.3867\n",
      "Epoch: 907, Train Loss: 689.7224, Test Loss: 1256.7930\n",
      "Epoch: 908, Train Loss: 676.5367, Test Loss: 1270.4272\n",
      "Epoch: 909, Train Loss: 686.7382, Test Loss: 1269.1227\n",
      "Epoch: 910, Train Loss: 686.0032, Test Loss: 1253.5079\n",
      "Epoch: 911, Train Loss: 685.5115, Test Loss: 1245.5006\n",
      "Epoch: 912, Train Loss: 694.3574, Test Loss: 1261.6544\n",
      "Epoch: 913, Train Loss: 650.1657, Test Loss: 1274.5338\n",
      "Epoch: 914, Train Loss: 712.9853, Test Loss: 1257.0758\n",
      "Epoch: 915, Train Loss: 663.1003, Test Loss: 1240.4288\n",
      "Epoch: 916, Train Loss: 689.4718, Test Loss: 1237.5638\n",
      "Epoch: 917, Train Loss: 668.7854, Test Loss: 1262.0685\n",
      "Epoch: 918, Train Loss: 650.5386, Test Loss: 1270.1073\n",
      "Epoch: 919, Train Loss: 698.2257, Test Loss: 1246.1653\n",
      "Epoch: 920, Train Loss: 684.1130, Test Loss: 1238.9437\n",
      "Epoch: 921, Train Loss: 652.4743, Test Loss: 1242.3114\n",
      "Epoch: 922, Train Loss: 667.9394, Test Loss: 1248.7683\n",
      "Epoch: 923, Train Loss: 668.3941, Test Loss: 1226.2743\n",
      "Epoch: 924, Train Loss: 664.0396, Test Loss: 1222.0219\n",
      "Epoch: 925, Train Loss: 700.4253, Test Loss: 1225.9814\n",
      "Epoch: 926, Train Loss: 661.1622, Test Loss: 1210.9331\n",
      "Epoch: 927, Train Loss: 643.4581, Test Loss: 1207.5962\n",
      "Epoch: 928, Train Loss: 659.9781, Test Loss: 1196.3798\n",
      "Epoch: 929, Train Loss: 671.0484, Test Loss: 1192.5547\n",
      "Epoch: 930, Train Loss: 663.5074, Test Loss: 1195.8185\n",
      "Epoch: 931, Train Loss: 651.5385, Test Loss: 1194.2151\n",
      "Epoch: 932, Train Loss: 690.0667, Test Loss: 1189.4325\n",
      "Epoch: 933, Train Loss: 639.7079, Test Loss: 1203.3732\n",
      "Epoch: 934, Train Loss: 679.6530, Test Loss: 1208.7352\n",
      "Epoch: 935, Train Loss: 663.2094, Test Loss: 1197.0624\n",
      "Epoch: 936, Train Loss: 677.0703, Test Loss: 1194.1953\n",
      "Epoch: 937, Train Loss: 660.1246, Test Loss: 1189.2157\n",
      "Epoch: 938, Train Loss: 636.9680, Test Loss: 1174.2025\n",
      "Epoch: 939, Train Loss: 615.8511, Test Loss: 1176.2375\n",
      "Epoch: 940, Train Loss: 641.4704, Test Loss: 1183.7101\n",
      "Epoch: 941, Train Loss: 659.9340, Test Loss: 1175.5339\n",
      "Epoch: 942, Train Loss: 628.6213, Test Loss: 1158.5396\n",
      "Epoch: 943, Train Loss: 631.8734, Test Loss: 1147.1189\n",
      "Epoch: 944, Train Loss: 651.4853, Test Loss: 1148.9171\n",
      "Epoch: 945, Train Loss: 604.7222, Test Loss: 1162.5055\n",
      "Epoch: 946, Train Loss: 630.6089, Test Loss: 1166.0822\n",
      "Epoch: 947, Train Loss: 614.2503, Test Loss: 1166.9390\n",
      "Epoch: 948, Train Loss: 655.1923, Test Loss: 1173.3596\n",
      "Epoch: 949, Train Loss: 596.5301, Test Loss: 1171.3314\n",
      "Epoch: 950, Train Loss: 625.1804, Test Loss: 1164.1746\n",
      "Epoch: 951, Train Loss: 640.6378, Test Loss: 1161.3140\n",
      "Epoch: 952, Train Loss: 638.4319, Test Loss: 1151.3127\n",
      "Epoch: 953, Train Loss: 642.2546, Test Loss: 1145.2380\n",
      "Epoch: 954, Train Loss: 626.3621, Test Loss: 1157.6565\n",
      "Epoch: 955, Train Loss: 635.3679, Test Loss: 1155.4556\n",
      "Epoch: 956, Train Loss: 655.0706, Test Loss: 1140.5902\n",
      "Epoch: 957, Train Loss: 625.8407, Test Loss: 1138.1750\n",
      "Epoch: 958, Train Loss: 598.4619, Test Loss: 1160.0226\n",
      "Epoch: 959, Train Loss: 628.3697, Test Loss: 1150.4719\n",
      "Epoch: 960, Train Loss: 627.3381, Test Loss: 1144.0815\n",
      "Epoch: 961, Train Loss: 615.2727, Test Loss: 1130.2728\n",
      "Epoch: 962, Train Loss: 615.0839, Test Loss: 1124.7987\n",
      "Epoch: 963, Train Loss: 627.3054, Test Loss: 1128.7429\n",
      "Epoch: 964, Train Loss: 612.0956, Test Loss: 1128.4192\n",
      "Epoch: 965, Train Loss: 633.0500, Test Loss: 1124.6140\n",
      "Epoch: 966, Train Loss: 601.2858, Test Loss: 1123.2897\n",
      "Epoch: 967, Train Loss: 628.7982, Test Loss: 1124.2902\n",
      "Epoch: 968, Train Loss: 635.4150, Test Loss: 1115.7837\n",
      "Epoch: 969, Train Loss: 636.5108, Test Loss: 1105.8156\n",
      "Epoch: 970, Train Loss: 604.9585, Test Loss: 1101.7175\n",
      "Epoch: 971, Train Loss: 618.6909, Test Loss: 1107.5704\n",
      "Epoch: 972, Train Loss: 630.7836, Test Loss: 1107.2046\n",
      "Epoch: 973, Train Loss: 611.5286, Test Loss: 1106.0464\n",
      "Epoch: 974, Train Loss: 614.4959, Test Loss: 1105.5779\n",
      "Epoch: 975, Train Loss: 612.7698, Test Loss: 1105.4557\n",
      "Epoch: 976, Train Loss: 596.4642, Test Loss: 1096.2120\n",
      "Epoch: 977, Train Loss: 595.4391, Test Loss: 1086.5360\n",
      "Epoch: 978, Train Loss: 601.2503, Test Loss: 1091.4270\n",
      "Epoch: 979, Train Loss: 593.0072, Test Loss: 1093.6422\n",
      "Epoch: 980, Train Loss: 582.7415, Test Loss: 1084.6727\n",
      "Epoch: 981, Train Loss: 611.6649, Test Loss: 1079.0426\n",
      "Epoch: 982, Train Loss: 598.2992, Test Loss: 1092.8264\n",
      "Epoch: 983, Train Loss: 592.4046, Test Loss: 1099.6144\n",
      "Epoch: 984, Train Loss: 594.7828, Test Loss: 1082.3317\n",
      "Epoch: 985, Train Loss: 621.0963, Test Loss: 1087.2393\n",
      "Epoch: 986, Train Loss: 583.4773, Test Loss: 1094.1208\n",
      "Epoch: 987, Train Loss: 595.5790, Test Loss: 1079.1816\n",
      "Epoch: 988, Train Loss: 622.9641, Test Loss: 1078.7169\n",
      "Epoch: 989, Train Loss: 619.9651, Test Loss: 1072.6912\n",
      "Epoch: 990, Train Loss: 582.4384, Test Loss: 1079.8265\n",
      "Epoch: 991, Train Loss: 583.5877, Test Loss: 1064.5540\n",
      "Epoch: 992, Train Loss: 591.1063, Test Loss: 1048.1239\n",
      "Epoch: 993, Train Loss: 581.3703, Test Loss: 1058.3431\n",
      "Epoch: 994, Train Loss: 604.8477, Test Loss: 1060.9287\n",
      "Epoch: 995, Train Loss: 580.6302, Test Loss: 1056.3599\n",
      "Epoch: 996, Train Loss: 569.1437, Test Loss: 1059.0734\n",
      "Epoch: 997, Train Loss: 583.6713, Test Loss: 1062.2603\n",
      "Epoch: 998, Train Loss: 586.2401, Test Loss: 1058.7643\n",
      "Epoch: 999, Train Loss: 579.4863, Test Loss: 1051.7764\n",
      "Epoch: 1000, Train Loss: 611.5411, Test Loss: 1057.9927\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(out, data.x)\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # Perform a single forward pass\n",
    "            out_x = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(out, data.x)\n",
    "\n",
    "            # Accumulate the training loss\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Compute the average test loss\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b28ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Generate a denoised graph\n",
    "\n",
    "n_atoms = 302\n",
    "n_edges = 6480\n",
    "\n",
    "# Define the random graph data\n",
    "x = torch.randn(n_atoms, 5)\n",
    "edge_index = torch.randn(2, n_edges)\n",
    "edge_attr = torch.randn(n_edges, 1)'''\n",
    "\n",
    "cloned_graph = dataset[0].clone()\n",
    "\n",
    "n_atoms = cloned_graph.num_nodes\n",
    "n_edges = cloned_graph.num_edges\n",
    "\n",
    "x          = cloned_graph.x\n",
    "edge_index = cloned_graph.edge_index\n",
    "edge_attr  = cloned_graph.edge_attr\n",
    "\n",
    "# Create a new Data object with the generated graph data\n",
    "noisy_graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate a new graph\n",
    "with torch.no_grad():\n",
    "    generated_graph = model(noisy_graph.x, noisy_graph.edge_index, noisy_graph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86294393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5290, -0.5382,  0.2129,  0.2277, -1.6697],\n",
       "        [-0.5290, -0.5382,  0.2129,  0.2277, -1.6697],\n",
       "        [-0.5290, -0.5382,  0.2129,  0.2277, -1.6697],\n",
       "        ...,\n",
       "        [ 2.1337,  2.1224,  0.3285,  0.0069, -1.6697],\n",
       "        [ 2.1337,  2.1224,  0.3285,  0.0069, -1.6697],\n",
       "        [ 2.1337,  2.1224,  0.3285,  0.0069, -1.6697]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloned_graph.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c499c386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.0679,   0.1566,   0.0869,  -0.0210,   0.0487],\n",
       "        [ -0.0679,   0.1566,   0.0869,  -0.0210,   0.0487],\n",
       "        [ -0.0679,   0.1566,   0.0869,  -0.0210,   0.0487],\n",
       "        ...,\n",
       "        [ -1.3510,  -5.5763, -11.5359,  -4.2516,  -4.0746],\n",
       "        [ -1.0962,  -5.8635, -12.1308,  -4.4616,  -4.4294],\n",
       "        [ -0.6836,  -4.9054, -12.6395,  -3.0544,  -2.7997]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
