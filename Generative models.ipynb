{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import networkx    as nx\n",
    "\n",
    "from torch.utils.data       import random_split\n",
    "from torch_geometric.utils  import convert\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn     import GraphConv, Linear\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs      = 1000\n",
    "batch_size    = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Number of diffusing and denoising steps, which can be different\n",
    "n_diffusing_steps = 10\n",
    "n_denoising_steps = 10\n",
    "\n",
    "# Dropouts for node and edge models (work independently)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Target to generate new crystals\n",
    "target = 'D'\n",
    "\n",
    "input_folder  = '../MP/models'\n",
    "target_folder = f'{input_folder}/{target}'\n",
    "model_name    = f'{target_folder}/model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d59a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "try:    \n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "except FileNotFoundError:\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    ### Santadirizing properties\n",
    "\n",
    "    # Compute means and standard deviations\n",
    "\n",
    "    target_list = torch.tensor([])\n",
    "    edge_list   = torch.tensor([])\n",
    "\n",
    "    for data in dataset:\n",
    "        target_list = torch.cat((target_list, data.y),         0)\n",
    "        edge_list   = torch.cat((edge_list,   data.edge_attr), 0)\n",
    "\n",
    "    scale = 1e0\n",
    "\n",
    "    target_mean = torch.mean(target_list)\n",
    "    target_std  = torch.std(target_list)\n",
    "\n",
    "    edge_mean = torch.mean(edge_list)\n",
    "    edge_std  = torch.std(edge_list)\n",
    "\n",
    "    target_factor = target_std / scale\n",
    "    edge_factor   = edge_std   / scale\n",
    "\n",
    "    # Update normalized values into the database\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y         = (data.y         - target_mean) / target_factor\n",
    "        data.edge_attr = (data.edge_attr - edge_mean)   / edge_factor\n",
    "\n",
    "    # Same for the node features\n",
    "\n",
    "    feat_mean = torch.tensor([])\n",
    "    feat_std  = torch.tensor([])\n",
    "\n",
    "    for feat_index in range(dataset[0].num_node_features):\n",
    "        feat_list = torch.tensor([])\n",
    "\n",
    "        for data in dataset:\n",
    "            feat_list = torch.cat((feat_list, data.x[:, feat_index]), 0)\n",
    "\n",
    "        feat_mean = torch.cat((feat_mean, torch.tensor([torch.mean(feat_list)])), 0)\n",
    "        feat_std  = torch.cat((feat_std,  torch.tensor([torch.std(feat_list)])),  0)\n",
    "\n",
    "        for data in dataset:\n",
    "            data.x[:, feat_index] = (data.x[:, feat_index] - feat_mean[feat_index]) * scale / feat_std[feat_index]\n",
    "    \n",
    "    parameters = [target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale]\n",
    "    \n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c0cbf",
   "metadata": {},
   "source": [
    "# Generation of diffusing and denoising Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a554a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_t(t, T, s=1e-2):\n",
    "    \"\"\"Defines constant alpha at time-step t, given a parameter s < 0.5 (else alpha increases).\n",
    "    \n",
    "    Args:\n",
    "        t (int):   time step (of diffusion or denoising) in which alpha is required.\n",
    "        T (int):   total number of steps.\n",
    "        s (float): parameter which controls the decay of alpha with t.\n",
    "    \n",
    "    Returns:\n",
    "        alpha (float): parameter which controls the velocity of diffusion or denoising.\n",
    "    \"\"\"\n",
    "    \n",
    "    return torch.tensor((1 - 2 * s) * (1 - (t / T)**2) + s)\n",
    "\n",
    "\n",
    "def get_random_graph(n_nodes, n_features, in_edge_index=None, n_edges=None):\n",
    "    \"\"\"Generates a random graph with specified number of nodes and features, and attributes. It is assumed\n",
    "    that all parameters are normally distributed N(0, 1).\n",
    "    \n",
    "    Args:\n",
    "        n_nodes       (int):   Number of nodes.\n",
    "        n_features    (int):   Number of features for each node.\n",
    "        in_edge_index (array): Positions of high-symmetry points in k-space (if None, they are randomized).\n",
    "        n_edges       (int):   Number of edges, if edge_index is randomized (if None, it is randomized).\n",
    "    \n",
    "    Returns:\n",
    "        graph (torch_geometric.data.data.Data): Graph structure with random node features and edge attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if in_edge_index is None:  # Randomize edge indexes\n",
    "        if n_edges is None:  # Randomize number of edges\n",
    "            n_edges = torch.randint(low=50, high=101, size=(1,)).item()\n",
    "        edge_index = torch.randn(2, n_edges)\n",
    "    else:\n",
    "        # Clone edge indexes\n",
    "        edge_index = torch.clone(in_edge_index)\n",
    "    \n",
    "    # Get number of edges\n",
    "    n_edges = torch.Tensor.size(edge_index)[1]\n",
    "    \n",
    "    # Generate random node features\n",
    "    x = torch.randn(n_nodes, n_features)\n",
    "    \n",
    "    # Generate random edge attributes\n",
    "    edge_attr = torch.randn(n_edges, 1)\n",
    "    \n",
    "    # Define graph with generated inputs\n",
    "    graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def diffusion_step(graph_0, t, n_diffusing_steps):\n",
    "    \"\"\"Performs a forward step of a diffusive, Markov chain.\n",
    "    \n",
    "    Args:\n",
    "        graph_0 (torch_geometric.data.data.Data): Graph which is to be diffused (step t-1).\n",
    "    \n",
    "    Returns:\n",
    "        graph_t (torch_geometric.data.data.Data): Diffused graph (step t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clone graph that we are diffusing (not extrictly necessary)\n",
    "    graph_t = graph_0.clone()\n",
    "    \n",
    "    # Number of nodes and features in the graph\n",
    "    n_nodes, n_features = torch.Tensor.size(graph_t.x)\n",
    "    \n",
    "    # Generate gaussian (normal) noise\n",
    "    epsilon = get_random_graph(n_nodes, n_features, graph_t.edge_index)\n",
    "    \n",
    "    # Compute alpha_t\n",
    "    alpha_t = get_alpha_t(t, n_diffusing_steps)\n",
    "    \n",
    "    # Forward pass\n",
    "    graph_t.x         = torch.sqrt(alpha_t) * graph_t.x         + torch.sqrt(1 - alpha_t) * epsilon.x\n",
    "    graph_t.edge_attr = torch.sqrt(alpha_t) * graph_t.edge_attr + torch.sqrt(1 - alpha_t) * epsilon.edge_attr\n",
    "    return graph_t\n",
    "\n",
    "\n",
    "def diffuse(graph_0, n_diffusing_steps):\n",
    "    \"\"\"Performs consecutive steps of diffusion in a reference graph.\n",
    "    \n",
    "    Args:\n",
    "        graph_0           (torch_geometric.data.data.Data): Reference graph to be diffused (step t-1).\n",
    "        n_diffusing_steps (int):                            Number of diffusive steps.\n",
    "    \n",
    "    Returns:\n",
    "        graph_t (torch_geometric.data.data.Data): Graph with random node features and edge attributes (step t).\n",
    "    \"\"\"\n",
    "    \n",
    "    graph_t = graph_0.clone()\n",
    "    for t in range(n_diffusing_steps):\n",
    "        graph_t = diffusion_step(graph_t, t, n_diffusing_steps)\n",
    "    return graph_t\n",
    "\n",
    "\n",
    "def denoising_step(graph_t, epsilon, t, n_denoising_steps):\n",
    "    \"\"\"Performs a forward step of a denoising chain.\n",
    "    \n",
    "    Args:\n",
    "        graph_t (torch_geometric.data.data.Data): Graph which is to be denoised (step t).\n",
    "        epsilon (torch_geometric.data.data.Data): Predicted noise to substract.\n",
    "    \n",
    "    Returns:\n",
    "        graph_0 (torch_geometric.data.data.Data): Denoised graph (step t-1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clone graph that we are denoising (not extrictly necessary)\n",
    "    graph_0 = graph_t.clone()\n",
    "    \n",
    "    # Compute alpha_t\n",
    "    alpha_t = get_alpha_t(t, n_denoising_steps)\n",
    "    \n",
    "    # Backard pass\n",
    "    graph_0.x         = graph_0.x         / torch.sqrt(alpha_t) - torch.sqrt((1 - alpha_t) / alpha_t) * epsilon.x\n",
    "    graph_0.edge_attr = graph_0.edge_attr / torch.sqrt(alpha_t) - torch.sqrt((1 - alpha_t) / alpha_t) * epsilon.edge_attr\n",
    "    return graph_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db2981",
   "metadata": {},
   "source": [
    "# Generation of Graph Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d809da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nGCNN(torch.nn.Module):\n",
    "    \"\"\"Graph convolution neural network for the prediction of node embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_channels, pdropout):\n",
    "        super(nGCNN, self).__init__()\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        # Define graph convolution layers\n",
    "        self.conv1 = GraphConv(features_channels, 512)\n",
    "        self.conv2 = GraphConv(512, 512)\n",
    "        \n",
    "        # Define linear layers\n",
    "        self.linconv = Linear(512, 16)\n",
    "        self.lin     = Linear(16, 1)\n",
    "        \n",
    "        self.pdropout = pdropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        ## CONVOLUTION\n",
    "        \n",
    "        # Apply graph convolution with ReLU activation function\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "\n",
    "class eGCNN(nn.Module):\n",
    "    \"\"\"Graph convolution neural network for the prediction of edge attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_channels, pdropout):\n",
    "        super(eGCNN, self).__init__()\n",
    "\n",
    "        self.linear1 = Linear(features_channels, 32)\n",
    "        self.linear2 = Linear(32, features_channels)\n",
    "\n",
    "        self.pdropout = pdropout\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        # Dot product between node distances (?)\n",
    "        x = x_i * x_j\n",
    "        \n",
    "        # Linear convolutions\n",
    "        x = self.linear1(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # Dropout layer (only for training)\n",
    "        x = F.dropout(x, p=self.pdropout, training=self.training)\n",
    "        \n",
    "        # Last linear convolution\n",
    "        x = self.linear2(x)\n",
    "        x = x.relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce282d51",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859c2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 171\n",
      "Number of testing  graphs: 43\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dbedf1",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1aa0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nodes:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(5, 512)\n",
      "  (conv2): GraphConv(512, 512)\n",
      "  (linconv): Linear(512, 16, bias=True)\n",
      "  (lin): Linear(16, 1, bias=True)\n",
      ")\n",
      "\n",
      "Edges:\n",
      "eGCNN(\n",
      "  (linear1): Linear(100, 64, bias=True)\n",
      "  (linear2): Linear(64, 64, bias=True)\n",
      "  (linear3): Linear(64, 100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset\n",
    "n_features = dataset[0].num_node_features\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_features, dropout_edge).to(device)\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d41dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_node = nn.MSELoss()\n",
    "criterion_edge = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    for graph in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Diffuse the graph with some noise\n",
    "        diffused_graph = diffuse(graph, n_diffusing_steps)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        denoised_graph = diffused_graph.clone()\n",
    "        for t in range(n_denoising_steps):\n",
    "            # Perform a single forward pass for predicting node features\n",
    "            out_x = node_model(diffused_graph.x, \n",
    "                               diffused_graph.edge_index,\n",
    "                               diffused_graph.edge_attr)\n",
    "            \n",
    "            # Define edges\n",
    "            # DEFINE x_i and x_j as connected nodes\n",
    "            \n",
    "            # Perform a single forward pass for predicting edge attributes\n",
    "            out_attr = edge_model(x_i, \n",
    "                                  x_j)\n",
    "\n",
    "            # Construct noise graph\n",
    "            noise_graph = Data(x=out_x, edge_index=diffused_graph.edge_index, edge_attr=out_attr)\n",
    "\n",
    "            # Denoise the graph with the predicted noise\n",
    "            denoised_graph = denoising_step(diffused_graph, noise_graph, t, n_denoising_steps)\n",
    "\n",
    "        # Calculate the loss for node features\n",
    "        loss_node = criterion_node(graph.x, denoised_graph.x)\n",
    "\n",
    "        # Calculate the loss for edge attributes\n",
    "        loss_edge = criterion_edge(graph.edge_attr, denoised_graph.edge_attr)\n",
    "\n",
    "        # Accumulate the total training loss\n",
    "        loss = loss_node + loss_edge\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
