{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn   as nn\n",
    "import numpy      as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from libraries.model   import nGCNN, eGCNN, denoise, get_random_graph\n",
    "from libraries.dataset import revert_standardize_dataset\n",
    "from libraries.graph   import POSCAR_graph_encoding\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import MP.MP_library as MPL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a88fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From random noise, we generate completely new materials\n",
    "# A target property can be seeked with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder in which all data will be stored\n",
    "target_folder    = 'models/GM_BiSI'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "\n",
    "# Number of graphs to predict\n",
    "N_predictions = 10\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = 0.1\n",
    "\n",
    "# Define target to be generated\n",
    "target_tensor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9310c",
   "metadata": {},
   "source": [
    "# Load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24741239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in JSON format to a dictionary\n",
    "with open(f'{target_folder}/model_parameters.json', 'r') as json_file:\n",
    "    model_parameters = json.load(json_file)\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = model_parameters['n_t_steps']\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = model_parameters['noise_contribution']\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = model_parameters['dropout_node']\n",
    "dropout_edge = model_parameters['dropout_edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b95aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_dataset_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset, with corresponding labels and parameters\n",
    "dataset = torch.load(dataset_name_std)\n",
    "labels  = torch.load(labels_name)\n",
    "\n",
    "# Read the file in JSON format to a dictionary\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    dataset_parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066eb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.0, 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the number of nodes\n",
    "total_nodes = torch.tensor([data.num_nodes for data in dataset])\n",
    "mean_nodes  = torch.mean(total_nodes.float()).item()\n",
    "std_nodes   = torch.std(total_nodes.float()).item()\n",
    "\n",
    "mean_nodes, std_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1aa0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(5, 256)\n",
      "  (conv2): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset, considering the t_step information\n",
    "n_features = dataset[0].num_node_features + 1\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_features, dropout_node).to(device)\n",
    "node_model.load_state_dict(torch.load(node_model_name))\n",
    "node_model.eval()\n",
    "\n",
    "edge_model = eGCNN(n_features, dropout_edge).to(device)\n",
    "edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "edge_model.eval()\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a0f21",
   "metadata": {},
   "source": [
    "# Generating new cystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f5788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556]),\n",
       " Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting loop\n",
    "predicted_dataset = []\n",
    "with torch.no_grad():\n",
    "    for i in range(N_predictions):\n",
    "        # Get random number of nodes\n",
    "        n_nodes = int(np.random.normal(mean_nodes, std_nodes))\n",
    "        \n",
    "        # Get random graph, acting as diffused\n",
    "        diffused_graph = get_random_graph(n_nodes, n_features-1)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        #print(f'Denoising...')\n",
    "        denoised_graph, _ = denoise(diffused_graph, n_t_steps, node_model, edge_model,\n",
    "                                    s=alpha_decay, sigma=sigma, target=target_tensor)\n",
    "        \n",
    "        # Append generated graph\n",
    "        predicted_dataset.append(denoised_graph)\n",
    "\n",
    "# Revert stardadization\n",
    "denoised_graphs = revert_standardize_dataset(predicted_dataset, dataset_parameters)\n",
    "denoised_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57947938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "6 34 51 3.9683318 4.2198825 4.2909393 1.9079686335276842 3.763916033617075\n",
      "\n",
      "2\n",
      "53 58 66 3.9628825 4.19982 4.3164225 1.8561518409550293 3.7673848656088764\n",
      "\n",
      "3\n",
      "3 16 67 4.135252 3.9470856 4.398083 1.6125568719214631 3.602658076798378\n",
      "\n",
      "4\n",
      "10 30 70 4.0659122 4.17059 4.220621 1.981329058213144 3.669898564820485\n",
      "\n",
      "5\n",
      "6 52 67 4.2347274 4.0536833 4.204234 1.9705730825872902 3.5424835734825955\n",
      "\n",
      "6\n",
      "31 39 68 4.0385537 4.448844 4.052917 2.436019649995078 3.7226362714639616\n",
      "\n",
      "7\n",
      "33 38 39 4.030197 4.175176 4.264099 1.9219957461202968 3.706484612455335\n",
      "\n",
      "8\n",
      "32 50 51 4.3014874 3.9877 4.2197437 1.9293684445996504 3.489883748690338\n",
      "\n",
      "9\n",
      "18 60 68 3.9393504 4.150427 4.358319 1.7451585131772733 3.7656958085429166\n",
      "\n",
      "10\n",
      "9 39 62 4.007904 4.084174 4.552939 1.4988533035577924 3.799199562078272\n"
     ]
    }
   ],
   "source": [
    "lattice_vectors = np.array([[12, 0, 0],\n",
    "                            [0, 17, 0],\n",
    "                            [0, 0, 10]])\n",
    "for i in range(N_predictions):\n",
    "    print()\n",
    "    print(i+1)\n",
    "    graph = denoised_graphs[i].clone()\n",
    "    try:\n",
    "        POSCAR_graph_encoding(graph, lattice_vectors, file_name=f'POSCAR-{i}', POSCAR_directory='./')\n",
    "    except SystemExit:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
