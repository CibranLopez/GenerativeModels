{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:12:46.236501896Z",
     "start_time": "2024-03-06T10:12:45.235799134Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from libraries.dataset    import standardize_dataset\n",
    "from libraries.graph      import graph_POSCAR_encoding\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a85832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:12:46.243113089Z",
     "start_time": "2024-03-06T10:12:46.237929170Z"
    }
   },
   "outputs": [],
   "source": [
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "data_path = '../MP/MOSES_dataset.txt'\n",
    "\n",
    "# Define folder in which all data will be stored\n",
    "data_folder = f'data/MOSES_dataset'\n",
    "\n",
    "# Define name for storing dataset basic description\n",
    "dataset_parameters_name = f'{data_folder}/dataset_parameters.json'\n",
    "\n",
    "encoding_type = 'molecule'  # 'voronoi' or 'sphere-images'\n",
    "\n",
    "# Define basic dataset parameters for tracking data\n",
    "dataset_parameters = {\n",
    "    'input_folder': data_path,\n",
    "    'output_folder': data_folder,\n",
    "    'encoding_type': encoding_type\n",
    "}\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.system(f'mkdir {data_folder}')\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "with open(dataset_parameters_name, 'w') as json_file:\n",
    "    json.dump(dataset_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standardized if possible."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:12:46] SMILES Parse Error: syntax error while parsing: SMILES\n",
      "[11:12:46] SMILES Parse Error: Failed parsing SMILES 'SMILES' for input: 'SMILES'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: SMILES not loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1936963, 1936962)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the raw dataset from scratch, and standardize it\n",
    "\n",
    "# Read all molecules within the database\n",
    "with open(data_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "total_structures  = 0\n",
    "loaded_structures = 0\n",
    "\n",
    "dataset = []\n",
    "labels  = []\n",
    "for line in lines:\n",
    "    total_structures += 1\n",
    "    \n",
    "    # Define molecule appart from train-test splitting\n",
    "    molecule = line.split(',')[0]\n",
    "    #print()\n",
    "    #print(molecule)\n",
    "    try:\n",
    "        nodes, edges, attributes = graph_POSCAR_encoding(molecule,\n",
    "                                                         encoding_type=encoding_type)\n",
    "    except:\n",
    "        print(f'Error: {molecule} not loaded')\n",
    "        continue\n",
    "    \n",
    "    # Construct temporal graph structure\n",
    "    graph = Data(x=nodes,\n",
    "                 edge_index=edges.t().contiguous(),\n",
    "                 edge_attr=attributes.ravel()\n",
    "                )\n",
    "\n",
    "    # Append to dataset and labels\n",
    "    dataset.append(graph)\n",
    "    labels.append(molecule)\n",
    "    \n",
    "    loaded_structures += 1\n",
    "total_structures, loaded_structures"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:22:51.047950236Z",
     "start_time": "2024-03-06T10:12:46.241579254Z"
    }
   },
   "id": "99734e3fd172ded7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# standardize_dataset requires some graph-level embedding\n",
    "for graph in dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:22:59.103275021Z",
     "start_time": "2024-03-06T10:22:51.047857852Z"
    }
   },
   "id": "76ae0f30e86e4a07",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614dc386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:28:52.690396627Z",
     "start_time": "2024-03-06T10:22:59.104080972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "dataset_std, dataset_parameters = standardize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64167d60-b767-42cd-881f-dba4f0a102cb",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels_name                 = f'{data_folder}/labels.pt'\n",
    "dataset_name                = f'{data_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{data_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{data_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "torch.save(labels,      labels_name)\n",
    "torch.save(dataset,     dataset_name)\n",
    "torch.save(dataset_std, dataset_name_std)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "    json.dump(numpy_dict, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:34:58.895998140Z",
     "start_time": "2024-03-06T10:28:52.731843386Z"
    }
   },
   "id": "dd6b6cc036cde0ec",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
