{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import networkx    as nx\n",
    "\n",
    "from torch.utils.data       import random_split\n",
    "from torch_geometric.utils  import convert\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn     import GraphConv\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs      = 1000\n",
    "batch_size    = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "n_diffusing_steps = 10\n",
    "n_denoising_steps = 10\n",
    "\n",
    "dropout_node = 0.4\n",
    "dropout_edge = 0.4\n",
    "\n",
    "n_edges = 100\n",
    "\n",
    "target = 'D'\n",
    "\n",
    "input_folder  = '../MP/models'\n",
    "target_folder = f'{input_folder}/{target}'\n",
    "model_name    = f'{target_folder}/model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08d59a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "try:    \n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "except FileNotFoundError:\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    ### Santadirizing properties\n",
    "\n",
    "    # Compute means and standard deviations\n",
    "\n",
    "    target_list = torch.tensor([])\n",
    "    edge_list   = torch.tensor([])\n",
    "\n",
    "    for data in dataset:\n",
    "        target_list = torch.cat((target_list, data.y),         0)\n",
    "        edge_list   = torch.cat((edge_list,   data.edge_attr), 0)\n",
    "\n",
    "    scale = 1e0\n",
    "\n",
    "    target_mean = torch.mean(target_list)\n",
    "    target_std  = torch.std(target_list)\n",
    "\n",
    "    edge_mean = torch.mean(edge_list)\n",
    "    edge_std  = torch.std(edge_list)\n",
    "\n",
    "    target_factor = target_std / scale\n",
    "    edge_factor   = edge_std   / scale\n",
    "\n",
    "    # Update normalized values into the database\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y         = (data.y         - target_mean) / target_factor\n",
    "        data.edge_attr = (data.edge_attr - edge_mean)   / edge_factor\n",
    "\n",
    "    # Same for the node features\n",
    "\n",
    "    feat_mean = torch.tensor([])\n",
    "    feat_std  = torch.tensor([])\n",
    "\n",
    "    for feat_index in range(dataset[0].num_node_features):\n",
    "        feat_list = torch.tensor([])\n",
    "\n",
    "        for data in dataset:\n",
    "            feat_list = torch.cat((feat_list, data.x[:, feat_index]), 0)\n",
    "\n",
    "        feat_mean = torch.cat((feat_mean, torch.tensor([torch.mean(feat_list)])), 0)\n",
    "        feat_std  = torch.cat((feat_std,  torch.tensor([torch.std(feat_list)])),  0)\n",
    "\n",
    "        for data in dataset:\n",
    "            data.x[:, feat_index] = (data.x[:, feat_index] - feat_mean[feat_index]) * scale / feat_std[feat_index]\n",
    "    \n",
    "    parameters = [target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale]\n",
    "    \n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2716665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = dataset.copy()\n",
    "dataset = []\n",
    "for data in old:\n",
    "    if data.num_edges > n_edges:\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        edge_values, edge_indices = torch.sort(edge_attr, dim=0)\n",
    "        selected_values = edge_values[:n_edges]\n",
    "        selected_indices = edge_indices[:n_edges]\n",
    "\n",
    "        selected_data = Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index[:, selected_indices].squeeze(),\n",
    "                edge_attr=edge_attr[selected_indices].squeeze(1)\n",
    "            )\n",
    "\n",
    "        dataset.append(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7545180c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[323, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[323, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[323, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[323, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[323, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[323, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[322, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[179, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[179, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[179, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[324, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[322, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[192, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[191, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[250, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[250, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[250, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[250, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[317, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[317, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[317, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[160, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[160, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[152, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[152, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[239, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[239, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[239, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[287, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[287, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[287, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[287, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[288, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[224, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[223, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[223, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[223, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[216, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[215, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[216, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[216, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[216, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[215, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[215, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[215, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[215, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[224, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[223, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[223, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[223, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[336, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[336, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[336, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[335, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[335, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[335, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[270, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[270, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[270, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[270, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[270, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[267, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[267, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[267, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[267, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[256, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[256, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[256, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[256, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[256, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[212, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[212, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[212, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[212, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[432, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[432, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[432, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[430, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[430, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[430, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[832, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[830, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[200, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[196, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[196, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[196, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[312, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[384, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[384, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[384, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[208, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[207, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[384, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[384, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[384, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[382, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[320, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[315, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[315, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[315, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[315, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[128, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[128, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[128, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[128, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[128, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[128, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[124, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[124, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[124, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[124, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[124, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[124, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[180, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[179, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[179, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[179, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[18, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[18, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[18, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[18, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[32, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[36, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[36, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[68, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[24, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[32, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[32, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[13, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[30, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[30, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[76, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[76, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[24, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[50, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[50, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[46, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[100, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[100, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[100, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[72, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[72, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[72, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[20, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[28, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[12, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[12, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[12, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[20, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[18, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[12, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[20, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[104, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[108, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[13, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[22, 5], edge_index=[2, 100], edge_attr=[100, 1]),\n",
       " Data(x=[64, 5], edge_index=[2, 100], edge_attr=[100, 1])]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c0cbf",
   "metadata": {},
   "source": [
    "# Generation of Graph Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a554a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_t(t, T):\n",
    "    \"\"\"Defines constant alpha at time-step t.\n",
    "    \"\"\"\n",
    "    s = 1e-5\n",
    "    return torch.tensor((1 - 2 * s) * (1 - (t / T)**2) + s)\n",
    "\n",
    "\n",
    "def get_random_graph(n_nodes, n_features, in_edge_index=None, n_edges=None):\n",
    "    \"\"\"Generates a random graph with specified number of nodes and features, and attributes. It is assumed\n",
    "    that all parameters are normally distributed N(0, 1).\n",
    "    \n",
    "    Args:\n",
    "        n_nodes       (int):   Number of nodes.\n",
    "        n_features    (int):   Number of features for each node.\n",
    "        in_edge_index (array): Positions of high-symmetry points in k-space (if None, they are randomized).\n",
    "        n_edges       (int):   Number of edges, if edge_index is randomized (if None, it is randomized).\n",
    "    \n",
    "    Returns:\n",
    "        graph (torch_geometric.data.data.Data): Graph structure with random node features and edge attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if in_edge_index is None:  # Randomize edge indexes\n",
    "        if n_edges is None:  # Randomize number of edges\n",
    "            n_edges = torch.randint(low=50, high=101, size=(1,)).item()\n",
    "        edge_index = torch.randn(2, n_edges)\n",
    "    else:\n",
    "        # Clone edge indexes\n",
    "        edge_index = torch.clone(in_edge_index)\n",
    "    \n",
    "    # Get number of edges\n",
    "    n_edges = torch.Tensor.size(edge_index)[1]\n",
    "    \n",
    "    # Generate random node features\n",
    "    x = torch.randn(n_nodes, n_features)\n",
    "    \n",
    "    # Generate random edge attributes\n",
    "    edge_attr = torch.randn(n_edges, 1)\n",
    "    \n",
    "    # Define graph with generated inputs\n",
    "    graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def diffusion_step(graph_0, t, n_diffusing_steps):\n",
    "    \"\"\"Performs a forward step of a diffusive, Markov chain.\n",
    "    \n",
    "    Args:\n",
    "        graph_0 (torch_geometric.data.data.Data): Graph which is to be diffused (step t-1).\n",
    "    \n",
    "    Returns:\n",
    "        graph_t (torch_geometric.data.data.Data): Diffused graph (step t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clone graph that we are diffusing (not extrictly necessary)\n",
    "    graph_t = graph_0.clone()\n",
    "    \n",
    "    # Number of nodes and features in the graph\n",
    "    n_nodes, n_features = torch.Tensor.size(graph_t.x)\n",
    "    \n",
    "    # Generate gaussian (normal) noise\n",
    "    epsilon = get_random_graph(n_nodes, n_features, graph_t.edge_index)\n",
    "    \n",
    "    # Compute alpha_t\n",
    "    alpha_t = get_alpha_t(t, n_diffusing_steps)\n",
    "    \n",
    "    # Forward pass\n",
    "    print(torch.sqrt(alpha_t), torch.sqrt(1 - alpha_t))\n",
    "    graph_t.x         = torch.sqrt(alpha_t) * graph_t.x         + torch.sqrt(1 - alpha_t) * epsilon.x\n",
    "    graph_t.edge_attr = torch.sqrt(alpha_t) * graph_t.edge_attr + torch.sqrt(1 - alpha_t) * epsilon.edge_attr\n",
    "    return graph_t\n",
    "\n",
    "\n",
    "def diffuse(graph_0, n_diffusing_steps):\n",
    "    \"\"\"Performs consecutive steps of diffusion in a reference graph.\n",
    "    \n",
    "    Args:\n",
    "        graph_0           (torch_geometric.data.data.Data): Reference graph to be diffused (step t-1).\n",
    "        n_diffusing_steps (int):                            Number of diffusive steps.\n",
    "    \n",
    "    Returns:\n",
    "        graph_t (torch_geometric.data.data.Data): Graph with random node features and edge attributes (step t).\n",
    "    \"\"\"\n",
    "    \n",
    "    graph_t = graph_0.clone()\n",
    "    for t in range(n_diffusing_steps):\n",
    "        graph_t = diffusion_step(graph_t, t, n_diffusing_steps)\n",
    "    return graph_t\n",
    "\n",
    "\n",
    "def denoising_step(graph_t, epsilon, t, n_denoising_steps):\n",
    "    \"\"\"Performs a forward step of a denoising chain.\n",
    "    \n",
    "    Args:\n",
    "        graph_t (torch_geometric.data.data.Data): Graph which is to be denoised (step t).\n",
    "        epsilon (torch_geometric.data.data.Data): Predicted noise to substract.\n",
    "    \n",
    "    Returns:\n",
    "        graph_0 (torch_geometric.data.data.Data): Denoised graph (step t-1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clone graph that we are denoising (not extrictly necessary)\n",
    "    graph_0 = graph_t.clone()\n",
    "    \n",
    "    # Compute alpha_t\n",
    "    alpha_t = get_alpha_t(t, n_denoising_steps)\n",
    "    \n",
    "    # Backard pass\n",
    "    graph_0.x         = graph_0.x         / torch.sqrt(alpha_t) - torch.sqrt((1 - alpha_t) / alpha_t) * epsilon.x\n",
    "    graph_0.edge_attr = graph_0.edge_attr / torch.sqrt(alpha_t) - torch.sqrt((1 - alpha_t) / alpha_t) * epsilon.edge_attr\n",
    "    return graph_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3c3bd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(torch.nn.Module):\n",
    "    \"\"\"Graph generative denoising neural network with edge diffusion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_edges, dropout_node, dropout_edge):\n",
    "        \"\"\"Instantiate constants for the class.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DDPM, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = GraphConv(n_features, 64)\n",
    "        self.conv2 = GraphConv(64, 64)\n",
    "        \n",
    "        # Linear layers\n",
    "        self.conv_node = nn.Linear(64, n_features)\n",
    "        self.conv_edge = nn.Linear(64, n_edges)\n",
    "        \n",
    "        # Drop-outs\n",
    "        self.dropout_node = dropout_node\n",
    "        self.dropout_edge = dropout_edge\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"Denoising process: predicts the noise added to the original graph.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Deep mesh to predict the noise\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        \n",
    "        # Predict node features and edge attributes\n",
    "        node_output = self.conv_node(x)\n",
    "        edge_output = self.conv_edge(x)\n",
    "        return node_output, edge_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c6afc16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 171\n",
      "Number of testing  graphs: 43\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7f8dae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPM(\n",
      "  (conv1): GraphConv(5, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv_node): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (conv_edge): Linear(in_features=64, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset\n",
    "n_features = dataset[0].num_node_features\n",
    "\n",
    "# Instantiate the model\n",
    "model = DDPM(n_features, n_edges, dropout_node, dropout_edge).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "65e7d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000) tensor(0.0032)\n",
      "tensor(0.9950) tensor(0.1000)\n",
      "tensor(0.9798) tensor(0.2000)\n",
      "tensor(0.9539) tensor(0.3000)\n",
      "tensor(0.9165) tensor(0.4000)\n",
      "tensor(0.8660) tensor(0.5000)\n",
      "tensor(0.8000) tensor(0.6000)\n",
      "tensor(0.7141) tensor(0.7000)\n",
      "tensor(0.6000) tensor(0.8000)\n",
      "tensor(0.4359) tensor(0.9000)\n",
      "tensor([[-2.7198e-01,  2.3485e-01, -2.6536e-02,  1.9663e-01,  9.5975e-04],\n",
      "        [-3.3694e-02, -1.8026e-01,  5.0951e-02, -2.2222e-01, -2.9849e-02],\n",
      "        [ 2.3274e-01, -6.8746e-01, -2.7226e-02, -4.5049e-01, -4.0362e-01],\n",
      "        ...,\n",
      "        [ 5.4583e-02, -4.6608e-01,  1.2975e-01, -3.1921e-01, -4.1198e-02],\n",
      "        [ 1.4452e-01, -2.9840e-01,  5.6992e-02, -3.7838e-01, -1.8660e-02],\n",
      "        [-1.0753e-04, -5.2201e-01,  2.3468e-01, -2.2276e-01,  9.1747e-02]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[-1.1318e-01,  2.2570e-01, -1.5103e-01,  ...,  3.4998e-01,\n",
      "         -2.0211e-01,  1.4038e-02],\n",
      "        [-7.0035e-02, -1.3265e-01, -9.1962e-02,  ...,  1.0376e-01,\n",
      "          1.1644e-01,  7.0077e-02],\n",
      "        [-9.9988e-02, -2.7411e-01,  1.6596e-01,  ...,  1.9039e-02,\n",
      "          5.5963e-01, -2.5797e-01],\n",
      "        ...,\n",
      "        [-2.7883e-01, -2.9338e-01, -1.5587e-01,  ...,  1.7209e-01,\n",
      "          3.4951e-01,  2.4173e-02],\n",
      "        [-2.1963e-01, -1.8131e-01, -1.1489e-01,  ...,  8.9172e-02,\n",
      "          2.8013e-01,  1.4198e-01],\n",
      "        [-4.9335e-01, -2.5342e-01, -2.2481e-01,  ...,  3.2203e-01,\n",
      "          3.8263e-01,  3.0357e-04]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (324) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     noise_graph \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mout_x, edge_index\u001b[38;5;241m=\u001b[39mdiffused_graph\u001b[38;5;241m.\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39mout_attr)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Denoise the graph with the predicted noise\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     denoised_graph \u001b[38;5;241m=\u001b[39m \u001b[43mdenoising_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffused_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_denoising_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate the loss for node features\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss_node \u001b[38;5;241m=\u001b[39m criterion_node(graph\u001b[38;5;241m.\u001b[39mx, denoised_graph\u001b[38;5;241m.\u001b[39mx)\n",
      "Cell \u001b[0;32mIn[161], line 109\u001b[0m, in \u001b[0;36mdenoising_step\u001b[0;34m(graph_t, epsilon, t, n_denoising_steps)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Backard pass\u001b[39;00m\n\u001b[1;32m    108\u001b[0m graph_0\u001b[38;5;241m.\u001b[39mx         \u001b[38;5;241m=\u001b[39m graph_t\u001b[38;5;241m.\u001b[39mx         \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(alpha_t) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_t) \u001b[38;5;241m/\u001b[39m alpha_t) \u001b[38;5;241m*\u001b[39m epsilon\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m--> 109\u001b[0m graph_0\u001b[38;5;241m.\u001b[39medge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_0\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (324) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_node = nn.MSELoss()\n",
    "criterion_edge = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    for graph in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Diffuse the graph with some noise\n",
    "        diffused_graph = diffuse(graph, n_diffusing_steps)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        denoised_graph = diffused_graph.clone()\n",
    "        for t in range(n_denoising_steps):\n",
    "            # Perform a single forward pass\n",
    "            out_x, out_attr = model(diffused_graph.x, \n",
    "                                    diffused_graph.edge_index,\n",
    "                                    diffused_graph.edge_attr)\n",
    "\n",
    "            # Construct noise graph\n",
    "            noise_graph = Data(x=out_x, edge_index=diffused_graph.edge_index, edge_attr=out_attr)\n",
    "\n",
    "            # Denoise the graph with the predicted noise\n",
    "            denoised_graph = denoising_step(diffused_graph, noise_graph, t, n_denoising_steps)\n",
    "\n",
    "        # Calculate the loss for node features\n",
    "        loss_node = criterion_node(graph.x, denoised_graph.x)\n",
    "\n",
    "        # Calculate the loss for edge attributes\n",
    "        loss_edge = criterion_edge(graph.edge_attr, denoised_graph.edge_attr)\n",
    "\n",
    "        # Accumulate the total training loss\n",
    "        loss = loss_node + loss_edge\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bac14",
   "metadata": {},
   "source": [
    "criterion_node = nn.MSELoss()\n",
    "criterion_edge = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    for graph in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Diffuse the graph with some noise\n",
    "        node_output, edge_output = diffuse(graph, n_diffusing_steps)\n",
    "        \n",
    "        # Construct diffused graph ???\n",
    "        diffused_graph = make_graph(node_output, edge_output)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        denoised_graph = diffused_graph.clone()\n",
    "        for i in range(n_denoising_steps):\n",
    "            # Perform a single forward pass\n",
    "            out_x = model(diffused_graph.x, \n",
    "                          diffused_graph.edge_index,\n",
    "                          diffused_graph.edge_attr).to(device)\n",
    "\n",
    "            # Construct noise graph\n",
    "            noise_graph = get_graph(out_x, out_attr)\n",
    "\n",
    "            # Denoise the graph with the predicted noise\n",
    "            denoised_graph = denoise(diffused_graph, noise_graph, n_diffusing_steps)\n",
    "\n",
    "        # Calculate the loss for node features\n",
    "        loss_node = criterion_node(graph.x, denoised_graph.x)\n",
    "\n",
    "        # Calculate the loss for edge attributes\n",
    "        loss_edge = criterion_edge(graph.edge_attr, denoised_graph.edge_attr)\n",
    "\n",
    "        # Accumulate the total training loss\n",
    "        loss = loss_node + loss_edge\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for graph in test_loader:\n",
    "            # Diffuse the graph with some noise\n",
    "            node_output, edge_output = diffuse(graph, n_diffusing_steps)\n",
    "\n",
    "            # Construct diffused graph ???\n",
    "            diffused_graph = make_graph(node_output, edge_output)\n",
    "            \n",
    "            # Denoise the diffused graph\n",
    "            denoised_graph = diffused_graph.clone()\n",
    "            for i in range(n_denoising_steps):\n",
    "                # Perform a single forward pass\n",
    "                out_x = model(diffused_graph.x, \n",
    "                              diffused_graph.edge_index,\n",
    "                              diffused_graph.edge_attr).to(device)\n",
    "\n",
    "                # Construct noise graph\n",
    "                noise_graph = get_graph(out_x, out_attr)\n",
    "\n",
    "                # Denoise the graph with the predicted noise\n",
    "                denoised_graph = denoise(diffused_graph, noise_graph, n_diffusing_steps)\n",
    "            \n",
    "            # Calculate the loss for node features\n",
    "            loss_node = criterion_node(graph.x, denoised_graph.x)\n",
    "\n",
    "            # Calculate the loss for edge attributes\n",
    "            loss_edge = criterion_edge(graph.edge_attr, denoised_graph.edge_attr)\n",
    "\n",
    "            # Accumulate the total test loss\n",
    "            loss = loss_node + loss_edge\n",
    "            test_loss = loss.item()\n",
    "    \n",
    "    # Compute the average test loss\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
