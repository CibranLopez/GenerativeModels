{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import GM_library  as GML\n",
    "import numpy       as np\n",
    "import torch\n",
    "\n",
    "from os                     import path, listdir\n",
    "from torch.utils.data       import random_split\n",
    "from torch_geometric.data   import Data\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import MP.MP_library as MPL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 1000\n",
    "batch_size    = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Number of diffusing and denoising steps, which can be different\n",
    "n_diffusing_steps = 10\n",
    "n_denoising_steps = 10\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Define box shape\n",
    "L = [20, 20, 20]\n",
    "\n",
    "# Target to generate new crystals\n",
    "target = 'GM_EPA'\n",
    "\n",
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "DB_path = '../MP/Loaded_EMP'\n",
    "\n",
    "input_folder  = 'models'\n",
    "target_folder = f'{input_folder}/{target}'\n",
    "model_name    = f'{target_folder}/model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc945b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "if path.exists(dataset_name_std) and path.exists(labels_name) and path.exists(parameters_name_std):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "\n",
    "elif path.exists(dataset_name) and path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)\n",
    "\n",
    "else:\n",
    "    # Generate the raw dataset from scratch, and standardize it\n",
    "    \n",
    "    # Read all mateials within the database\n",
    "    materials = listdir(DB_path)\n",
    "    \n",
    "    dataset = []\n",
    "    labels  = []\n",
    "    for material in materials:\n",
    "        try:\n",
    "            # Try to read the polyforms\n",
    "            polymorfs = listdir(f'{DB_path}/{material}')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        print(material)\n",
    "        for polymorf in polymorfs:\n",
    "            # Path to folder containing the POSCAR\n",
    "            path_to_POSCAR = f'{DB_path}/{material}/{polymorf}'\n",
    "            \n",
    "            # Check that the folder is valid\n",
    "            if path.exists(f'{path_to_POSCAR}/POSCAR'):\n",
    "                print(f'\\t{polymorf}')\n",
    "\n",
    "                # Extract parameters from POSCAR\n",
    "                cell, composition, concentration, positions = MPL.information_from_VASPfile(path_to_POSCAR,\n",
    "                                                                                            'POSCAR')\n",
    "\n",
    "                # Generate POSCAR covering the box\n",
    "                try:\n",
    "                    nodes, edges, attributes = GML.graph_POSCAR_encoding(cell, composition, concentration, positions, L)\n",
    "                except:\n",
    "                    print(f'Error: {material} {polymorf} not loaded')\n",
    "                    continue\n",
    "\n",
    "                # Load ground state energy per atom\n",
    "                gs_energy = float(np.loadtxt(f'{path_to_POSCAR}/EPA'))\n",
    "\n",
    "                # Construct temporal graph structure\n",
    "                graph = Data(x=nodes,\n",
    "                             edge_index=edges,\n",
    "                             edge_attr=attributes,\n",
    "                             y=torch.tensor([[gs_energy]], dtype=torch.float)\n",
    "                            )\n",
    "\n",
    "                # Append to dataset and labels\n",
    "                dataset.append(graph)\n",
    "                labels.append(f'{material}-{polymorf}')\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c0cbf",
   "metadata": {},
   "source": [
    "# Generation of diffusing and denoising Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a554a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GM-library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449ee41",
   "metadata": {},
   "source": [
    "# Generation of Graph Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77bb07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GM-library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e59715",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "859c2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1600\n",
      "Number of testing  graphs: 400\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1aa0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(4, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 4)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset\n",
    "n_features = dataset[0].num_node_features\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = GML.nGCNN(n_features, dropout_node).to(device)\n",
    "edge_model = GML.eGCNN(n_features, dropout_edge).to(device)\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8f049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize loss criterions\n",
    "node_criterion = nn.MSELoss()\n",
    "edge_criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    train_loss = 0\n",
    "    for graph in train_dataset:\n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Diffuse the graph with some noise\n",
    "        #print()\n",
    "        #print('Diffusing...')\n",
    "        diffused_graph = GML.diffuse(graph, n_diffusing_steps)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        denoised_graph = diffused_graph.clone()\n",
    "        #print(f'Denoising...')\n",
    "        for t in range(n_denoising_steps):\n",
    "            #print(f'\\t {t}')\n",
    "            # Perform a single forward pass for predicting node features\n",
    "            out_x = node_model(diffused_graph.x, \n",
    "                               diffused_graph.edge_index,\n",
    "                               diffused_graph.edge_attr)\n",
    "            \n",
    "            # Define x_i and x_j as features of every corresponding pair of nodes (same order than attributes)\n",
    "            x_i = diffused_graph.x[diffused_graph.edge_index[0]]\n",
    "            x_j = diffused_graph.x[diffused_graph.edge_index[1]]\n",
    "            \n",
    "            # Perform a single forward pass for predicting edge attributes\n",
    "            out_attr = edge_model(x_i, x_j)\n",
    "\n",
    "            # Construct noise graph\n",
    "            noise_graph = Data(x=out_x, edge_index=diffused_graph.edge_index, edge_attr=out_attr.ravel())\n",
    "            \n",
    "            # Denoise the graph with the predicted noise\n",
    "            denoised_graph = GML.denoising_step(denoised_graph, noise_graph, t, n_denoising_steps)\n",
    "        \n",
    "        #print('Backpropagating...')\n",
    "        # Backpropagation and optimization step\n",
    "        \n",
    "        # Calculate the loss for node features\n",
    "        node_loss = node_criterion(graph.x,\n",
    "                                   denoised_graph.x)\n",
    "        node_loss.backward()\n",
    "        node_optimizer.step()\n",
    "\n",
    "        # Calculate the loss for edge attributes\n",
    "        edge_loss = edge_criterion(graph.edge_attr,\n",
    "                                   denoised_graph.edge_attr)\n",
    "        edge_loss.backward()\n",
    "        edge_optimizer.step()\n",
    "                \n",
    "        # Accumulate the total training loss\n",
    "        loss = node_loss + edge_loss\n",
    "        train_loss = loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
