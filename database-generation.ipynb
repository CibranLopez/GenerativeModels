{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T00:27:23.463467Z",
     "start_time": "2024-02-09T00:27:06.631418Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from libraries.dataset        import standardize_dataset\n",
    "from libraries.graph          import graph_POSCAR_encoding\n",
    "from libraries.structure      import compute_diffraction_pattern\n",
    "from torch.utils.data         import random_split\n",
    "from torch_geometric.data     import Data\n",
    "from pymatgen.core            import Structure\n",
    "from pymatgen.io.vasp.outputs import Vasprun\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../UCL/m3gnet')\n",
    "import ML_library as MLL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a85832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T00:27:23.803826Z",
     "start_time": "2024-02-09T00:27:23.607468Z"
    }
   },
   "outputs": [],
   "source": [
    "check_labels = False  # Whether to train-test split attending to labels or not\n",
    "\n",
    "test_ratio  = 0.0\n",
    "\n",
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "DB_path = '../MP/Loaded_PT'\n",
    "\n",
    "# Define folder in which all data will be stored\n",
    "data_folder = 'data/GM_BiSI_diffraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standardized if possible."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sr2LiCBr3N2\n",
      "\tFd-3m\n",
      "KBaIn2(PO4)3\n",
      "\tP1\n",
      "TiCu2\n",
      "\tCmcm\n",
      "K2PrCl5\n",
      "\tPnma\n",
      "Dy4CrGa12\n",
      "\tIm-3m\n",
      "Nd6InFe13\n",
      "\tI4-mcm\n",
      "Cd3Pb\n",
      "\tP6_3-mmc\n",
      "Dy2MgV2O7\n",
      "\tCm\n",
      "\tC2\n",
      "SrEu(WO3)4\n",
      "\tAmm2\n",
      "LiUNbO6\n",
      "\tP2_1-c\n",
      "Sn4GeTe4Se\n",
      "\tR3m\n",
      "Gd3Y\n",
      "\tPm-3m\n",
      "NaCo(PO3)3\n",
      "\tI-43d\n",
      "NiH20Se2(NO7)2\n",
      "\tP2_1-c\n",
      "B11H9C(Br3O2)2\n",
      "\tP2_1-c\n",
      "Mg2Cu3Si\n",
      "\tP6_3-mmc\n",
      "TiH20C8(NF3)2\n",
      "\tP1\n",
      "EuZrFeBiO6\n",
      "\tP222\n",
      "\tF-43m\n"
     ]
    }
   ],
   "source": [
    "# Generate the raw dataset from scratch, and standardize it\n",
    "\n",
    "# Read all materials within the database\n",
    "materials = os.listdir(DB_path)\n",
    "\n",
    "dataset = []\n",
    "labels  = []\n",
    "for material in materials[:2000]:\n",
    "    try:\n",
    "        # Try to read the polymorphs\n",
    "        polymorphs = os.listdir(f'{DB_path}/{material}')\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    print(material)\n",
    "    for polymorf in polymorphs:\n",
    "        # Path to folder containing the POSCAR\n",
    "        path_to_POSCAR = f'{DB_path}/{material}/{polymorf}/POSCAR'\n",
    "        \n",
    "        # Check that the folder is valid\n",
    "        if os.path.exists(path_to_POSCAR):\n",
    "            print(f'\\t{polymorf}')\n",
    "            \n",
    "            try:\n",
    "                # Load pymatgen structure object\n",
    "                structure = Structure.from_file(path_to_POSCAR)\n",
    "                \n",
    "                nodes, edges, attributes = graph_POSCAR_encoding(structure,\n",
    "                                                                 encoding_type='sphere-images')\n",
    "            except:\n",
    "                print(f'Error: {material} {polymorf} not loaded')\n",
    "                continue\n",
    "\n",
    "            # Load ground state energy per atom\n",
    "            #gs_energy = float(np.loadtxt(f'{path_to_POSCAR}/EPA'))\n",
    "\n",
    "            # Compute diffraction pattern from given structure\n",
    "            diffraction_pattern = compute_diffraction_pattern(structure, diffraction='neutron')\n",
    "            \n",
    "            # Construct temporal graph structure\n",
    "            graph = Data(x=nodes,\n",
    "                         edge_index=edges.t().contiguous(),\n",
    "                         edge_attr=attributes.ravel(),\n",
    "                         y=torch.tensor(diffraction_pattern, dtype=torch.float)\n",
    "                        )\n",
    "\n",
    "            # Append to dataset and labels\n",
    "            dataset.append(graph)\n",
    "            labels.append(f'{material}-{polymorf}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-09T00:34:07.131647Z"
    }
   },
   "id": "965db1373732057c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataset = []\n",
    "labels  = []\n",
    "\n",
    "### Loaded database ###\n",
    "\n",
    "data_path = 'Loaded_BiSI/gamma'\n",
    "\n",
    "# Iterate over materials and relaxations in the dataset\n",
    "while len(dataset) < 200:\n",
    "    for material in os.listdir(data_path):\n",
    "        # Define path to material\n",
    "        path_to_material = f'{data_path}/{material}'\n",
    "\n",
    "        # Check if it is a folder\n",
    "        if not os.path.isdir(path_to_material):\n",
    "            continue\n",
    "\n",
    "        print()\n",
    "        print(material)\n",
    "\n",
    "        # Get relaxations steps (rel1, rel2...)\n",
    "        relaxation_steps = os.listdir(path_to_material)\n",
    "\n",
    "        # Determine all defect states across every folder\n",
    "        defect_states = []\n",
    "        for relaxation_step in relaxation_steps:\n",
    "            path_to_relaxation_step = f'{path_to_material}/{relaxation_step}'\n",
    "            if os.path.isdir(path_to_relaxation_step):\n",
    "                for defect_state in os.listdir(path_to_relaxation_step):\n",
    "                    if os.path.isdir(f'{path_to_material}/{relaxation_step}/{defect_state}'):\n",
    "                        defect_states.append(defect_state)\n",
    "\n",
    "        # Determine unique defect states across every folder\n",
    "        unique_defect_states = np.unique(defect_states)\n",
    "\n",
    "        # Run over all defect states\n",
    "        for defect_state in unique_defect_states:\n",
    "            print(f'\\t{defect_state}')\n",
    "\n",
    "            # Run over all relaxation steps\n",
    "            for relaxation_step in relaxation_steps:\n",
    "                # Define path to relaxation loading every relaxation step\n",
    "                # of a same defect state in the same data column\n",
    "                path_to_deformation = f'{path_to_material}/{relaxation_step}/{defect_state}'\n",
    "\n",
    "                # Avoiding non-directories (such as .DS_Store)\n",
    "                if not os.path.isdir(path_to_deformation):\n",
    "                    continue\n",
    "\n",
    "                # Define name for the defect state folder\n",
    "                temp_relaxation = f'{material}_{defect_state}'\n",
    "\n",
    "                # Check if it is a valid relaxation (with a vasprun.xml file)\n",
    "                # If not, it might be that there are different deformation folders of the defect state\n",
    "                if MLL.is_relaxation_folder_valid(path_to_deformation):\n",
    "                    path_to_relaxations = [path_to_deformation]\n",
    "                else:\n",
    "                    # Try to extract deformation folders\n",
    "                    deformation_folders = os.listdir(path_to_deformation)\n",
    "\n",
    "                    # Run over deformations\n",
    "                    path_to_relaxations = []\n",
    "                    for deformation_folder in deformation_folders:\n",
    "                        path_to_relaxation = f'{path_to_deformation}/{deformation_folder}'\n",
    "                        if MLL.is_relaxation_folder_valid(path_to_relaxation):\n",
    "                            path_to_relaxations.append(path_to_relaxation)\n",
    "\n",
    "                # Gather relaxations from different deformations as different ionic steps\n",
    "                for path_to_relaxation in path_to_relaxations:\n",
    "                    # Remove invalid characters from the vasprun.xml file\n",
    "                    MLL.clean_vasprun(path_to_relaxation)  # Uncomment is it happens to you as well!!\n",
    "\n",
    "                    if not os.path.exists(f'{path_to_relaxation}/vasprun.xml'):\n",
    "                        print(f'Check {path_to_relaxation}')\n",
    "\n",
    "                    # Load data from relaxation\n",
    "                    try:\n",
    "                        # Try to load those unfinished relaxations as well\n",
    "                        vasprun = Vasprun(f'{path_to_relaxation}/vasprun.xml', exception_on_bad_xml=False)\n",
    "                    except:\n",
    "                        print('Error: vasprun not correctly loaded.')\n",
    "                        continue\n",
    "\n",
    "                    # Run over ionic steps\n",
    "                    for ionic_step_idx in range(len(vasprun.ionic_steps)):\n",
    "                        temp_ionic_step = f'{temp_relaxation}_{ionic_step_idx}'\n",
    "                        # Extract data from each ionic step\n",
    "                        temp_structure = vasprun.ionic_steps[ionic_step_idx]['structure']\n",
    "                        temp_energy    = vasprun.ionic_steps[ionic_step_idx]['e_fr_energy']\n",
    "                        \n",
    "                        nodes, edges, attributes = graph_POSCAR_encoding(temp_structure,\n",
    "                                                                         encoding_type='sphere-images')\n",
    "\n",
    "                        # Construct temporal graph structure\n",
    "                        graph = Data(x=nodes,\n",
    "                                     edge_index=edges.t().contiguous(),\n",
    "                                     edge_attr=attributes.ravel(),\n",
    "                                     y=torch.tensor([temp_energy], dtype=torch.float)\n",
    "                                    )\n",
    "\n",
    "                        # Append to dataset and labels\n",
    "                        dataset.append(graph)\n",
    "                        labels.append(f'')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76426c86b09cef26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dc386",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "dataset_std, dataset_parameters = standardize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e59715",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96173a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T15:46:22.279182Z",
     "start_time": "2024-02-08T15:46:21.721632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 3196\n",
      "Number of testing  graphs: 0\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "test_size  = int(test_ratio * len(dataset_std))\n",
    "train_size = len(dataset_std) - test_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset_std, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64167d60-b767-42cd-881f-dba4f0a102cb",
   "metadata": {},
   "source": [
    "# Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7445ef5c-dbba-4913-91ae-aa299addd2c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T15:46:31.571276Z",
     "start_time": "2024-02-08T15:46:21.722543Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_name                 = f'{data_folder}/labels.pt'\n",
    "train_dataset_name_std      = f'{data_folder}/train_dataset.pt'\n",
    "test_dataset_name_std       = f'{data_folder}/test_dataset.pt'\n",
    "dataset_parameters_name_std = f'{data_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "torch.save(test_dataset,  labels_name)\n",
    "torch.save(train_dataset, train_dataset_name_std)\n",
    "torch.save(test_dataset,  test_dataset_name_std)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "    json.dump(numpy_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
