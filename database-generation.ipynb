{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T12:31:23.905925428Z",
     "start_time": "2024-03-06T12:31:22.717022826Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from libraries.dataset        import standardize_dataset, check_extend_POSCAR\n",
    "from libraries.graph          import graph_POSCAR_encoding\n",
    "from libraries.structure      import compute_diffraction_pattern\n",
    "from torch_geometric.data     import Data\n",
    "from pymatgen.core            import Structure\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a85832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T12:31:23.942167554Z",
     "start_time": "2024-03-06T12:31:23.907308969Z"
    }
   },
   "outputs": [],
   "source": [
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "data_path = '../MP/Loaded_PT'\n",
    "\n",
    "# Define diffraction type ('neutron', 'xrd' or 'EPA')\n",
    "target = 'EPA'\n",
    "\n",
    "# Define folder in which all data will be stored\n",
    "data_folder = f'data/GM_PT_{target}-voronoi'\n",
    "\n",
    "# Define name for storing dataset basic description\n",
    "dataset_parameters_name = f'{data_folder}/dataset_parameters.json'\n",
    "\n",
    "encoding_type      = 'voronoi'  # 'voronoi' or 'sphere-images'\n",
    "distance_threshold = 6  # Used in general\n",
    "\n",
    "minimum_lattice_vector = 3 * distance_threshold  # Allowing three convolutions\n",
    "\n",
    "# Define basic dataset parameters for tracking data\n",
    "dataset_parameters = {\n",
    "    'input_folder': data_path,\n",
    "    'output_folder': data_folder,\n",
    "    'target': target,\n",
    "    'encoding_type': encoding_type,\n",
    "    'distance_threshold': distance_threshold,\n",
    "    'minimum_lattice_vector': minimum_lattice_vector\n",
    "}\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.system(f'mkdir {data_folder}')\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "with open(dataset_parameters_name, 'w') as json_file:\n",
    "    json.dump(dataset_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standardized if possible."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CeSnRh\n",
      "\tP-62m\n",
      "LuSi\n",
      "\tCmcm\n",
      "\tP-6m2\n",
      "Gd2Ti2O7\n",
      "\tFd-3m\n",
      "\tC2-m\n",
      "LiAs3H2O9\n",
      "\tP2_1-c\n"
     ]
    }
   ],
   "source": [
    "# Generate the raw dataset from scratch, and standardize it\n",
    "\n",
    "# Read all materials within the database\n",
    "materials = os.listdir(data_path)\n",
    "\n",
    "dataset = []\n",
    "labels  = []\n",
    "for material in materials[:100]:\n",
    "    try:\n",
    "        # Try to read the polymorphs\n",
    "        polymorphs = os.listdir(f'{data_path}/{material}')\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    print(material)\n",
    "    for polymorf in polymorphs:\n",
    "        # Path to folder containing the POSCAR\n",
    "        path_to_POSCAR = f'{data_path}/{material}/{polymorf}'\n",
    "        \n",
    "        # Check that the folder is valid\n",
    "        if os.path.exists(path_to_POSCAR):\n",
    "            print(f'\\t{polymorf}')\n",
    "            \n",
    "            try:\n",
    "                # Load pymatgen structure object\n",
    "                structure = Structure.from_file(f'{path_to_POSCAR}/POSCAR')\n",
    "                \n",
    "                # Check that POSCAR is big enough, otherwise extend it where necessary\n",
    "                structure = check_extend_POSCAR(structure, minimum_lattice_vector)\n",
    "                \n",
    "                nodes, edges, attributes = graph_POSCAR_encoding(structure,\n",
    "                                                                 encoding_type=encoding_type,\n",
    "                                                                 distance_threshold=distance_threshold)\n",
    "            except:\n",
    "                print(f'Error: {material} {polymorf} not loaded')\n",
    "                continue\n",
    "            \n",
    "            if target == 'EPA':\n",
    "                # Load ground state energy per atom\n",
    "                extracted_target = [float(np.loadtxt(f'{path_to_POSCAR}/EPA'))]\n",
    "            else:\n",
    "                # Compute diffraction pattern from given structure\n",
    "                extracted_target = compute_diffraction_pattern(structure, diffraction=target)\n",
    "            \n",
    "            # Construct temporal graph structure\n",
    "            graph = Data(x=nodes,\n",
    "                         edge_index=edges.t().contiguous(),\n",
    "                         edge_attr=attributes.ravel(),\n",
    "                         y=torch.tensor(extracted_target, dtype=torch.float)\n",
    "                        )\n",
    "\n",
    "            # Append to dataset and labels\n",
    "            dataset.append(graph)\n",
    "            labels.append(f'{material}-{polymorf}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-06T12:31:23.943638349Z"
    }
   },
   "id": "965db1373732057c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dc386",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "dataset_std, dataset_parameters = standardize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64167d60-b767-42cd-881f-dba4f0a102cb",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels_name                 = f'{data_folder}/labels.pt'\n",
    "dataset_name                = f'{data_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{data_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{data_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "torch.save(labels,      labels_name)\n",
    "torch.save(dataset,     dataset_name)\n",
    "torch.save(dataset_std, dataset_name_std)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "    json.dump(numpy_dict, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e4233942ef25f557",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
