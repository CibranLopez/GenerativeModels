{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import GM_library  as GML\n",
    "import numpy       as np\n",
    "import torch\n",
    "\n",
    "from os                   import path, listdir\n",
    "from torch.utils.data     import random_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import MP.MP_library as MPL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target value to look for\n",
    "#seeked_target = ##\n",
    "\n",
    "# Machine-learning parameters\n",
    "n_epochs      = 1000\n",
    "batch_size    = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Number of diffusing and denoising steps, which can be different\n",
    "n_diffusing_steps = 20\n",
    "n_denoising_steps = 20\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.15\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Define box shape\n",
    "L = [20, 20, 20]\n",
    "\n",
    "# Target to generate new crystals\n",
    "target = 'GM_EPA'\n",
    "\n",
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "DB_path = '../MP/Loaded_EMP'\n",
    "\n",
    "input_folder    = 'models'\n",
    "target_folder   = f'{input_folder}/{target}'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38439e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset, with corresponding labels and parameters\n",
    "dataset    = torch.load(dataset_name_std)\n",
    "parameters = torch.load(parameters_name_std)\n",
    "\n",
    "# Assigning parameters accordingly\n",
    "target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = target_std / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dcaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "if path.exists(dataset_name_std) and path.exists(parameters_name_std):  # and path.exists(labels_name)\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    #labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "\n",
    "elif path.exists(dataset_name) and path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)\n",
    "\n",
    "else:\n",
    "    # Generate the raw dataset from scratch, and standardize it\n",
    "    \n",
    "    # Read all mateials within the database\n",
    "    materials = listdir(DB_path)\n",
    "    \n",
    "    dataset = []\n",
    "    labels  = []\n",
    "    for material in materials:\n",
    "        try:\n",
    "            # Try to read the polyforms\n",
    "            polymorfs = listdir(f'{DB_path}/{material}')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        print(material)\n",
    "        for polymorf in polymorfs:\n",
    "            # Path to folder containing the POSCAR\n",
    "            path_to_POSCAR = f'{DB_path}/{material}/{polymorf}'\n",
    "            \n",
    "            # Check that the folder is valid\n",
    "            if path.exists(f'{path_to_POSCAR}/POSCAR'):\n",
    "                print(f'\\t{polymorf}')\n",
    "\n",
    "                # Extract parameters from POSCAR\n",
    "                cell, composition, concentration, positions = MPL.information_from_VASPfile(path_to_POSCAR,\n",
    "                                                                                            'POSCAR')\n",
    "\n",
    "                # Generate POSCAR covering the box\n",
    "                try:\n",
    "                    nodes, edges, attributes = GML.graph_POSCAR_encoding(cell, composition, concentration, positions, L)\n",
    "                except:\n",
    "                    print(f'Error: {material} {polymorf} not loaded')\n",
    "                    continue\n",
    "\n",
    "                # Load ground state energy per atom\n",
    "                gs_energy = float(np.loadtxt(f'{path_to_POSCAR}/EPA'))\n",
    "\n",
    "                # Construct temporal graph structure\n",
    "                graph = Data(x=nodes,\n",
    "                             edge_index=edges,\n",
    "                             edge_attr=attributes,\n",
    "                             y=torch.tensor([[gs_energy]], dtype=torch.float)\n",
    "                            )\n",
    "\n",
    "                # Append to dataset and labels\n",
    "                dataset.append(graph)\n",
    "                labels.append(f'{material}-{polymorf}')\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)\n",
    "    torch.save(labels,     labels_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e59715",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859c2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1600\n",
      "Number of testing  graphs: 400\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1aa0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(4, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 4)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset\n",
    "n_features = dataset[0].num_node_features\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = GML.nGCNN(n_features, dropout_node).to(device)\n",
    "edge_model = GML.eGCNN(n_features, dropout_edge).to(device)\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110c0072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GM_library' from '/Users/cibran/Work/UPC/GenerativeModels/GM_library.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    train_loss = 0\n",
    "    for graph in train_dataset:\n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Diffuse the graph with some noise\n",
    "        #print()\n",
    "        #print('Diffusing...')\n",
    "        diffused_graph, _ = GML.diffuse(graph, n_diffusing_steps,\n",
    "                                        s=alpha_decay)\n",
    "        \n",
    "        # Denoise the diffused graph\n",
    "        #print(f'Denoising...')\n",
    "        denoised_graph, _ = GML.denoise(diffused_graph, n_denoising_steps, node_model, edge_model,\n",
    "                                        s=alpha_decay)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        #print('Backpropagating...')\n",
    "        \n",
    "        # Calculate the loss for node features and edge attributes\n",
    "        node_loss, edge_loss = GML.get_graph_losses(graph, denoised_graph)\n",
    "        \n",
    "        # Backpropagate node and edge losses (retained graph, as it is used two times)\n",
    "        node_loss.backward(retain_graph=True)\n",
    "        edge_loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Perform a single step for each optimized\n",
    "        node_optimizer.step()\n",
    "        edge_optimizer.step()\n",
    "        \n",
    "        # Predict target for current graph\n",
    "        #predicted_target = ###\n",
    "        \n",
    "        # Compute target loss\n",
    "        #target_loss = GML.get_target_loss(predicted_target, seeked_target)\n",
    "        target_loss = 0\n",
    "                \n",
    "        # Accumulate the total training loss\n",
    "        loss = node_loss + edge_loss + target_loss\n",
    "        train_loss = loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(edge_model.state_dict(), edge_model_name)\n",
    "torch.save(node_model.state_dict(), node_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(train_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
