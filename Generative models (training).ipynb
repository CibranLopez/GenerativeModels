{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import numpy       as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from libraries.model      import nGCNN, eGCNN, diffusion_step, add_features_to_graph, get_graph_losses\n",
    "from libraries.dataset    import standardize_dataset\n",
    "from libraries.graph      import graph_POSCAR_encoding\n",
    "from torch.utils.data     import random_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686ad446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to materials\n",
    "# the models is able to learn hidded patterns\n",
    "# It can be trained regarding some target property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a85832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "DB_path = '../MP/Loaded_PT'\n",
    "\n",
    "# Define folder in which all data will be stored\n",
    "target_folder    = 'models/GM_BiSI'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target value to look for\n",
    "#seeked_target = ##\n",
    "\n",
    "# Machine-learning parameters\n",
    "n_epochs      = 3000\n",
    "batch_size    = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Ratios for dividing training data\n",
    "test_ratio = 0.0\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 100\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'test_ratio':         test_ratio,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standardized if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c70e2b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_dataset_parameters.json'\n",
    "\n",
    "if os.path.exists(dataset_name_std) and os.path.exists(dataset_parameters_name_std) and os.path.exists(labels_name):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset = torch.load(dataset_name_std)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Load the data from the JSON file\n",
    "    with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "        numpy_dict = json.load(json_file)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    dataset_parameters = {key: torch.tensor(value) for key, value in numpy_dict.items()}\n",
    "\n",
    "elif os.path.exists(dataset_name) and os.path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)\n",
    "\n",
    "else:\n",
    "    # Generate the raw dataset from scratch, and standardize it\n",
    "    \n",
    "    # Read all mateials within the database\n",
    "    materials = os.listdir(DB_path)\n",
    "    \n",
    "    dataset = []\n",
    "    labels  = []\n",
    "    for material in materials:\n",
    "        try:\n",
    "            # Try to read the polyforms\n",
    "            polymorfs = os.listdir(f'{DB_path}/{material}')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        print(material)\n",
    "        for polymorf in polymorfs:\n",
    "            # Path to folder containing the POSCAR\n",
    "            path_to_POSCAR = f'{DB_path}/{material}/{polymorf}'\n",
    "            \n",
    "            # Check that the folder is valid\n",
    "            if os.path.exists(f'{path_to_POSCAR}/POSCAR'):\n",
    "                print(f'\\t{polymorf}')\n",
    "\n",
    "                # Extract parameters from POSCAR\n",
    "                cell, composition, concentration, positions = MPL.information_from_VASPfile(path_to_POSCAR,\n",
    "                                                                                            'POSCAR')\n",
    "\n",
    "                # Generate POSCAR covering the box\n",
    "                try:\n",
    "                    nodes, edges, attributes, _, _, _ = graph_POSCAR_encoding(cell,\n",
    "                                                                                  composition,\n",
    "                                                                                  concentration,\n",
    "                                                                                  positions,\n",
    "                                                                                  L)\n",
    "                except:\n",
    "                    print(f'Error: {material} {polymorf} not loaded')\n",
    "                    continue\n",
    "\n",
    "                # Load ground state energy per atom\n",
    "                gs_energy = float(np.loadtxt(f'{path_to_POSCAR}/EPA'))\n",
    "\n",
    "                # Construct temporal graph structure\n",
    "                graph = Data(x=nodes,\n",
    "                             edge_index=edges,\n",
    "                             edge_attr=attributes,\n",
    "                             y=torch.tensor([[gs_energy]], dtype=torch.float)\n",
    "                            )\n",
    "\n",
    "                # Append to dataset and labels\n",
    "                dataset.append(graph)\n",
    "                labels.append(f'{material}-{polymorf}')\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    torch.save(labels,  labels_name)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1439db4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BiSBr\n",
      "\n",
      "BiSeBr\n",
      "\n",
      "BiSeI\n",
      "\n",
      "BiSI\n",
      "\tas_1_Bi_on_S_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_0\n",
      "\tas_1_Bi_on_S_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_3\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_4\n",
      "\tas_1_Bi_on_S_5\n",
      "\tas_1_I_on_Bi_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_0\n",
      "\tas_1_I_on_Bi_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_2\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_3\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_4\n",
      "\tas_1_I_on_Bi_5\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_0\n",
      "\tas_1_S_on_Bi_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_2\n",
      "Error: vasprun not correctly loaded.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m\n\u001b[1;32m    100\u001b[0m temp_energy    \u001b[38;5;241m=\u001b[39m vasprun\u001b[38;5;241m.\u001b[39mionic_steps[ionic_step_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_fr_energy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Save temp_structure to POSCAR\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#temp_Poscar = Poscar(temp_structure)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m#temp_Poscar.write_file('POSCAR')\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m nodes, edges, attributes \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_POSCAR_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_structure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Construct temporal graph structure\u001b[39;00m\n\u001b[1;32m    109\u001b[0m graph \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    110\u001b[0m              edge_index\u001b[38;5;241m=\u001b[39medges,\n\u001b[1;32m    111\u001b[0m              edge_attr\u001b[38;5;241m=\u001b[39mattributes,\n\u001b[1;32m    112\u001b[0m              y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[temp_energy]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    113\u001b[0m             )\n",
      "File \u001b[0;32m~/Work/UPC/GenerativeModels/libraries/graph.py:368\u001b[0m, in \u001b[0;36mgraph_POSCAR_encoding\u001b[0;34m(structure, encoding_type)\u001b[0m\n\u001b[1;32m    364\u001b[0m         ionization_energies[key] \u001b[38;5;241m=\u001b[39m ionization_energy\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoronoi\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# Get edges and attributes for the corresponding tessellation\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     nodes, edges, attributes \u001b[38;5;241m=\u001b[39m \u001b[43mget_voronoi_tessellation\u001b[49m\u001b[43m(\u001b[49m\u001b[43matomic_masses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43melectronegativities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mionization_energies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m# Convert to torch tensors and return\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     nodes      \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(nodes,      dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[0;32m~/Work/UPC/GenerativeModels/libraries/graph.py:251\u001b[0m, in \u001b[0;36mget_voronoi_tessellation\u001b[0;34m(atomic_masses, charges, electronegativities, ionization_energies, structure)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Apply periodic bounday conditions\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(frac_coords_uc \u001b[38;5;241m>\u001b[39m  \u001b[38;5;241m1\u001b[39m): frac_coords_uc[np\u001b[38;5;241m.\u001b[39mwhere(frac_coords_uc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m)]  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrac_coords_uc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m: frac_coords_uc[np\u001b[38;5;241m.\u001b[39mwhere(frac_coords_uc \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Obtain mapping to index in unit cell\u001b[39;00m\n\u001b[1;32m    254\u001b[0m uc_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(structure\u001b[38;5;241m.\u001b[39mfrac_coords \u001b[38;5;241m-\u001b[39m frac_coords_uc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2317\u001b[0m, in \u001b[0;36m_any_dispatcher\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m   2313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2314\u001b[0m                           initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m-> 2317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2318\u001b[0m                     where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_any_dispatcher)\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21many\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_dataset_parameters.json'\n",
    "\n",
    "from pymatgen.io.vasp.inputs  import Poscar\n",
    "from pymatgen.io.vasp.outputs import Vasprun\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../UCL/m3gnet')\n",
    "import ML_library as MLL\n",
    "\n",
    "dataset = []\n",
    "labels  = []\n",
    "\n",
    "### Loaded database ###\n",
    "\n",
    "data_path = 'Loaded_BiSI/gamma'\n",
    "\n",
    "# Iterate over materials and relaxations in the dataset\n",
    "while len(dataset) < 200:\n",
    "    for material in os.listdir(data_path):\n",
    "        # Define path to material\n",
    "        path_to_material = f'{data_path}/{material}'\n",
    "\n",
    "        # Check if it is a folder\n",
    "        if not os.path.isdir(path_to_material):\n",
    "            continue\n",
    "\n",
    "        print()\n",
    "        print(material)\n",
    "\n",
    "        # Get relaxations steps (rel1, rel2...)\n",
    "        relaxation_steps = os.listdir(path_to_material)\n",
    "\n",
    "        # Determine all defect states across every folder\n",
    "        defect_states = []\n",
    "        for relaxation_step in relaxation_steps:\n",
    "            path_to_relaxation_step = f'{path_to_material}/{relaxation_step}'\n",
    "            if os.path.isdir(path_to_relaxation_step):\n",
    "                for defect_state in os.listdir(path_to_relaxation_step):\n",
    "                    if os.path.isdir(f'{path_to_material}/{relaxation_step}/{defect_state}'):\n",
    "                        defect_states.append(defect_state)\n",
    "\n",
    "        # Determine unique defect states across every folder\n",
    "        unique_defect_states = np.unique(defect_states)\n",
    "\n",
    "        # Run over all defect states\n",
    "        for defect_state in unique_defect_states:\n",
    "            print(f'\\t{defect_state}')\n",
    "\n",
    "            # Run over all relaxation steps\n",
    "            for relaxation_step in relaxation_steps:\n",
    "                # Define path to relaxation loading every relaxation step of a same defect state in the same data column\n",
    "                path_to_deformation = f'{path_to_material}/{relaxation_step}/{defect_state}'\n",
    "\n",
    "                # Avoiding non-directories (such as .DS_Store)\n",
    "                if not os.path.isdir(path_to_deformation):\n",
    "                    continue\n",
    "\n",
    "                # Define name for the defect state folder\n",
    "                temp_relaxation = f'{material}_{defect_state}'\n",
    "\n",
    "                # Check if it is a valid relaxation (with a vasprun.xml file)\n",
    "                # If not, it might be that there are different deformation folders of the defect state\n",
    "                if MLL.is_relaxation_folder_valid(path_to_deformation):\n",
    "                    path_to_relaxations = [path_to_deformation]\n",
    "                else:\n",
    "                    # Try to extact deformation folders\n",
    "                    deformation_folders = os.listdir(path_to_deformation)\n",
    "\n",
    "                    # Run over deformations\n",
    "                    path_to_relaxations = []\n",
    "                    for deformation_folder in deformation_folders:\n",
    "                        path_to_relaxation = f'{path_to_deformation}/{deformation_folder}'\n",
    "                        if MLL.is_relaxation_folder_valid(path_to_relaxation):\n",
    "                            path_to_relaxations.append(path_to_relaxation)\n",
    "\n",
    "                # Gather relaxations from different deformations as different ionic steps\n",
    "                for path_to_relaxation in path_to_relaxations:\n",
    "                    # Remove invalid characters from the vasprun.xml file\n",
    "                    MLL.clean_vasprun(path_to_relaxation)  # Uncomment is it happens to you as well!!\n",
    "\n",
    "                    if not os.path.exists(f'{path_to_relaxation}/vasprun.xml'):\n",
    "                        print(f'Check {path_to_relaxation}')\n",
    "\n",
    "                    # Load data from relaxation\n",
    "                    try:\n",
    "                        # Try to load those unfinished relaxations as well\n",
    "                        vasprun = Vasprun(f'{path_to_relaxation}/vasprun.xml', exception_on_bad_xml=False)\n",
    "                    except:\n",
    "                        print('Error: vasprun not correctly loaded.')\n",
    "                        continue\n",
    "\n",
    "                    # Run over ionic steps\n",
    "                    for ionic_step_idx in range(len(vasprun.ionic_steps)):\n",
    "                        temp_ionic_step = f'{temp_relaxation}_{ionic_step_idx}'\n",
    "                        # Extract data from each ionic step\n",
    "                        temp_structure = vasprun.ionic_steps[ionic_step_idx]['structure']\n",
    "                        temp_energy    = vasprun.ionic_steps[ionic_step_idx]['e_fr_energy']\n",
    "\n",
    "                        # Save temp_structure to POSCAR\n",
    "                        #temp_Poscar = Poscar(temp_structure)\n",
    "                        #temp_Poscar.write_file('POSCAR')\n",
    "                        \n",
    "                        nodes, edges, attributes = graph_POSCAR_encoding(temp_structure)\n",
    "\n",
    "                        # Construct temporal graph structure\n",
    "                        graph = Data(x=nodes,\n",
    "                                     edge_index=edges,\n",
    "                                     edge_attr=attributes,\n",
    "                                     y=torch.tensor([[temp_energy]], dtype=torch.float)\n",
    "                                    )\n",
    "\n",
    "                        # Append to dataset and labels\n",
    "                        dataset.append(graph)\n",
    "                        labels.append(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94dd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "\n",
    "# Save standardized dataset\n",
    "torch.save(dataset, dataset_name_std)\n",
    "torch.save(labels,  labels_name)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "    json.dump(numpy_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e59715",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96173a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 28569\n",
      "Number of testing  graphs: 0\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "test_size  = int(test_ratio * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f86aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(5, 256)\n",
      "  (conv2): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset, considering the t_step information\n",
    "n_features = dataset[0].num_node_features + 1\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_features, dropout_edge).to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Evaluate model state\n",
    "node_model.eval()\n",
    "edge_model.eval()\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, total loss: 5162.7179, edge loss: 1.7447, node loss: 5160.9732\n",
      "Epoch: 2, total loss: 40.5703, edge loss: 1.0000, node loss: 39.5702\n",
      "Epoch: 3, total loss: 15.8177, edge loss: 1.0000, node loss: 14.8177\n",
      "Epoch: 4, total loss: 4.7041, edge loss: 1.0000, node loss: 3.7041\n"
     ]
    }
   ],
   "source": [
    "# Loss factor for normalization\n",
    "loss_factor = len(train_dataset) * n_t_steps\n",
    "\n",
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_losses = []\n",
    "edge_losses  = []\n",
    "node_losses  = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    total_loss_cum = 0\n",
    "    edge_loss_cum  = 0\n",
    "    node_loss_cum  = 0\n",
    "    for graph in train_dataset:\n",
    "        #print()\n",
    "        # Clone existing graph\n",
    "        graph_0 = graph.clone()\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        for t_step in np.arange(1, n_t_steps+1):\n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            graph_t, epsilon_t = diffusion_step(graph_0, t_step, n_t_steps, alpha_decay)\n",
    "            \n",
    "            # Update diffused graph as next one\n",
    "            graph_0 = graph_t.clone()\n",
    "\n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add t_step information to graph_t\n",
    "            t_step_std = (t_step/n_t_steps - 0.5)  # Standard normalization\n",
    "            graph_t = add_features_to_graph(graph_t,\n",
    "                                            torch.tensor([[t_step_std]], dtype=torch.float))  # To match graph.y shape\n",
    "            \n",
    "            # Add target information\n",
    "            #graph_t = add_features_to_graph(graph_t,\n",
    "            #                                    graph_t.y)\n",
    "\n",
    "            # Perform a single forward pass for predicting node features\n",
    "            out_x = node_model(graph_t.x,\n",
    "                               graph_t.edge_index,\n",
    "                               graph_t.edge_attr)\n",
    "            \n",
    "            # Remove t_step information\n",
    "            out_x = out_x[:, :-1]\n",
    "\n",
    "            # Define x_i and x_j as features of every corresponding pair of nodes (same order than attributes)\n",
    "            x_i = graph_t.x[graph_t.edge_index[0]]\n",
    "            x_j = graph_t.x[graph_t.edge_index[1]]\n",
    "\n",
    "            # Perform a single forward pass for predicting edge attributes\n",
    "            # Introduce previous edge attributes as features as well\n",
    "            out_attr = edge_model(x_i, x_j, graph_t.edge_attr)\n",
    "\n",
    "            # Construct noise graph\n",
    "            pred_epsilon_t = Data(x=out_x,\n",
    "                                  edge_index=graph_t.edge_index,\n",
    "                                  edge_attr=out_attr.ravel())\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the loss for node features and edge attributes\n",
    "            node_loss, edge_loss = get_graph_losses(epsilon_t, pred_epsilon_t)\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            node_loss.backward(retain_graph=True)\n",
    "            node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            edge_loss.backward(retain_graph=True)\n",
    "            edge_optimizer.step()\n",
    "\n",
    "            # Accumulate the total training loss\n",
    "            loss = node_loss + edge_loss\n",
    "            \n",
    "            # Get items\n",
    "            total_loss_cum += loss.item()\n",
    "            edge_loss_cum  += edge_loss.item()\n",
    "            node_loss_cum  += node_loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    total_loss_cum = total_loss_cum / loss_factor\n",
    "    edge_loss_cum  = edge_loss_cum  / loss_factor\n",
    "    node_loss_cum  = node_loss_cum  / loss_factor\n",
    "    \n",
    "    # Append average losses\n",
    "    total_losses.append(total_loss_cum)\n",
    "    edge_losses.append(edge_loss_cum)\n",
    "    node_losses.append(node_loss_cum)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, total loss: {total_loss_cum:.4f}, edge loss: {edge_loss_cum:.4f}, node loss: {node_loss_cum:.4f}')\n",
    "    \n",
    "    # Save some checkpoints\n",
    "    if (epoch % 20) == 0:\n",
    "        torch.save(node_model.state_dict(), node_model_name)\n",
    "        torch.save(edge_model.state_dict(), edge_model_name)\n",
    "\n",
    "torch.save(node_model.state_dict(), node_model_name)\n",
    "torch.save(edge_model.state_dict(), edge_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeffa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(total_losses), label='Total')\n",
    "plt.plot(np.log(edge_losses),  label='Edge')\n",
    "plt.plot(np.log(node_losses),  label='Node')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
