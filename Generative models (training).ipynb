{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a69f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import GM_library  as GML\n",
    "import numpy       as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from os                   import path, listdir\n",
    "from torch.utils.data     import random_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import MP.MP_library as MPL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af510bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target to generate new crystals\n",
    "target = 'GM_BiSI'\n",
    "\n",
    "# In case database is created from scratch (otherwise, it is not being used)\n",
    "DB_path = '../MP/Loaded_PT'\n",
    "\n",
    "input_folder    = 'models'\n",
    "target_folder   = f'{input_folder}/{target}'\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target value to look for\n",
    "#seeked_target = ##\n",
    "\n",
    "# Machine-learning parameters\n",
    "n_epochs      = 300\n",
    "batch_size    = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Ratios for dividing training data\n",
    "test_ratio = 0.0\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 10\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.15\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'n_epochs': n_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'test_ratio': test_ratio,\n",
    "    'n_t_steps': n_t_steps,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node': dropout_node,\n",
    "    'dropout_edge': dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49ab8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[72, 4], edge_index=[2, 2556], edge_attr=[2556], y=[1, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f78ad964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8986, 0.6007, 0.0000,  ..., 0.3057, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35fb0c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid graph, atoms overlapping. Applying brute force :)\n",
      "14 41 58 6.8787885 14.762562 12.048041 8.72941957421743 11.905060467711378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='.//CONTCAR' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GML.POSCAR_graph_encoding(dataset[0], L, file_name=f'CONTCAR', POSCAR_directory='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Generation of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the datasets, already standarized if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5002428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BiSBr\n",
      "\n",
      "BiSeBr\n",
      "\n",
      "BiSeI\n",
      "\n",
      "BiSI\n",
      "\tas_1_Bi_on_S_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_0\n",
      "\tas_1_Bi_on_S_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_3\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_Bi_on_S_4\n",
      "\tas_1_Bi_on_S_5\n",
      "\tas_1_I_on_Bi_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_0\n",
      "\tas_1_I_on_Bi_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_2\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_3\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_I_on_Bi_4\n",
      "\tas_1_I_on_Bi_5\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_0\n",
      "\tas_1_S_on_Bi_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_2\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_3\n",
      "\tas_1_S_on_Bi_4\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_1_S_on_Bi_5\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_Bi_on_I_-1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_Bi_on_I_-2\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_Bi_on_I_0\n",
      "\tas_2_Bi_on_I_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_Bi_on_I_2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_Bi_on_I_3\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_Bi_on_I_4\n",
      "\tas_2_Bi_on_I_5\n",
      "\tas_2_I_on_S_-1\n",
      "\tas_2_I_on_S_-2\n",
      "\tas_2_I_on_S_0\n",
      "\tas_2_I_on_S_1\n",
      "\tas_2_I_on_S_2\n",
      "\tas_2_I_on_S_3\n",
      "\tas_2_I_on_S_4\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_I_on_S_5\n",
      "\tas_2_S_on_I_-1\n",
      "\tas_2_S_on_I_-2\n",
      "\tas_2_S_on_I_0\n",
      "\tas_2_S_on_I_1\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_S_on_I_2\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_S_on_I_3\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_S_on_I_4\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tas_2_S_on_I_5\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "Error: vasprun not correctly loaded.\n",
      "\tvac_1_Bi_-1\n",
      "\tvac_1_Bi_-2\n",
      "\tvac_1_Bi_-3\n",
      "\tvac_1_Bi_0\n",
      "\tvac_1_Bi_1\n",
      "\tvac_1_Bi_2\n",
      "\tvac_1_Bi_3\n",
      "\tvac_2_S_-1\n",
      "\tvac_2_S_-2\n",
      "\tvac_2_S_0\n",
      "\tvac_2_S_1\n",
      "\tvac_2_S_2\n",
      "\tvac_3_I_-1\n",
      "\tvac_3_I_0\n",
      "\tvac_3_I_1\n"
     ]
    }
   ],
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "from pymatgen.io.vasp.inputs  import Poscar\n",
    "from pymatgen.io.vasp.outputs import Vasprun\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../UCL/m3gnet')\n",
    "import ML_library as MLL\n",
    "\n",
    "dataset = []\n",
    "labels  = []\n",
    "\n",
    "### Loaded database ###\n",
    "\n",
    "data_path = 'Loaded_BiSI/gamma'\n",
    "\n",
    "# Iterate over materials and relaxations in the dataset\n",
    "while len(dataset) < 200:\n",
    "    for material in listdir(data_path):\n",
    "        # Define path to material\n",
    "        path_to_material = f'{data_path}/{material}'\n",
    "\n",
    "        # Check if it is a folder\n",
    "        if not path.isdir(path_to_material):\n",
    "            continue\n",
    "\n",
    "        print()\n",
    "        print(material)\n",
    "\n",
    "        # Get relaxations steps (rel1, rel2...)\n",
    "        relaxation_steps = listdir(path_to_material)\n",
    "\n",
    "        # Determine all defect states across every folder\n",
    "        defect_states = []\n",
    "        for relaxation_step in relaxation_steps:\n",
    "            path_to_relaxation_step = f'{path_to_material}/{relaxation_step}'\n",
    "            if path.isdir(path_to_relaxation_step):\n",
    "                for defect_state in listdir(path_to_relaxation_step):\n",
    "                    if path.isdir(f'{path_to_material}/{relaxation_step}/{defect_state}'):\n",
    "                        defect_states.append(defect_state)\n",
    "\n",
    "        # Determine unique defect states across every folder\n",
    "        unique_defect_states = np.unique(defect_states)\n",
    "\n",
    "        # Run over all defect states\n",
    "        for defect_state in unique_defect_states:\n",
    "            print(f'\\t{defect_state}')\n",
    "\n",
    "            # Run over all relaxation steps\n",
    "            for relaxation_step in relaxation_steps:\n",
    "                # Define path to relaxation loading every relaxation step of a same defect state in the same data column\n",
    "                path_to_deformation = f'{path_to_material}/{relaxation_step}/{defect_state}'\n",
    "\n",
    "                # Avoiding non-directories (such as .DS_Store)\n",
    "                if not path.isdir(path_to_deformation):\n",
    "                    continue\n",
    "\n",
    "                # Define name for the defect state folder\n",
    "                temp_relaxation = f'{material}_{defect_state}'\n",
    "\n",
    "                # Check if it is a valid relaxation (with a vasprun.xml file)\n",
    "                # If not, it might be that there are different deformation folders of the defect state\n",
    "                if MLL.is_relaxation_folder_valid(path_to_deformation):\n",
    "                    path_to_relaxations = [path_to_deformation]\n",
    "                else:\n",
    "                    # Try to extact deformation folders\n",
    "                    deformation_folders = listdir(path_to_deformation)\n",
    "\n",
    "                    # Run over deformations\n",
    "                    path_to_relaxations = []\n",
    "                    for deformation_folder in deformation_folders:\n",
    "                        path_to_relaxation = f'{path_to_deformation}/{deformation_folder}'\n",
    "                        if MLL.is_relaxation_folder_valid(path_to_relaxation):\n",
    "                            path_to_relaxations.append(path_to_relaxation)\n",
    "\n",
    "                # Gather relaxations from different deformations as different ionic steps\n",
    "                for path_to_relaxation in path_to_relaxations:\n",
    "                    # Remove invalid characters from the vasprun.xml file\n",
    "                    MLL.clean_vasprun(path_to_relaxation)  # Uncomment is it happens to you as well!!\n",
    "\n",
    "                    if not path.exists(f'{path_to_relaxation}/vasprun.xml'):\n",
    "                        print(f'Check {path_to_relaxation}')\n",
    "\n",
    "                    # Load data from relaxation\n",
    "                    try:\n",
    "                        # Try to load those unfinished relaxations as well\n",
    "                        vasprun = Vasprun(f'{path_to_relaxation}/vasprun.xml', exception_on_bad_xml=False)\n",
    "                    except:\n",
    "                        print('Error: vasprun not correctly loaded.')\n",
    "                        continue\n",
    "\n",
    "                    # Run over ionic steps\n",
    "                    for ionic_step_idx in range(len(vasprun.ionic_steps)):\n",
    "                        temp_ionic_step = f'{temp_relaxation}_{ionic_step_idx}'\n",
    "                        # Extract data from each ionic step\n",
    "                        temp_structure = vasprun.ionic_steps[ionic_step_idx]['structure']\n",
    "                        temp_energy    = vasprun.ionic_steps[ionic_step_idx]['e_fr_energy']\n",
    "\n",
    "                        # Save temp_structure to POSCAR\n",
    "                        temp_Poscar = Poscar(temp_structure)\n",
    "                        temp_Poscar.write_file('POSCAR')\n",
    "\n",
    "                        # Extract parameters from POSCAR\n",
    "                        cell, composition, concentration, positions = MPL.information_from_VASPfile('.',\n",
    "                                                                                                    'POSCAR')\n",
    "\n",
    "                        # Generate POSCAR covering the box\n",
    "                        try:\n",
    "                            nodes, edges, attributes, _, _, _ = GML.graph_POSCAR_encoding(cell,\n",
    "                                                                                          composition,\n",
    "                                                                                          concentration,\n",
    "                                                                                          positions,\n",
    "                                                                                          L)\n",
    "                        except:\n",
    "                            print(f'Error: ionic step {ionic_step_idx} not loaded')\n",
    "                            continue\n",
    "\n",
    "                        # Construct temporal graph structure\n",
    "                        graph = Data(x=nodes,\n",
    "                                     edge_index=edges,\n",
    "                                     edge_attr=attributes,\n",
    "                                     y=torch.tensor([[temp_energy]], dtype=torch.float)\n",
    "                                    )\n",
    "\n",
    "                        # Append to dataset and labels\n",
    "                        dataset.append(graph)\n",
    "                        labels.append(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5853919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "dataset, parameters = GML.standardize_dataset(dataset)\n",
    "\n",
    "# Save standardized dataset\n",
    "torch.save(dataset,    dataset_name_std)\n",
    "torch.save(parameters, parameters_name_std)\n",
    "torch.save(labels,     labels_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423b5e1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "labels_name         = f'{target_folder}/labels.pt'\n",
    "dataset_name        = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std    = f'{target_folder}/standardized_dataset.pt'\n",
    "parameters_name_std = f'{target_folder}/standardized_parameters.pt'  # Parameters for rescaling the predictions\n",
    "\n",
    "if path.exists(dataset_name_std) and path.exists(parameters_name_std) and path.exists(labels_name):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset    = torch.load(dataset_name_std)\n",
    "    labels     = torch.load(labels_name)\n",
    "    parameters = torch.load(parameters_name_std)\n",
    "\n",
    "    # Assigning parameters accordingly\n",
    "    target_mean, feat_mean, edge_mean, target_std, edge_std, feat_std, scale = parameters\n",
    "    \n",
    "    # Defining target factor\n",
    "    target_factor = target_std / scale\n",
    "\n",
    "elif path.exists(dataset_name) and path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)\n",
    "\n",
    "else:\n",
    "    # Generate the raw dataset from scratch, and standardize it\n",
    "    \n",
    "    # Read all mateials within the database\n",
    "    materials = listdir(DB_path)[:1000]\n",
    "    \n",
    "    dataset = []\n",
    "    labels  = []\n",
    "    for material in materials:\n",
    "        try:\n",
    "            # Try to read the polyforms\n",
    "            polymorfs = listdir(f'{DB_path}/{material}')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        print(material)\n",
    "        for polymorf in polymorfs:\n",
    "            # Path to folder containing the POSCAR\n",
    "            path_to_POSCAR = f'{DB_path}/{material}/{polymorf}'\n",
    "            \n",
    "            # Check that the folder is valid\n",
    "            if path.exists(f'{path_to_POSCAR}/POSCAR'):\n",
    "                print(f'\\t{polymorf}')\n",
    "\n",
    "                # Extract parameters from POSCAR\n",
    "                cell, composition, concentration, positions = MPL.information_from_VASPfile(path_to_POSCAR,\n",
    "                                                                                            'POSCAR')\n",
    "\n",
    "                # Generate POSCAR covering the box\n",
    "                try:\n",
    "                    nodes, edges, attributes, _, _, _ = GML.graph_POSCAR_encoding(cell,\n",
    "                                                                                  composition,\n",
    "                                                                                  concentration,\n",
    "                                                                                  positions,\n",
    "                                                                                  L)\n",
    "                except:\n",
    "                    print(f'Error: {material} {polymorf} not loaded')\n",
    "                    continue\n",
    "\n",
    "                # Load ground state energy per atom\n",
    "                gs_energy = float(np.loadtxt(f'{path_to_POSCAR}/EPA'))\n",
    "\n",
    "                # Construct temporal graph structure\n",
    "                graph = Data(x=nodes,\n",
    "                             edge_index=edges,\n",
    "                             edge_attr=attributes,\n",
    "                             y=torch.tensor([[gs_energy]], dtype=torch.float)\n",
    "                            )\n",
    "\n",
    "                # Append to dataset and labels\n",
    "                dataset.append(graph)\n",
    "                labels.append(f'{material}-{polymorf}')\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, parameters = GML.standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset,    dataset_name_std)\n",
    "    torch.save(parameters, parameters_name_std)\n",
    "    torch.save(labels,     labels_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e59715",
   "metadata": {},
   "source": [
    "# Definition of train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96173a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 73670\n",
      "Number of testing  graphs: 0\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(12345)\n",
    "\n",
    "# Define the sizes of the train and test sets\n",
    "test_size  = int(test_ratio * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "\n",
    "# Use random_split() to generate train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of testing  graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d7523e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GM_library' from '/Users/cibran/Work/UPC/GenerativeModels/GM_library.py'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f86aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(5, 256)\n",
      "  (conv2): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of features in dataset, considering the t_step information\n",
    "n_features = dataset[0].num_node_features + 1\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = GML.nGCNN(n_features, dropout_node).to(device)\n",
    "edge_model = GML.eGCNN(n_features, dropout_edge).to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Evaluate model state\n",
    "node_model.eval()\n",
    "edge_model.eval()\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d75d00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 179278.5708\n",
      "Epoch: 2, Train Loss: 316.5640\n",
      "Epoch: 3, Train Loss: 63.3639\n",
      "Epoch: 4, Train Loss: 7.9131\n",
      "Epoch: 5, Train Loss: 2.7593\n",
      "Epoch: 6, Train Loss: 2.1329\n",
      "Epoch: 7, Train Loss: 2.1127\n",
      "Epoch: 8, Train Loss: 2.0214\n",
      "Epoch: 9, Train Loss: 2.0088\n",
      "Epoch: 10, Train Loss: 2.0163\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m graph_t \u001b[38;5;241m=\u001b[39m GML\u001b[38;5;241m.\u001b[39madd_t_information(graph_t, t_step)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Perform a single forward pass for predicting node features\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m out_x \u001b[38;5;241m=\u001b[39m \u001b[43mnode_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgraph_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgraph_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Remove t_step information\u001b[39;00m\n\u001b[1;32m     48\u001b[0m out_x \u001b[38;5;241m=\u001b[39m out_x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Work/UPC/GenerativeModels/GM_library.py:646\u001b[0m, in \u001b[0;36mnGCNN.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    644\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index, edge_attr)\n\u001b[1;32m    645\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[0;32m--> 646\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch_geometric/nn/conv/graph_conv.py:86\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, x, edge_index, edge_weight, size)\u001b[0m\n\u001b[1;32m     83\u001b[0m     x: OptPairTensor \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_rel(out)\n\u001b[1;32m     90\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:467\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 467\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    469\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    train_loss = 0\n",
    "    graph_counter = 0\n",
    "    for graph in train_dataset:\n",
    "        # Count new graph\n",
    "        graph_counter += 1\n",
    "        \n",
    "        #print()\n",
    "        #print(f'Graph: {graph_counter}')\n",
    "        # Clone existing graph\n",
    "        graph_0 = graph.clone()\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        for t_step in np.arange(1, n_t_steps+1):\n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            graph_t, epsilon_t = GML.diffusion_step(graph_0, t_step, n_t_steps, alpha_decay)\n",
    "            \n",
    "            # Update diffused graph as next one\n",
    "            graph_0 = graph_t.clone()\n",
    "\n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add t_step information to graph_t\n",
    "            graph_t = GML.add_t_information(graph_t, t_step)\n",
    "\n",
    "            # Perform a single forward pass for predicting node features\n",
    "            out_x = node_model(graph_t.x,\n",
    "                               graph_t.edge_index,\n",
    "                               graph_t.edge_attr)\n",
    "            \n",
    "            # Remove t_step information\n",
    "            out_x = out_x[:, :-1]\n",
    "\n",
    "            # Define x_i and x_j as features of every corresponding pair of nodes (same order than attributes)\n",
    "            x_i = graph_t.x[graph_t.edge_index[0]]\n",
    "            x_j = graph_t.x[graph_t.edge_index[1]]\n",
    "\n",
    "            # Perform a single forward pass for predicting edge attributes\n",
    "            # Introduce previous edge attributes as features as well\n",
    "            out_attr = edge_model(x_i, x_j, graph_t.edge_attr)\n",
    "\n",
    "            # Construct noise graph\n",
    "            pred_epsilon_t = Data(x=out_x,\n",
    "                                  edge_index=graph_t.edge_index,\n",
    "                                  edge_attr=out_attr.ravel())\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the loss for node features and edge attributes\n",
    "            node_loss, edge_loss = GML.get_graph_losses(epsilon_t, pred_epsilon_t)\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            node_loss.backward(retain_graph=True)\n",
    "            node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            edge_loss.backward(retain_graph=True)\n",
    "            edge_optimizer.step()\n",
    "\n",
    "            # Predict target for current graph\n",
    "            #predicted_target = ###\n",
    "\n",
    "            # Compute target loss\n",
    "            #target_loss = GML.get_target_loss(predicted_target, seeked_target)\n",
    "            target_loss = 0\n",
    "\n",
    "            # Accumulate the total training loss\n",
    "            loss = node_loss + edge_loss + target_loss\n",
    "            train_loss += loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    train_loss = train_loss / (len(train_dataset) * n_t_steps)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "    \n",
    "    if (epoch % 20) == 0:\n",
    "        torch.save(node_model.state_dict(), node_model_name)\n",
    "        torch.save(edge_model.state_dict(), edge_model_name)\n",
    "\n",
    "torch.save(node_model.state_dict(), node_model_name)\n",
    "torch.save(edge_model.state_dict(), edge_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8712c82-7363-4d0a-acc4-6c16d4e95eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(train_losses))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
