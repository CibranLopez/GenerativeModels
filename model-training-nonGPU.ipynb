{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.501591Z",
     "start_time": "2024-04-04T23:51:32.986514Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy             as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torch_geometric.data   import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph, predict_noise, diffuse, denoise, EarlyStopping\n",
    "from libraries.dataset      import standardize_dataset, get_datasets\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.570246Z",
     "start_time": "2024-04-04T23:51:42.504503Z"
    }
   },
   "id": "74450972cebeaa56",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ad446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.572006Z",
     "start_time": "2024-04-04T23:51:42.535542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs\n",
    "# The models is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property\n",
    "# Although denoising includes noise, I think it is better not to add it when training"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'models/GM_molecules/GM_v7'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'GM_molecules'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "# Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "i = 0\n",
    "while True:\n",
    "    target_folder = f'{general_folder}/GM_v{i}'\n",
    "    if not os.path.exists(target_folder):\n",
    "        # Copy all data\n",
    "        os.system(f'cp -r {data_folder} {target_folder}')\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "target_folder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.702987Z",
     "start_time": "2024-04-04T23:51:42.600065Z"
    }
   },
   "id": "4d31022ebdd0dbf4",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 500\n",
    "batch_size    = 10\n",
    "learning_rate = 0.0001\n",
    "patience      = 20\n",
    "delta         = 0.2\n",
    "check_labels  = True  # Whether to train-test split attending to labels or not\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 20\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = 0  # Zero for training purposes\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':        data_folder,\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'patience':           patience,\n",
    "    'delta':              delta,\n",
    "    'check_labels':       check_labels,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'sigma':              sigma,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.728882Z",
     "start_time": "2024-04-04T23:51:42.702493Z"
    }
   },
   "id": "ffa9ac369c3a9006",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load of graph database for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeb3c2d70eb68bdb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset, already standardized."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4b0adac6b66bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "if os.path.exists(dataset_name_std) and os.path.exists(dataset_parameters_name_std) and os.path.exists(labels_name):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset = torch.load(dataset_name_std)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Load the data from the JSON file\n",
    "    with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "        numpy_dict = json.load(json_file)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    dataset_parameters = {}\n",
    "    for key, value in numpy_dict.items():\n",
    "        try:\n",
    "            dataset_parameters[key] = torch.tensor(value)\n",
    "        except:\n",
    "            dataset_parameters[key] = value\n",
    "\n",
    "elif os.path.exists(dataset_name) and os.path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)\n",
    "\n",
    "else:\n",
    "    sys.exit('Error: the database is not available')\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.824387Z",
     "start_time": "2024-04-04T23:51:42.726128Z"
    }
   },
   "id": "bf186ea431a29ccc",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split in train, validation and test sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4295b379d810af37"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   graphs: 68\n",
      "Number of validation graphs: 9\n",
      "Number of testing    graphs: 8\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio  = 0.1\n",
    "\n",
    "# Check if data has been already split, else do it randomly\n",
    "path_to_train_labels = f'{target_folder}/train_labels.txt'\n",
    "path_to_val_labels   = f'{target_folder}/validation_labels.txt'\n",
    "path_to_test_labels  = f'{target_folder}/test_labels.txt'\n",
    "\n",
    "# Copy labels\n",
    "material_labels = labels.copy()\n",
    "\n",
    "if os.path.exists(path_to_train_labels) and os.path.exists(path_to_val_labels) and os.path.exists(path_to_test_labels):\n",
    "    # Read labels splitting (which are strings)\n",
    "    train_labels = np.genfromtxt(path_to_train_labels, dtype='str').tolist()\n",
    "    val_labels   = np.genfromtxt(path_to_val_labels,   dtype='str').tolist()\n",
    "    test_labels  = np.genfromtxt(path_to_test_labels,  dtype='str').tolist()\n",
    "else:\n",
    "    if check_labels:\n",
    "        # Splitting into train-test sets considering that Fvs from the same materials must be in the same dataset\n",
    "        material_labels = [label.split()[0] for label in material_labels]\n",
    "        \n",
    "        # Define unique labels\n",
    "        unique_labels = np.unique(material_labels)\n",
    "    else:\n",
    "        # Completely randomly splitting\n",
    "        # Copy material_labels\n",
    "        unique_labels = material_labels.copy()\n",
    "    \n",
    "    # Shuffle the list of unique labels\n",
    "    np.random.shuffle(unique_labels)\n",
    "\n",
    "    # Define the sizes of the train and test sets\n",
    "    # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "    train_size = int(train_ratio * len(unique_labels))\n",
    "    test_size  = int(test_ratio  * len(unique_labels))\n",
    "    \n",
    "    train_labels = unique_labels[:train_size]\n",
    "    val_labels   = unique_labels[train_size:-test_size]\n",
    "    test_labels  = unique_labels[-test_size:]\n",
    "\n",
    "    # Save this splitting for transfer-learning approaches\n",
    "    np.savetxt(path_to_train_labels, train_labels, fmt='%s')\n",
    "    np.savetxt(path_to_val_labels,   val_labels,   fmt='%s')\n",
    "    np.savetxt(path_to_test_labels,  test_labels,  fmt='%s')\n",
    "\n",
    "# Use the computed indexes to generate train and test sets\n",
    "# We iteratively check where labels equals a unique train/test labels and append the index to a list\n",
    "train_dataset = get_datasets(train_labels, material_labels, dataset)\n",
    "val_dataset   = get_datasets(val_labels,   material_labels, dataset)\n",
    "test_dataset  = get_datasets(test_labels,  material_labels, dataset)\n",
    "\n",
    "del dataset  # Free up CUDA memory\n",
    "\n",
    "print(f'Number of training   graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of testing    graphs: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.946584Z",
     "start_time": "2024-04-04T23:51:42.823254Z"
    }
   },
   "id": "55178ef4803e580c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for graph in train_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for graph in val_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for graph in test_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.949319Z",
     "start_time": "2024-04-04T23:51:42.855134Z"
    }
   },
   "id": "e435f7665da018fd",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define data loaders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "549a2953188746d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(train_dataset[0].y)\n",
    "\n",
    "del train_dataset, val_dataset, test_dataset  # Free up CUDA memory"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:42.979998Z",
     "start_time": "2024-04-04T23:51:42.883364Z"
    }
   },
   "id": "86402c04-4b6c-4dfb-b112-6574fc89380d",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definition of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28881dd9d41fdf8e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1591eccef168173b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T23:51:43.138931Z",
     "start_time": "2024-04-04T23:51:42.941265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(6, 256)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (conv3): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "\n",
    "# Moving models to device\n",
    "node_model = node_model.to(device)\n",
    "edge_model = edge_model.to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "    \n",
    "    # Evaluate model state\n",
    "    node_model.eval()\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training of the model"
   ],
   "metadata": {},
   "id": "31a76fc0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "del train_loader, val_loader, test_loader  # Free up CUDA memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4895cf6639b5ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch_geometric.data          import Data\n",
    "\n",
    "def predict_noise(graph, node_model, edge_model):\n",
    "    \"\"\"Predicts noise given some batch of noisy graphs using specified nmodels\n",
    "\n",
    "    Args:\n",
    "        g_batch_t  (torch_geometric.data.Data): Batch with noisy undirected graphs, consistent with model definitions.\n",
    "        node_model (torch.nn.Module):           Model for graph-node prediction.\n",
    "        edge_model (torch.nn.Module):           Model for graph-edge prediction.\n",
    "\n",
    "    Returns:\n",
    "        pred_e_batch_t (torch_geometric.data.Data): Predicted noise for batch g_batch_t.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform a single forward pass for predicting node features\n",
    "    out_x = node_model(graph.x,\n",
    "                       graph.edge_index,\n",
    "                       graph.edge_attr)\n",
    "\n",
    "    # Remove t_step information\n",
    "    out_x = out_x[:, :-1]\n",
    "\n",
    "    # Define x_i and x_j as features of every corresponding pair of nodes (same order than attributes)\n",
    "    x_i = graph.x[graph.edge_index[0]]\n",
    "    x_j = graph.x[graph.edge_index[1]]\n",
    "\n",
    "    # Perform a single forward pass for predicting edge attributes\n",
    "    # Introduce previous edge attributes as features as well\n",
    "    out_attr = edge_model(x_i, x_j, graph.edge_attr)\n",
    "\n",
    "    # Moving data to device\n",
    "    out_x    = out_x.to(device)\n",
    "    out_attr = out_attr.to(device).ravel()\n",
    "\n",
    "    # Generate batch objects\n",
    "    pred_e = Data(x=out_x,\n",
    "                  edge_index=graph.edge_index,\n",
    "                  edge_attr=out_attr)\n",
    "\n",
    "    # Move data to device\n",
    "    pred_e = pred_e.to(device)\n",
    "    return pred_e"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e83adec06eda442"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, edge loss: 1.0013, node loss: 1.4274 1.4490 1.1713 1.0861\n",
      "Epoch: 2, edge loss: 1.0013, node loss: 1.0514 1.0460 1.0622 1.0312\n",
      "Epoch: 3, edge loss: 0.9988, node loss: 1.0210 1.0221 1.0183 1.0572\n",
      "Epoch: 4, edge loss: 0.9999, node loss: 1.0144 1.0357 1.0201 1.0166\n",
      "Epoch: 5, edge loss: 0.9998, node loss: 0.9980 1.0640 1.0163 1.0059\n",
      "Epoch: 6, edge loss: 1.0005, node loss: 1.0153 1.0120 1.0180 1.0090\n",
      "Epoch: 7, edge loss: 0.9991, node loss: 1.0045 1.0142 1.0067 1.0016\n",
      "Epoch: 8, edge loss: 1.0008, node loss: 1.0122 1.0001 1.0157 1.0107\n",
      "Epoch: 9, edge loss: 0.9983, node loss: 0.9977 1.0070 0.9851 1.0019\n",
      "Epoch: 10, edge loss: 1.0002, node loss: 1.0099 0.9989 1.0044 0.9991\n",
      "Epoch: 11, edge loss: 0.9979, node loss: 1.0106 1.0067 1.0059 1.0102\n",
      "Epoch: 12, edge loss: 0.9977, node loss: 1.0009 1.0122 1.0069 1.0020\n",
      "Epoch: 13, edge loss: 1.0011, node loss: 1.0134 0.9990 1.0059 1.0119\n",
      "Epoch: 14, edge loss: 1.0011, node loss: 1.0055 1.0047 1.0126 1.0030\n",
      "Epoch: 15, edge loss: 0.9951, node loss: 0.9971 0.9890 1.0060 0.9974\n",
      "Epoch: 16, edge loss: 0.9947, node loss: 1.0030 1.0026 0.9860 1.0044\n",
      "Epoch: 17, edge loss: 0.9963, node loss: 0.9975 0.9916 0.9876 0.9981\n",
      "Epoch: 18, edge loss: 1.0001, node loss: 1.0009 0.9970 0.9896 0.9975\n",
      "Epoch: 19, edge loss: 1.0015, node loss: 0.9996 0.9939 1.0004 0.9916\n",
      "Epoch: 20, edge loss: 0.9978, node loss: 0.9917 1.0035 1.0052 1.0048\n",
      "Epoch: 21, edge loss: 0.9999, node loss: 1.0072 0.9899 0.9887 0.9929\n",
      "Epoch: 22, edge loss: 0.9994, node loss: 0.9971 0.9945 0.9941 0.9977\n",
      "Epoch: 23, edge loss: 0.9976, node loss: 0.9963 0.9905 0.9909 0.9935\n",
      "Epoch: 24, edge loss: 0.9960, node loss: 0.9949 1.0079 1.0004 1.0034\n",
      "Epoch: 25, edge loss: 0.9940, node loss: 0.9925 0.9832 0.9843 0.9993\n",
      "Epoch: 26, edge loss: 0.9984, node loss: 0.9869 0.9892 0.9944 0.9861\n",
      "Epoch: 27, edge loss: 0.9975, node loss: 0.9784 0.9784 0.9811 0.9989\n",
      "Epoch: 28, edge loss: 0.9982, node loss: 0.9915 0.9776 0.9831 0.9905\n",
      "Epoch: 29, edge loss: 0.9986, node loss: 0.9830 0.9750 0.9790 0.9999\n",
      "Epoch: 30, edge loss: 0.9964, node loss: 0.9828 0.9835 0.9897 0.9989\n",
      "Epoch: 31, edge loss: 0.9954, node loss: 0.9674 0.9699 0.9798 0.9862\n",
      "Epoch: 32, edge loss: 0.9958, node loss: 0.9754 0.9678 0.9819 0.9922\n",
      "Epoch: 33, edge loss: 0.9990, node loss: 0.9775 0.9768 0.9690 0.9921\n",
      "Epoch: 34, edge loss: 0.9983, node loss: 0.9747 0.9811 0.9670 0.9781\n",
      "Epoch: 35, edge loss: 0.9968, node loss: 0.9638 0.9653 0.9659 0.9839\n",
      "Epoch: 36, edge loss: 0.9991, node loss: 0.9571 0.9477 0.9674 0.9770\n",
      "Epoch: 37, edge loss: 0.9984, node loss: 0.9584 0.9707 0.9633 0.9834\n",
      "Epoch: 38, edge loss: 0.9976, node loss: 0.9442 0.9636 0.9606 0.9763\n",
      "Epoch: 39, edge loss: 0.9984, node loss: 0.9506 0.9701 0.9492 0.9634\n",
      "Epoch: 40, edge loss: 0.9945, node loss: 0.9349 0.9420 0.9327 0.9586\n",
      "Epoch: 41, edge loss: 0.9971, node loss: 0.9607 0.9479 0.9271 0.9683\n",
      "Epoch: 42, edge loss: 0.9960, node loss: 0.9439 0.9438 0.9339 0.9659\n",
      "Epoch: 43, edge loss: 0.9943, node loss: 0.9361 0.9429 0.9296 0.9682\n",
      "Epoch: 44, edge loss: 0.9955, node loss: 0.9195 0.9288 0.9269 0.9459\n",
      "Epoch: 45, edge loss: 0.9964, node loss: 0.9291 0.9248 0.9139 0.9589\n",
      "Epoch: 46, edge loss: 0.9973, node loss: 0.9035 0.9165 0.9006 0.9519\n",
      "Epoch: 47, edge loss: 0.9946, node loss: 0.9056 0.8976 0.8817 0.9440\n",
      "Epoch: 48, edge loss: 0.9985, node loss: 0.9016 0.9074 0.8871 0.9386\n",
      "Epoch: 49, edge loss: 0.9966, node loss: 0.9049 0.9197 0.8980 0.9307\n",
      "Epoch: 50, edge loss: 0.9968, node loss: 0.8731 0.8802 0.8716 0.9289\n",
      "Epoch: 51, edge loss: 0.9936, node loss: 0.8884 0.8849 0.8672 0.9360\n",
      "Epoch: 52, edge loss: 0.9946, node loss: 0.8756 0.8883 0.8672 0.9276\n",
      "Epoch: 53, edge loss: 0.9941, node loss: 0.8599 0.8605 0.8388 0.9200\n",
      "Epoch: 54, edge loss: 0.9964, node loss: 0.8390 0.8583 0.8255 0.9001\n",
      "Epoch: 55, edge loss: 0.9984, node loss: 0.8766 0.8509 0.8517 0.9290\n",
      "Epoch: 56, edge loss: 0.9965, node loss: 0.8406 0.8532 0.8240 0.9135\n",
      "Epoch: 57, edge loss: 0.9937, node loss: 0.8302 0.8446 0.8230 0.9121\n",
      "Epoch: 58, edge loss: 0.9967, node loss: 0.8421 0.8466 0.7915 0.8839\n",
      "Epoch: 59, edge loss: 0.9963, node loss: 0.8122 0.8125 0.7698 0.8747\n",
      "Epoch: 60, edge loss: 0.9954, node loss: 0.8625 0.8862 0.8029 0.8922\n",
      "Epoch: 61, edge loss: 0.9985, node loss: 0.8004 0.8042 0.7584 0.8776\n",
      "Epoch: 62, edge loss: 0.9979, node loss: 0.7973 0.7997 0.7833 0.8642\n",
      "Epoch: 63, edge loss: 0.9948, node loss: 0.8212 0.8125 0.7569 0.8592\n",
      "Epoch: 64, edge loss: 0.9966, node loss: 0.8051 0.8485 0.7640 0.8572\n",
      "Epoch: 65, edge loss: 0.9953, node loss: 0.7956 0.7736 0.7753 0.8483\n",
      "Epoch: 66, edge loss: 0.9956, node loss: 0.7729 0.7618 0.7200 0.8504\n",
      "Epoch: 67, edge loss: 0.9981, node loss: 0.7846 0.8139 0.7646 0.8388\n",
      "Epoch: 68, edge loss: 0.9984, node loss: 0.7865 0.7731 0.7277 0.8310\n",
      "Epoch: 69, edge loss: 0.9933, node loss: 0.7667 0.7714 0.6901 0.8426\n",
      "Epoch: 70, edge loss: 0.9967, node loss: 0.7663 0.7533 0.6834 0.8586\n",
      "Epoch: 71, edge loss: 0.9933, node loss: 0.7999 0.7596 0.7206 0.8673\n",
      "Epoch: 72, edge loss: 0.9957, node loss: 0.7355 0.7362 0.6832 0.8297\n",
      "Epoch: 73, edge loss: 0.9963, node loss: 0.7313 0.7940 0.7527 0.8109\n",
      "Epoch: 74, edge loss: 0.9930, node loss: 0.7438 0.7368 0.6558 0.7924\n",
      "Epoch: 75, edge loss: 0.9921, node loss: 0.8026 0.7302 0.6607 0.8078\n",
      "Epoch: 76, edge loss: 0.9940, node loss: 0.7035 0.6994 0.6594 0.7908\n",
      "Epoch: 77, edge loss: 0.9956, node loss: 0.7350 0.7995 0.6536 0.8209\n",
      "Epoch: 78, edge loss: 0.9953, node loss: 0.7313 0.7579 0.6710 0.8110\n",
      "Epoch: 79, edge loss: 0.9940, node loss: 0.7009 0.7059 0.6498 0.7904\n",
      "Epoch: 80, edge loss: 0.9944, node loss: 0.7260 0.6971 0.6531 0.8135\n",
      "Epoch: 81, edge loss: 0.9906, node loss: 0.7146 0.7047 0.6901 0.7875\n",
      "Epoch: 82, edge loss: 0.9957, node loss: 0.6633 0.7015 0.6261 0.7521\n",
      "Epoch: 83, edge loss: 0.9928, node loss: 0.7308 0.7130 0.6412 0.7960\n",
      "Epoch: 84, edge loss: 0.9937, node loss: 0.7065 0.7131 0.6141 0.7715\n",
      "Epoch: 85, edge loss: 0.9916, node loss: 0.7422 0.6901 0.6302 0.7944\n",
      "Epoch: 86, edge loss: 0.9950, node loss: 0.6770 0.6821 0.5802 0.7885\n",
      "Epoch: 87, edge loss: 0.9917, node loss: 0.6940 0.6850 0.6085 0.7722\n",
      "Epoch: 88, edge loss: 0.9940, node loss: 0.6889 0.7388 0.6068 0.7484\n",
      "Epoch: 89, edge loss: 0.9929, node loss: 0.6571 0.6866 0.5508 0.7991\n",
      "Epoch: 90, edge loss: 0.9959, node loss: 0.6508 0.6371 0.6141 0.7737\n",
      "Epoch: 91, edge loss: 0.9953, node loss: 0.7440 0.6820 0.5988 0.7692\n",
      "Epoch: 92, edge loss: 0.9936, node loss: 0.6467 0.6451 0.5953 0.7552\n",
      "Epoch: 93, edge loss: 0.9933, node loss: 0.7181 0.7356 0.6179 0.7375\n",
      "Epoch: 94, edge loss: 0.9903, node loss: 0.6491 0.6537 0.6081 0.7482\n",
      "Epoch: 95, edge loss: 0.9931, node loss: 0.6606 0.6359 0.5791 0.7838\n",
      "Epoch: 96, edge loss: 0.9965, node loss: 0.6482 0.6774 0.5521 0.7248\n",
      "Epoch: 97, edge loss: 0.9920, node loss: 0.6585 0.6974 0.5655 0.7465\n",
      "Epoch: 98, edge loss: 0.9939, node loss: 0.6637 0.6501 0.5742 0.7482\n",
      "Epoch: 99, edge loss: 0.9911, node loss: 0.6864 0.6513 0.5964 0.7458\n",
      "Epoch: 100, edge loss: 0.9888, node loss: 0.6190 0.6119 0.5799 0.7187\n",
      "Epoch: 101, edge loss: 0.9940, node loss: 0.6602 0.6234 0.6025 0.7130\n",
      "Epoch: 102, edge loss: 0.9917, node loss: 0.6679 0.7305 0.5619 0.7336\n",
      "Epoch: 103, edge loss: 0.9896, node loss: 0.6421 0.6453 0.5354 0.7345\n",
      "Epoch: 104, edge loss: 0.9926, node loss: 0.6560 0.6305 0.5749 0.7374\n",
      "Epoch: 105, edge loss: 0.9932, node loss: 0.6528 0.5969 0.5527 0.7461\n",
      "Epoch: 106, edge loss: 0.9915, node loss: 0.6451 0.6609 0.6018 0.6984\n",
      "Epoch: 107, edge loss: 0.9906, node loss: 0.6195 0.6305 0.5270 0.7009\n",
      "Epoch: 108, edge loss: 0.9910, node loss: 0.6590 0.6179 0.5043 0.7674\n",
      "Epoch: 109, edge loss: 0.9919, node loss: 0.6463 0.6586 0.6058 0.6966\n",
      "Epoch: 110, edge loss: 0.9934, node loss: 0.6681 0.6055 0.6060 0.7383\n",
      "Epoch: 111, edge loss: 0.9917, node loss: 0.5868 0.5751 0.5071 0.6956\n",
      "Epoch: 112, edge loss: 0.9920, node loss: 0.5916 0.6492 0.5702 0.6965\n",
      "Epoch: 113, edge loss: 0.9917, node loss: 0.7136 0.7032 0.5302 0.7610\n",
      "Epoch: 114, edge loss: 0.9920, node loss: 0.6330 0.6777 0.5427 0.7127\n",
      "Epoch: 115, edge loss: 0.9934, node loss: 0.6213 0.6071 0.5359 0.7145\n",
      "Epoch: 116, edge loss: 0.9915, node loss: 0.5965 0.6072 0.5358 0.6961\n",
      "Epoch: 117, edge loss: 0.9915, node loss: 0.7051 0.6345 0.5602 0.7017\n",
      "Epoch: 118, edge loss: 0.9914, node loss: 0.6063 0.5839 0.5120 0.7255\n",
      "Epoch: 119, edge loss: 0.9927, node loss: 0.6051 0.5954 0.6085 0.7268\n",
      "Epoch: 120, edge loss: 0.9935, node loss: 0.6147 0.6045 0.5397 0.7176\n",
      "Epoch: 121, edge loss: 0.9862, node loss: 0.6182 0.6604 0.5474 0.7134\n",
      "Epoch: 122, edge loss: 0.9931, node loss: 0.6169 0.6106 0.5181 0.6939\n",
      "Epoch: 123, edge loss: 0.9917, node loss: 0.6436 0.6142 0.5148 0.6940\n",
      "Epoch: 124, edge loss: 0.9909, node loss: 0.6291 0.5930 0.5263 0.6933\n",
      "Epoch: 125, edge loss: 0.9913, node loss: 0.5883 0.6048 0.5365 0.6727\n",
      "Epoch: 126, edge loss: 0.9896, node loss: 0.6326 0.5846 0.5369 0.7214\n",
      "Epoch: 127, edge loss: 0.9905, node loss: 0.6486 0.6324 0.5247 0.7893\n",
      "Epoch: 128, edge loss: 0.9878, node loss: 0.5900 0.5821 0.5139 0.6770\n",
      "Epoch: 129, edge loss: 0.9905, node loss: 0.6035 0.5763 0.5440 0.6857\n",
      "Epoch: 130, edge loss: 0.9905, node loss: 0.6308 0.6594 0.5125 0.6847\n",
      "Epoch: 131, edge loss: 0.9917, node loss: 0.5828 0.6350 0.5223 0.6785\n",
      "Epoch: 132, edge loss: 0.9934, node loss: 0.6040 0.6324 0.5331 0.7059\n",
      "Epoch: 133, edge loss: 0.9881, node loss: 0.6002 0.5554 0.5221 0.6946\n",
      "Epoch: 134, edge loss: 0.9917, node loss: 0.6217 0.6262 0.4783 0.6722\n",
      "Epoch: 135, edge loss: 0.9903, node loss: 0.6340 0.5932 0.5493 0.7164\n",
      "Epoch: 136, edge loss: 0.9912, node loss: 0.5594 0.5614 0.5004 0.6841\n",
      "Epoch: 137, edge loss: 0.9910, node loss: 0.6093 0.5989 0.5379 0.7421\n",
      "Epoch: 138, edge loss: 0.9910, node loss: 0.5801 0.5754 0.4934 0.6594\n",
      "Epoch: 139, edge loss: 0.9887, node loss: 0.6257 0.6764 0.5659 0.7413\n",
      "Epoch: 140, edge loss: 0.9934, node loss: 0.5817 0.5839 0.4822 0.6948\n",
      "Epoch: 141, edge loss: 0.9872, node loss: 0.6827 0.5748 0.4658 0.6782\n",
      "Epoch: 142, edge loss: 0.9904, node loss: 0.6159 0.5537 0.5058 0.6723\n",
      "Epoch: 143, edge loss: 0.9933, node loss: 0.5570 0.5776 0.5007 0.6885\n",
      "Epoch: 144, edge loss: 0.9887, node loss: 0.6096 0.6373 0.4663 0.7023\n",
      "Epoch: 145, edge loss: 0.9878, node loss: 0.5616 0.5613 0.4413 0.6851\n",
      "Epoch: 146, edge loss: 0.9919, node loss: 0.6100 0.5830 0.6666 0.6671\n",
      "Epoch: 147, edge loss: 0.9914, node loss: 0.5575 0.5523 0.4997 0.6488\n",
      "Epoch: 148, edge loss: 0.9899, node loss: 0.6314 0.5941 0.5050 0.7377\n",
      "Epoch: 149, edge loss: 0.9910, node loss: 0.6031 0.6591 0.4756 0.7071\n",
      "Epoch: 150, edge loss: 0.9922, node loss: 0.5449 0.5395 0.4995 0.6480\n",
      "Epoch: 151, edge loss: 0.9882, node loss: 0.5929 0.5684 0.6167 0.6618\n",
      "Epoch: 152, edge loss: 0.9916, node loss: 0.6024 0.5801 0.4695 0.6543\n",
      "Epoch: 153, edge loss: 0.9881, node loss: 0.5769 0.5762 0.4666 0.6509\n",
      "Epoch: 154, edge loss: 0.9890, node loss: 0.5962 0.5671 0.4720 0.6560\n",
      "Epoch: 155, edge loss: 0.9929, node loss: 0.5960 0.6131 0.5053 0.7577\n",
      "Epoch: 156, edge loss: 0.9889, node loss: 0.5471 0.5911 0.4693 0.7089\n",
      "Epoch: 157, edge loss: 0.9911, node loss: 0.5415 0.5583 0.4876 0.6612\n",
      "Epoch: 158, edge loss: 0.9899, node loss: 0.6328 0.5927 0.4973 0.7234\n",
      "Epoch: 159, edge loss: 0.9867, node loss: 0.6016 0.6058 0.4437 0.6714\n",
      "Epoch: 160, edge loss: 0.9880, node loss: 0.6004 0.5513 0.5483 0.6525\n",
      "Epoch: 161, edge loss: 0.9874, node loss: 0.5603 0.5607 0.5304 0.6499\n",
      "Epoch: 162, edge loss: 0.9917, node loss: 0.5514 0.5662 0.4567 0.6174\n",
      "Epoch: 163, edge loss: 0.9872, node loss: 0.6245 0.5658 0.5187 0.6773\n",
      "Epoch: 164, edge loss: 0.9889, node loss: 0.5752 0.5331 0.4712 0.6561\n",
      "Epoch: 165, edge loss: 0.9896, node loss: 0.5774 0.5764 0.5438 0.6775\n",
      "Epoch: 166, edge loss: 0.9913, node loss: 0.6076 0.5858 0.5027 0.6571\n",
      "Epoch: 167, edge loss: 0.9904, node loss: 0.5539 0.5381 0.4477 0.6437\n",
      "Epoch: 168, edge loss: 0.9874, node loss: 0.6578 0.5998 0.4592 0.6430\n",
      "Epoch: 169, edge loss: 0.9894, node loss: 0.5769 0.5713 0.4637 0.6743\n",
      "Epoch: 170, edge loss: 0.9895, node loss: 0.5710 0.5443 0.4898 0.6856\n",
      "Epoch: 171, edge loss: 0.9913, node loss: 0.5819 0.5490 0.4633 0.6506\n",
      "Epoch: 172, edge loss: 0.9907, node loss: 0.5390 0.5403 0.5351 0.6246\n",
      "Epoch: 173, edge loss: 0.9859, node loss: 0.5413 0.6338 0.4564 0.7112\n",
      "Epoch: 174, edge loss: 0.9891, node loss: 0.5508 0.5560 0.4489 0.6891\n",
      "Epoch: 175, edge loss: 0.9876, node loss: 0.5836 0.5711 0.4640 0.6253\n",
      "Epoch: 176, edge loss: 0.9915, node loss: 0.5533 0.5525 0.5197 0.6637\n",
      "Epoch: 177, edge loss: 0.9871, node loss: 0.5828 0.5358 0.4815 0.6432\n",
      "Epoch: 178, edge loss: 0.9904, node loss: 0.5581 0.5168 0.4934 0.6751\n",
      "Epoch: 179, edge loss: 0.9927, node loss: 0.5887 0.6200 0.4520 0.6460\n",
      "Epoch: 180, edge loss: 0.9876, node loss: 0.5615 0.5778 0.4765 0.6579\n",
      "Epoch: 181, edge loss: 0.9906, node loss: 0.5211 0.5139 0.4435 0.6949\n",
      "Epoch: 182, edge loss: 0.9893, node loss: 0.5814 0.6107 0.5474 0.7463\n",
      "Epoch: 183, edge loss: 0.9914, node loss: 0.5591 0.5369 0.4486 0.6339\n",
      "Epoch: 184, edge loss: 0.9897, node loss: 0.5740 0.5490 0.4540 0.6544\n",
      "Epoch: 185, edge loss: 0.9942, node loss: 0.5332 0.5489 0.4990 0.6176\n",
      "Epoch: 186, edge loss: 0.9879, node loss: 0.6280 0.5187 0.5616 0.6626\n",
      "Epoch: 187, edge loss: 0.9888, node loss: 0.5261 0.5533 0.4583 0.6319\n",
      "Epoch: 188, edge loss: 0.9868, node loss: 0.5370 0.5928 0.4881 0.6737\n",
      "Epoch: 189, edge loss: 0.9919, node loss: 0.5569 0.5016 0.4818 0.6403\n",
      "Epoch: 190, edge loss: 0.9882, node loss: 0.6193 0.5367 0.4307 0.6699\n",
      "Epoch: 191, edge loss: 0.9882, node loss: 0.5278 0.5391 0.4549 0.6320\n",
      "Epoch: 192, edge loss: 0.9930, node loss: 0.5558 0.5135 0.5186 0.6250\n",
      "Epoch: 193, edge loss: 0.9880, node loss: 0.5958 0.5364 0.4697 0.6659\n",
      "Epoch: 194, edge loss: 0.9864, node loss: 0.5272 0.5559 0.4197 0.6320\n",
      "Epoch: 195, edge loss: 0.9901, node loss: 0.5486 0.5345 0.4782 0.6410\n",
      "Epoch: 196, edge loss: 0.9919, node loss: 0.5047 0.5708 0.5070 0.6365\n",
      "Epoch: 197, edge loss: 0.9902, node loss: 0.5733 0.5448 0.4911 0.6641\n",
      "Epoch: 198, edge loss: 0.9880, node loss: 0.5904 0.5442 0.4785 0.6173\n",
      "Epoch: 199, edge loss: 0.9904, node loss: 0.5948 0.5771 0.4174 0.6589\n",
      "Epoch: 200, edge loss: 0.9897, node loss: 0.5349 0.5021 0.4074 0.6066\n",
      "Epoch: 201, edge loss: 0.9892, node loss: 0.5401 0.5300 0.4972 0.6258\n",
      "Epoch: 202, edge loss: 0.9888, node loss: 0.5754 0.5673 0.4651 0.6479\n",
      "Epoch: 203, edge loss: 0.9895, node loss: 0.5017 0.5968 0.4367 0.6341\n",
      "Epoch: 204, edge loss: 0.9890, node loss: 0.5415 0.5130 0.4433 0.6284\n",
      "Epoch: 205, edge loss: 0.9916, node loss: 0.5591 0.5211 0.4193 0.6221\n",
      "Epoch: 206, edge loss: 0.9899, node loss: 0.5545 0.5226 0.4424 0.7158\n",
      "Epoch: 207, edge loss: 0.9909, node loss: 0.5701 0.5156 0.4122 0.6416\n",
      "Epoch: 208, edge loss: 0.9932, node loss: 0.5776 0.5640 0.5096 0.6611\n",
      "Epoch: 209, edge loss: 0.9906, node loss: 0.5109 0.4917 0.4701 0.6112\n",
      "Epoch: 210, edge loss: 0.9903, node loss: 0.5226 0.6257 0.4877 0.6835\n",
      "Epoch: 211, edge loss: 0.9894, node loss: 0.5313 0.5491 0.4637 0.6453\n",
      "Epoch: 212, edge loss: 0.9896, node loss: 0.5183 0.5156 0.4553 0.6005\n",
      "Epoch: 213, edge loss: 0.9831, node loss: 0.5663 0.5489 0.4678 0.6653\n",
      "Epoch: 214, edge loss: 0.9901, node loss: 0.5281 0.5247 0.4549 0.6340\n",
      "Epoch: 215, edge loss: 0.9868, node loss: 0.5592 0.5547 0.4857 0.6583\n",
      "Epoch: 216, edge loss: 0.9904, node loss: 0.5269 0.5074 0.3903 0.6314\n",
      "Epoch: 217, edge loss: 0.9891, node loss: 0.5420 0.5327 0.4264 0.6335\n",
      "Epoch: 218, edge loss: 0.9900, node loss: 0.5521 0.5251 0.4357 0.6283\n",
      "Epoch: 219, edge loss: 0.9868, node loss: 0.6086 0.4849 0.4779 0.6515\n",
      "Epoch: 220, edge loss: 0.9913, node loss: 0.5267 0.5721 0.4529 0.5975\n",
      "Epoch: 221, edge loss: 0.9875, node loss: 0.5744 0.5301 0.4490 0.6794\n",
      "Epoch: 222, edge loss: 0.9901, node loss: 0.5082 0.5050 0.4527 0.6233\n",
      "Epoch: 223, edge loss: 0.9889, node loss: 0.5467 0.5256 0.4716 0.5725\n",
      "Epoch: 224, edge loss: 0.9885, node loss: 0.5508 0.4999 0.4666 0.6054\n",
      "Epoch: 225, edge loss: 0.9883, node loss: 0.5050 0.5385 0.4696 0.5906\n",
      "Epoch: 226, edge loss: 0.9897, node loss: 0.5324 0.5476 0.4162 0.6259\n",
      "Epoch: 227, edge loss: 0.9881, node loss: 0.5191 0.5386 0.4429 0.7071\n",
      "Epoch: 228, edge loss: 0.9882, node loss: 0.5828 0.5147 0.4571 0.6434\n",
      "Epoch: 229, edge loss: 0.9887, node loss: 0.5320 0.4831 0.4225 0.6093\n",
      "Epoch: 230, edge loss: 0.9852, node loss: 0.5353 0.5192 0.4377 0.6205\n",
      "Epoch: 231, edge loss: 0.9912, node loss: 0.5418 0.5555 0.4635 0.6029\n",
      "Epoch: 232, edge loss: 0.9882, node loss: 0.5414 0.5272 0.4629 0.6357\n",
      "Epoch: 233, edge loss: 0.9924, node loss: 0.5023 0.4777 0.4779 0.6338\n",
      "Epoch: 234, edge loss: 0.9927, node loss: 0.5797 0.5284 0.4179 0.5948\n",
      "Epoch: 235, edge loss: 0.9900, node loss: 0.5454 0.5500 0.4059 0.6068\n",
      "Epoch: 236, edge loss: 0.9904, node loss: 0.5267 0.5284 0.5050 0.6433\n",
      "Epoch: 237, edge loss: 0.9908, node loss: 0.4952 0.4757 0.4249 0.5878\n",
      "Epoch: 238, edge loss: 0.9912, node loss: 0.4899 0.6058 0.4324 0.6438\n",
      "Epoch: 239, edge loss: 0.9857, node loss: 0.5561 0.5326 0.4115 0.6256\n",
      "Epoch: 240, edge loss: 0.9887, node loss: 0.5380 0.4607 0.4450 0.6259\n",
      "Epoch: 241, edge loss: 0.9903, node loss: 0.5468 0.4878 0.4659 0.6642\n",
      "Epoch: 242, edge loss: 0.9892, node loss: 0.4989 0.5181 0.4724 0.5771\n",
      "Epoch: 243, edge loss: 0.9862, node loss: 0.4949 0.4956 0.4434 0.5668\n",
      "Epoch: 244, edge loss: 0.9901, node loss: 0.5545 0.5834 0.4498 0.6758\n",
      "Epoch: 245, edge loss: 0.9896, node loss: 0.4898 0.4735 0.4122 0.5852\n",
      "Epoch: 246, edge loss: 0.9879, node loss: 0.5655 0.5115 0.4300 0.6892\n",
      "Epoch: 247, edge loss: 0.9905, node loss: 0.5014 0.5157 0.4032 0.5956\n",
      "Epoch: 248, edge loss: 0.9897, node loss: 0.5879 0.5066 0.4555 0.5931\n",
      "Epoch: 249, edge loss: 0.9911, node loss: 0.5082 0.4782 0.4235 0.5953\n",
      "Epoch: 250, edge loss: 0.9886, node loss: 0.5148 0.6024 0.4395 0.5965\n",
      "Epoch: 251, edge loss: 0.9885, node loss: 0.4943 0.5502 0.4274 0.6182\n",
      "Epoch: 252, edge loss: 0.9878, node loss: 0.5090 0.5122 0.4367 0.5923\n",
      "Epoch: 253, edge loss: 0.9886, node loss: 0.5300 0.4990 0.4326 0.5734\n",
      "Epoch: 254, edge loss: 0.9924, node loss: 0.4861 0.5014 0.4636 0.6291\n",
      "Epoch: 255, edge loss: 0.9932, node loss: 0.4949 0.5208 0.4346 0.5651\n",
      "Epoch: 256, edge loss: 0.9883, node loss: 0.5074 0.4777 0.4926 0.6024\n",
      "Epoch: 257, edge loss: 0.9830, node loss: 0.4862 0.4958 0.5405 0.5732\n",
      "Epoch: 258, edge loss: 0.9905, node loss: 0.5892 0.4993 0.4295 0.5994\n",
      "Epoch: 259, edge loss: 0.9891, node loss: 0.5331 0.5376 0.4047 0.5981\n",
      "Epoch: 260, edge loss: 0.9952, node loss: 0.5685 0.4786 0.4172 0.6201\n",
      "Epoch: 261, edge loss: 0.9893, node loss: 0.5626 0.5592 0.3942 0.6445\n",
      "Epoch: 262, edge loss: 0.9880, node loss: 0.5250 0.5024 0.4131 0.5740\n",
      "Epoch: 263, edge loss: 0.9882, node loss: 0.5265 0.5088 0.4054 0.5916\n",
      "Epoch: 264, edge loss: 0.9892, node loss: 0.5143 0.4820 0.3813 0.6117\n",
      "Epoch: 265, edge loss: 0.9855, node loss: 0.4836 0.5889 0.4060 0.5743\n",
      "Epoch: 266, edge loss: 0.9911, node loss: 0.4716 0.5366 0.4439 0.6647\n",
      "Epoch: 267, edge loss: 0.9889, node loss: 0.5318 0.5467 0.3952 0.5939\n",
      "Epoch: 268, edge loss: 0.9898, node loss: 0.5127 0.4692 0.4659 0.5445\n",
      "Epoch: 269, edge loss: 0.9871, node loss: 0.5153 0.4945 0.4343 0.6336\n",
      "Epoch: 270, edge loss: 0.9882, node loss: 0.4969 0.4560 0.3988 0.5901\n",
      "Epoch: 271, edge loss: 0.9909, node loss: 0.5311 0.5255 0.3945 0.6146\n",
      "Epoch: 272, edge loss: 0.9915, node loss: 0.4888 0.5009 0.4068 0.5916\n",
      "Epoch: 273, edge loss: 0.9871, node loss: 0.4923 0.5036 0.4785 0.5514\n",
      "Epoch: 274, edge loss: 0.9878, node loss: 0.5167 0.5220 0.4156 0.5679\n",
      "Epoch: 275, edge loss: 0.9845, node loss: 0.5307 0.4721 0.4268 0.6177\n",
      "Epoch: 276, edge loss: 0.9884, node loss: 0.5502 0.5011 0.4081 0.5848\n",
      "Epoch: 277, edge loss: 0.9865, node loss: 0.5321 0.5065 0.4030 0.5606\n",
      "Epoch: 278, edge loss: 0.9882, node loss: 0.4972 0.4641 0.4379 0.5725\n",
      "Epoch: 279, edge loss: 0.9880, node loss: 0.5299 0.5278 0.4066 0.6502\n",
      "Epoch: 280, edge loss: 0.9906, node loss: 0.4538 0.4719 0.4028 0.6311\n",
      "Epoch: 281, edge loss: 0.9916, node loss: 0.4915 0.4789 0.4588 0.5648\n",
      "Epoch: 282, edge loss: 0.9896, node loss: 0.4656 0.5331 0.4106 0.5585\n",
      "Epoch: 283, edge loss: 0.9916, node loss: 0.5186 0.5430 0.4013 0.5858\n",
      "Epoch: 284, edge loss: 0.9895, node loss: 0.5306 0.4986 0.4266 0.5488\n",
      "Epoch: 285, edge loss: 0.9873, node loss: 0.5233 0.4698 0.3970 0.5911\n",
      "Epoch: 286, edge loss: 0.9895, node loss: 0.5032 0.4684 0.4237 0.6079\n",
      "Epoch: 287, edge loss: 0.9882, node loss: 0.4901 0.4963 0.4181 0.5478\n",
      "Epoch: 288, edge loss: 0.9896, node loss: 0.4961 0.4865 0.4336 0.5881\n",
      "Epoch: 289, edge loss: 0.9862, node loss: 0.4818 0.5183 0.3798 0.6369\n",
      "Epoch: 290, edge loss: 0.9908, node loss: 0.4645 0.4998 0.4119 0.5931\n",
      "Epoch: 291, edge loss: 0.9904, node loss: 0.5401 0.4963 0.4196 0.5617\n",
      "Epoch: 292, edge loss: 0.9881, node loss: 0.5050 0.5236 0.3782 0.5698\n",
      "Epoch: 293, edge loss: 0.9872, node loss: 0.4932 0.5051 0.4012 0.5927\n",
      "Epoch: 294, edge loss: 0.9909, node loss: 0.4912 0.4495 0.4015 0.5629\n",
      "Epoch: 295, edge loss: 0.9892, node loss: 0.5328 0.4464 0.4268 0.6303\n",
      "Epoch: 296, edge loss: 0.9902, node loss: 0.5136 0.4730 0.3980 0.5837\n",
      "Epoch: 297, edge loss: 0.9875, node loss: 0.5064 0.5018 0.3725 0.5395\n",
      "Epoch: 298, edge loss: 0.9894, node loss: 0.4717 0.4953 0.4422 0.6022\n",
      "Epoch: 299, edge loss: 0.9916, node loss: 0.5058 0.4983 0.4317 0.5308\n",
      "Epoch: 300, edge loss: 0.9896, node loss: 0.5246 0.4747 0.4068 0.5494\n",
      "Epoch: 301, edge loss: 0.9898, node loss: 0.4751 0.4863 0.4160 0.6259\n",
      "Epoch: 302, edge loss: 0.9893, node loss: 0.5005 0.4791 0.3971 0.5756\n",
      "Epoch: 303, edge loss: 0.9910, node loss: 0.4772 0.4673 0.3759 0.5788\n",
      "Epoch: 304, edge loss: 0.9908, node loss: 0.4661 0.5474 0.4124 0.5587\n",
      "Epoch: 305, edge loss: 0.9913, node loss: 0.4880 0.4744 0.3885 0.5994\n",
      "Epoch: 306, edge loss: 0.9896, node loss: 0.4823 0.4523 0.3658 0.5781\n",
      "Epoch: 307, edge loss: 0.9912, node loss: 0.5391 0.5127 0.4014 0.5877\n",
      "Epoch: 308, edge loss: 0.9885, node loss: 0.4826 0.4954 0.3828 0.5894\n",
      "Epoch: 309, edge loss: 0.9884, node loss: 0.5281 0.4537 0.3731 0.5655\n",
      "Epoch: 310, edge loss: 0.9890, node loss: 0.4954 0.4762 0.4155 0.5701\n",
      "Epoch: 311, edge loss: 0.9881, node loss: 0.4592 0.4742 0.4397 0.5306\n",
      "Epoch: 312, edge loss: 0.9881, node loss: 0.4613 0.4424 0.4464 0.5669\n",
      "Epoch: 313, edge loss: 0.9932, node loss: 0.5181 0.4831 0.4407 0.5439\n",
      "Epoch: 314, edge loss: 0.9900, node loss: 0.4811 0.4706 0.4300 0.5378\n",
      "Epoch: 315, edge loss: 0.9862, node loss: 0.4285 0.4876 0.3896 0.5114\n",
      "Epoch: 316, edge loss: 0.9904, node loss: 0.5135 0.5396 0.4491 0.5647\n",
      "Epoch: 317, edge loss: 0.9890, node loss: 0.4664 0.4653 0.4044 0.5843\n",
      "Epoch: 318, edge loss: 0.9856, node loss: 0.4924 0.4492 0.4248 0.5342\n",
      "Epoch: 319, edge loss: 0.9900, node loss: 0.5029 0.4444 0.3992 0.6032\n",
      "Epoch: 320, edge loss: 0.9887, node loss: 0.5115 0.4419 0.3760 0.5911\n",
      "Epoch: 321, edge loss: 0.9908, node loss: 0.4745 0.4774 0.4035 0.5247\n",
      "Epoch: 322, edge loss: 0.9875, node loss: 0.5212 0.5034 0.3871 0.5829\n",
      "Epoch: 323, edge loss: 0.9862, node loss: 0.4612 0.4891 0.3753 0.5376\n",
      "Epoch: 324, edge loss: 0.9899, node loss: 0.4696 0.4960 0.4243 0.5374\n",
      "Epoch: 325, edge loss: 0.9920, node loss: 0.5072 0.4700 0.4222 0.5467\n",
      "Epoch: 326, edge loss: 0.9881, node loss: 0.4844 0.4820 0.3840 0.5552\n",
      "Epoch: 327, edge loss: 0.9883, node loss: 0.4543 0.4543 0.3926 0.5680\n",
      "Epoch: 328, edge loss: 0.9872, node loss: 0.4546 0.4416 0.3786 0.5104\n",
      "Epoch: 329, edge loss: 0.9919, node loss: 0.5089 0.5696 0.4660 0.5761\n",
      "Epoch: 330, edge loss: 0.9914, node loss: 0.4993 0.4480 0.3831 0.5336\n",
      "Epoch: 331, edge loss: 0.9896, node loss: 0.5209 0.4551 0.4146 0.5293\n",
      "Epoch: 332, edge loss: 0.9888, node loss: 0.4872 0.5304 0.3953 0.5696\n",
      "Epoch: 333, edge loss: 0.9893, node loss: 0.4335 0.4657 0.4256 0.5392\n",
      "Epoch: 334, edge loss: 0.9891, node loss: 0.4532 0.4294 0.4441 0.5482\n",
      "Epoch: 335, edge loss: 0.9896, node loss: 0.5133 0.4343 0.4169 0.5792\n",
      "Epoch: 336, edge loss: 0.9873, node loss: 0.4715 0.4752 0.3674 0.5246\n",
      "Epoch: 337, edge loss: 0.9883, node loss: 0.4954 0.5009 0.3840 0.5767\n",
      "Epoch: 338, edge loss: 0.9903, node loss: 0.4837 0.4510 0.3815 0.5189\n",
      "Epoch: 339, edge loss: 0.9860, node loss: 0.4642 0.4323 0.4220 0.5849\n",
      "Epoch: 340, edge loss: 0.9863, node loss: 0.4488 0.4562 0.3920 0.5347\n",
      "Epoch: 341, edge loss: 0.9893, node loss: 0.4492 0.4593 0.3859 0.5448\n",
      "Epoch: 342, edge loss: 0.9880, node loss: 0.5027 0.4556 0.3758 0.5487\n",
      "Epoch: 343, edge loss: 0.9880, node loss: 0.4647 0.4848 0.4040 0.6105\n",
      "Epoch: 344, edge loss: 0.9905, node loss: 0.4302 0.4645 0.4062 0.5407\n",
      "Epoch: 345, edge loss: 0.9872, node loss: 0.4774 0.4838 0.4307 0.5385\n",
      "Epoch: 346, edge loss: 0.9901, node loss: 0.4582 0.4247 0.3507 0.5373\n",
      "Epoch: 347, edge loss: 0.9923, node loss: 0.4268 0.4840 0.4338 0.5244\n",
      "Epoch: 348, edge loss: 0.9899, node loss: 0.5007 0.4505 0.3433 0.5505\n",
      "Epoch: 349, edge loss: 0.9858, node loss: 0.4540 0.4161 0.3577 0.5371\n",
      "Epoch: 350, edge loss: 0.9887, node loss: 0.4851 0.4328 0.4211 0.5285\n",
      "Epoch: 351, edge loss: 0.9908, node loss: 0.4277 0.4766 0.3986 0.5370\n",
      "Epoch: 352, edge loss: 0.9862, node loss: 0.4895 0.4636 0.4003 0.5080\n",
      "Epoch: 353, edge loss: 0.9868, node loss: 0.4889 0.5100 0.3748 0.5164\n",
      "Epoch: 354, edge loss: 0.9867, node loss: 0.4739 0.4392 0.3395 0.5139\n",
      "Epoch: 355, edge loss: 0.9868, node loss: 0.4917 0.4443 0.3669 0.5299\n",
      "Epoch: 356, edge loss: 0.9882, node loss: 0.4391 0.4342 0.4569 0.5638\n",
      "Epoch: 357, edge loss: 0.9877, node loss: 0.4059 0.4536 0.4531 0.5206\n",
      "Epoch: 358, edge loss: 0.9903, node loss: 0.4137 0.4415 0.4360 0.5067\n",
      "Epoch: 359, edge loss: 0.9916, node loss: 0.4878 0.4627 0.3587 0.5295\n",
      "Epoch: 360, edge loss: 0.9856, node loss: 0.4774 0.4379 0.3993 0.4878\n",
      "Epoch: 361, edge loss: 0.9901, node loss: 0.4447 0.4404 0.3844 0.4870\n",
      "Epoch: 362, edge loss: 0.9867, node loss: 0.5149 0.4427 0.3660 0.6058\n",
      "Epoch: 363, edge loss: 0.9865, node loss: 0.4498 0.4326 0.3811 0.4920\n",
      "Epoch: 364, edge loss: 0.9873, node loss: 0.4335 0.4410 0.4037 0.5273\n",
      "Epoch: 365, edge loss: 0.9892, node loss: 0.4213 0.4836 0.3771 0.4859\n",
      "Epoch: 366, edge loss: 0.9867, node loss: 0.4721 0.4206 0.4244 0.5326\n",
      "Epoch: 367, edge loss: 0.9864, node loss: 0.4479 0.4450 0.4071 0.5204\n",
      "Epoch: 368, edge loss: 0.9873, node loss: 0.4374 0.5413 0.3843 0.4994\n",
      "Epoch: 369, edge loss: 0.9879, node loss: 0.5168 0.4117 0.4149 0.4962\n",
      "Epoch: 370, edge loss: 0.9862, node loss: 0.4365 0.4055 0.3821 0.5663\n",
      "Epoch: 371, edge loss: 0.9885, node loss: 0.4619 0.4396 0.4233 0.5489\n",
      "Epoch: 372, edge loss: 0.9921, node loss: 0.5166 0.4186 0.3526 0.4932\n",
      "Epoch: 373, edge loss: 0.9897, node loss: 0.4600 0.4723 0.4063 0.5417\n",
      "Epoch: 374, edge loss: 0.9901, node loss: 0.4128 0.3884 0.3655 0.4720\n",
      "Epoch: 375, edge loss: 0.9861, node loss: 0.5323 0.4607 0.3966 0.5102\n",
      "Epoch: 376, edge loss: 0.9871, node loss: 0.4290 0.4222 0.3975 0.5243\n",
      "Epoch: 377, edge loss: 0.9923, node loss: 0.4353 0.4425 0.3515 0.5035\n",
      "Epoch: 378, edge loss: 0.9880, node loss: 0.4767 0.5308 0.3537 0.5132\n",
      "Epoch: 379, edge loss: 0.9895, node loss: 0.4317 0.4007 0.3808 0.4724\n",
      "Epoch: 380, edge loss: 0.9894, node loss: 0.4328 0.4368 0.4054 0.5003\n",
      "Epoch: 381, edge loss: 0.9937, node loss: 0.4116 0.4398 0.3784 0.5047\n",
      "Epoch: 382, edge loss: 0.9885, node loss: 0.4288 0.4436 0.4175 0.5028\n",
      "Epoch: 383, edge loss: 0.9898, node loss: 0.4497 0.4223 0.3686 0.4836\n",
      "Epoch: 384, edge loss: 0.9892, node loss: 0.5152 0.4665 0.3488 0.4932\n",
      "Epoch: 385, edge loss: 0.9896, node loss: 0.4294 0.4399 0.4113 0.5109\n",
      "Epoch: 386, edge loss: 0.9859, node loss: 0.4052 0.4243 0.3944 0.5710\n",
      "Epoch: 387, edge loss: 0.9893, node loss: 0.4682 0.4396 0.3584 0.4887\n",
      "Epoch: 388, edge loss: 0.9889, node loss: 0.4622 0.5009 0.3562 0.4558\n",
      "Epoch: 389, edge loss: 0.9886, node loss: 0.4447 0.4148 0.4108 0.6122\n",
      "Epoch: 390, edge loss: 0.9902, node loss: 0.4067 0.3959 0.3837 0.5021\n",
      "Epoch: 391, edge loss: 0.9884, node loss: 0.4835 0.4812 0.3755 0.4686\n",
      "Epoch: 392, edge loss: 0.9904, node loss: 0.4342 0.4096 0.3956 0.5083\n",
      "Epoch: 393, edge loss: 0.9905, node loss: 0.4464 0.3989 0.4113 0.5146\n",
      "Epoch: 394, edge loss: 0.9893, node loss: 0.4207 0.4397 0.3614 0.4905\n",
      "Epoch: 395, edge loss: 0.9916, node loss: 0.4609 0.4210 0.3354 0.5112\n",
      "Epoch: 396, edge loss: 0.9924, node loss: 0.4538 0.4949 0.3339 0.4692\n",
      "Epoch: 397, edge loss: 0.9921, node loss: 0.4351 0.4385 0.3453 0.5196\n",
      "Epoch: 398, edge loss: 0.9899, node loss: 0.4227 0.4111 0.3965 0.4726\n",
      "Epoch: 399, edge loss: 0.9919, node loss: 0.4408 0.4068 0.4420 0.5109\n",
      "Epoch: 400, edge loss: 0.9897, node loss: 0.4753 0.4612 0.3471 0.4979\n",
      "Epoch: 401, edge loss: 0.9890, node loss: 0.4413 0.4478 0.3543 0.5263\n",
      "Epoch: 402, edge loss: 0.9898, node loss: 0.4265 0.4112 0.3208 0.5701\n",
      "Epoch: 403, edge loss: 0.9888, node loss: 0.4029 0.4419 0.3711 0.5180\n",
      "Epoch: 404, edge loss: 0.9871, node loss: 0.4308 0.4308 0.3861 0.5054\n",
      "Epoch: 405, edge loss: 0.9870, node loss: 0.3916 0.3959 0.4363 0.5711\n",
      "Epoch: 406, edge loss: 0.9883, node loss: 0.4587 0.4493 0.3773 0.4737\n",
      "Epoch: 407, edge loss: 0.9899, node loss: 0.4525 0.4011 0.3529 0.5006\n",
      "Epoch: 408, edge loss: 0.9869, node loss: 0.4091 0.3926 0.3788 0.5233\n",
      "Epoch: 409, edge loss: 0.9890, node loss: 0.5140 0.4201 0.3587 0.4627\n",
      "Epoch: 410, edge loss: 0.9897, node loss: 0.4528 0.4262 0.3842 0.4424\n",
      "Epoch: 411, edge loss: 0.9904, node loss: 0.4044 0.4379 0.3498 0.4839\n",
      "Epoch: 412, edge loss: 0.9886, node loss: 0.4416 0.4594 0.4485 0.4652\n",
      "Epoch: 413, edge loss: 0.9879, node loss: 0.4390 0.4128 0.3403 0.5332\n",
      "Epoch: 414, edge loss: 0.9845, node loss: 0.4564 0.4076 0.3230 0.4856\n",
      "Epoch: 415, edge loss: 0.9911, node loss: 0.4798 0.3957 0.4331 0.5279\n",
      "Epoch: 416, edge loss: 0.9881, node loss: 0.3981 0.3879 0.3451 0.5034\n",
      "Epoch: 417, edge loss: 0.9858, node loss: 0.4296 0.4501 0.3631 0.4608\n",
      "Epoch: 418, edge loss: 0.9862, node loss: 0.4646 0.4170 0.3215 0.4581\n",
      "Epoch: 419, edge loss: 0.9860, node loss: 0.4835 0.4087 0.3723 0.5156\n",
      "Epoch: 420, edge loss: 0.9842, node loss: 0.4125 0.4784 0.3314 0.5025\n",
      "Epoch: 421, edge loss: 0.9907, node loss: 0.3989 0.4874 0.3988 0.4557\n",
      "Epoch: 422, edge loss: 0.9835, node loss: 0.4049 0.4167 0.3522 0.5367\n",
      "Epoch: 423, edge loss: 0.9882, node loss: 0.4386 0.3964 0.3664 0.5139\n",
      "Epoch: 424, edge loss: 0.9882, node loss: 0.4348 0.4532 0.3798 0.4527\n",
      "Epoch: 425, edge loss: 0.9916, node loss: 0.3871 0.3850 0.3498 0.4793\n",
      "Epoch: 426, edge loss: 0.9883, node loss: 0.4495 0.4448 0.3877 0.4746\n",
      "Epoch: 427, edge loss: 0.9880, node loss: 0.4050 0.4197 0.3224 0.5265\n",
      "Epoch: 428, edge loss: 0.9892, node loss: 0.4060 0.4048 0.3595 0.4711\n",
      "Epoch: 429, edge loss: 0.9900, node loss: 0.3662 0.3771 0.4042 0.4649\n",
      "Epoch: 430, edge loss: 0.9903, node loss: 0.4817 0.4730 0.3820 0.4754\n",
      "Epoch: 431, edge loss: 0.9867, node loss: 0.4314 0.4151 0.3821 0.5021\n",
      "Epoch: 432, edge loss: 0.9864, node loss: 0.3975 0.3857 0.4188 0.4309\n",
      "Epoch: 433, edge loss: 0.9899, node loss: 0.3750 0.4016 0.3784 0.4705\n",
      "Epoch: 434, edge loss: 0.9876, node loss: 0.3940 0.3946 0.3593 0.5285\n",
      "Epoch: 435, edge loss: 0.9855, node loss: 0.4789 0.4150 0.3261 0.4340\n",
      "Epoch: 436, edge loss: 0.9859, node loss: 0.4171 0.4142 0.3359 0.4805\n",
      "Epoch: 437, edge loss: 0.9884, node loss: 0.3781 0.3974 0.3652 0.4830\n",
      "Epoch: 438, edge loss: 0.9883, node loss: 0.3835 0.3901 0.3446 0.5364\n",
      "Epoch: 439, edge loss: 0.9907, node loss: 0.4283 0.5047 0.3619 0.5232\n",
      "Epoch: 440, edge loss: 0.9926, node loss: 0.3984 0.4187 0.3188 0.4914\n",
      "Epoch: 441, edge loss: 0.9895, node loss: 0.4397 0.4659 0.3462 0.4888\n",
      "Epoch: 442, edge loss: 0.9862, node loss: 0.3871 0.4258 0.3327 0.4541\n",
      "Epoch: 443, edge loss: 0.9891, node loss: 0.4139 0.4095 0.3484 0.4340\n",
      "Epoch: 444, edge loss: 0.9859, node loss: 0.4130 0.3863 0.3719 0.4227\n",
      "Epoch: 445, edge loss: 0.9919, node loss: 0.5030 0.4015 0.3394 0.4713\n",
      "Epoch: 446, edge loss: 0.9900, node loss: 0.4156 0.3953 0.3498 0.5697\n",
      "Epoch: 447, edge loss: 0.9915, node loss: 0.4228 0.4135 0.3643 0.4535\n",
      "Epoch: 448, edge loss: 0.9862, node loss: 0.4340 0.3950 0.3494 0.5136\n",
      "Epoch: 449, edge loss: 0.9894, node loss: 0.4528 0.4043 0.3334 0.4213\n",
      "Epoch: 450, edge loss: 0.9899, node loss: 0.4230 0.3958 0.3817 0.4995\n",
      "Epoch: 451, edge loss: 0.9866, node loss: 0.4110 0.3665 0.3533 0.5025\n",
      "Epoch: 452, edge loss: 0.9864, node loss: 0.4319 0.3724 0.3399 0.5008\n",
      "Epoch: 453, edge loss: 0.9874, node loss: 0.3981 0.4590 0.3312 0.4607\n",
      "Epoch: 454, edge loss: 0.9875, node loss: 0.4070 0.4437 0.3702 0.4504\n",
      "Epoch: 455, edge loss: 0.9910, node loss: 0.4062 0.3753 0.3397 0.4838\n",
      "Epoch: 456, edge loss: 0.9873, node loss: 0.4040 0.3686 0.3786 0.4624\n",
      "Epoch: 457, edge loss: 0.9878, node loss: 0.4067 0.4006 0.3757 0.4290\n",
      "Epoch: 458, edge loss: 0.9878, node loss: 0.4435 0.4539 0.3417 0.4507\n",
      "Epoch: 459, edge loss: 0.9886, node loss: 0.4180 0.4384 0.3410 0.4579\n",
      "Epoch: 460, edge loss: 0.9920, node loss: 0.4201 0.4020 0.3183 0.4838\n",
      "Epoch: 461, edge loss: 0.9899, node loss: 0.4068 0.3829 0.3941 0.4175\n",
      "Epoch: 462, edge loss: 0.9887, node loss: 0.4290 0.4055 0.3653 0.4068\n",
      "Epoch: 463, edge loss: 0.9912, node loss: 0.4013 0.4149 0.3714 0.4495\n",
      "Epoch: 464, edge loss: 0.9898, node loss: 0.4087 0.3869 0.3501 0.5121\n",
      "Epoch: 465, edge loss: 0.9872, node loss: 0.4394 0.4554 0.3211 0.4985\n",
      "Epoch: 466, edge loss: 0.9868, node loss: 0.4454 0.3844 0.3102 0.4790\n",
      "Epoch: 467, edge loss: 0.9886, node loss: 0.4040 0.3831 0.4312 0.5124\n",
      "Epoch: 468, edge loss: 0.9863, node loss: 0.4144 0.4167 0.3275 0.3968\n",
      "Epoch: 469, edge loss: 0.9858, node loss: 0.3967 0.3555 0.3487 0.4869\n",
      "Epoch: 470, edge loss: 0.9926, node loss: 0.4859 0.3948 0.3284 0.5062\n",
      "Epoch: 471, edge loss: 0.9886, node loss: 0.3678 0.4244 0.3034 0.4532\n",
      "Epoch: 472, edge loss: 0.9884, node loss: 0.4047 0.4740 0.3280 0.4970\n",
      "Epoch: 473, edge loss: 0.9866, node loss: 0.3654 0.3759 0.3646 0.4998\n",
      "Epoch: 474, edge loss: 0.9894, node loss: 0.3623 0.4162 0.3380 0.4354\n",
      "Epoch: 475, edge loss: 0.9894, node loss: 0.4018 0.4352 0.3777 0.4554\n",
      "Epoch: 476, edge loss: 0.9863, node loss: 0.4414 0.4154 0.3713 0.4719\n",
      "Epoch: 477, edge loss: 0.9937, node loss: 0.3776 0.4381 0.3156 0.4257\n",
      "Epoch: 478, edge loss: 0.9888, node loss: 0.4040 0.3965 0.3327 0.4340\n",
      "Epoch: 479, edge loss: 0.9880, node loss: 0.4402 0.3882 0.3302 0.4596\n",
      "Epoch: 480, edge loss: 0.9875, node loss: 0.4086 0.4079 0.3253 0.4434\n",
      "Epoch: 481, edge loss: 0.9882, node loss: 0.3939 0.4381 0.3347 0.4349\n",
      "Epoch: 482, edge loss: 0.9902, node loss: 0.4040 0.3648 0.3581 0.4336\n",
      "Epoch: 483, edge loss: 0.9885, node loss: 0.4142 0.3703 0.3580 0.5416\n",
      "Epoch: 484, edge loss: 0.9925, node loss: 0.3755 0.3785 0.3560 0.4879\n",
      "Epoch: 485, edge loss: 0.9876, node loss: 0.3804 0.3810 0.4130 0.4371\n",
      "Epoch: 486, edge loss: 0.9907, node loss: 0.4039 0.4679 0.3639 0.4153\n",
      "Epoch: 487, edge loss: 0.9887, node loss: 0.3739 0.3621 0.3071 0.4119\n",
      "Epoch: 488, edge loss: 0.9876, node loss: 0.4129 0.3937 0.3273 0.5301\n",
      "Epoch: 489, edge loss: 0.9855, node loss: 0.3971 0.3752 0.3003 0.4297\n",
      "Epoch: 490, edge loss: 0.9886, node loss: 0.3822 0.4420 0.3720 0.4171\n",
      "Epoch: 491, edge loss: 0.9900, node loss: 0.4395 0.4209 0.3559 0.4161\n",
      "Epoch: 492, edge loss: 0.9886, node loss: 0.3900 0.3673 0.3289 0.5031\n",
      "Epoch: 493, edge loss: 0.9874, node loss: 0.3927 0.3694 0.4118 0.4543\n",
      "Epoch: 494, edge loss: 0.9879, node loss: 0.3763 0.3699 0.3587 0.4572\n",
      "Epoch: 495, edge loss: 0.9869, node loss: 0.4260 0.3747 0.3429 0.5103\n",
      "Epoch: 496, edge loss: 0.9854, node loss: 0.4155 0.3885 0.3040 0.4576\n",
      "Epoch: 497, edge loss: 0.9873, node loss: 0.4070 0.3987 0.3124 0.4559\n",
      "Epoch: 498, edge loss: 0.9869, node loss: 0.3790 0.4083 0.3074 0.4556\n",
      "Epoch: 499, edge loss: 0.9880, node loss: 0.3705 0.4239 0.3503 0.5072\n",
      "Epoch: 500, edge loss: 0.9893, node loss: 0.3853 0.3809 0.3465 0.4488\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize early stopping\n",
    "node_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=node_model_name)\n",
    "edge_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=edge_model_name)\n",
    "\n",
    "# Training loop\n",
    "edge_train_losses = []\n",
    "node_train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    edge_loss_cum = 0\n",
    "    node_loss_cum = np.zeros(n_node_features-1, dtype=float)\n",
    "    node_loss2_cum = 0\n",
    "    for batch_0 in train_loader:\n",
    "    for graph_0 in train_dataset:\n",
    "        #print()\n",
    "        # Clone batch of graphs\n",
    "        g_batch_0 = batch_0.clone()\n",
    "        \n",
    "        # Move batch data to GPU\n",
    "        g_batch_0 = g_batch_0.to(device)\n",
    "\n",
    "        # Read number of graphs in batch\n",
    "        batch_size_0 = g_batch_0.num_graphs\n",
    "\n",
    "        # Save graph-level embeddings\n",
    "        embedding_batch_0 = []\n",
    "        for idx in range(batch_size_0):\n",
    "            embedding_batch_0.append(g_batch_0[idx].y.detach().to(device))\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        t_steps = np.arange(1, n_t_steps+1)\n",
    "        for t_step in t_steps:\n",
    "            # Read time step, which is added to node-level graph embeddings\n",
    "            t_step_std = torch.tensor([t_step / n_t_steps - 0.5], dtype=torch.float).to(device)  # Standard normalization\n",
    "        \n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            g_batch_t = []\n",
    "            e_batch_t = []\n",
    "            for idx in range(batch_size_0):\n",
    "                # Perform a diffusion step at time step t_step for each graph within the batch\n",
    "                graph_t, epsilon_t = diffusion_step(g_batch_0[idx], t_step, n_t_steps, alpha_decay)\n",
    "\n",
    "                # Append noisy graphs and noises\n",
    "                g_batch_t.append(graph_t)\n",
    "                e_batch_t.append(epsilon_t)\n",
    "\n",
    "                # Update diffused graph as next one\n",
    "                g_batch_0[idx] = graph_t.clone()\n",
    "            \n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add embeddings to noisy graphs (t_step information and graph-level embeddings)\n",
    "            for idx in range(batch_size_0):\n",
    "                # Add graph-level embedding to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       embedding_batch_0[idx])  # To match graph.y shape\n",
    "\n",
    "                # Add t_step information to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       t_step_std)  # To match graph.y shape, which is 1D\n",
    "\n",
    "            # Generate batch objects\n",
    "            g_batch_t = Batch.from_data_list(g_batch_t)\n",
    "            e_batch_t = Batch.from_data_list(e_batch_t)\n",
    "            \n",
    "            # Move data to device\n",
    "            g_batch_t = g_batch_t.to(device)\n",
    "            e_batch_t = e_batch_t.to(device)\n",
    "            \n",
    "            # Predict batch noise at given time step\n",
    "            pred_epsilon_t = predict_noise(g_batch_t, node_model, edge_model)\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the losses for node features and edge attributes\n",
    "            #print(e_batch_t.size(), pred_epsilon_t.size())\n",
    "            node_losses, edge_loss = get_graph_losses(e_batch_t, pred_epsilon_t, batch_size_0)\n",
    "            \n",
    "            # Combine losses for each attribute tensors\n",
    "            node_loss = torch.stack(node_losses).sum()\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            if not node_early_stopping.early_stop:\n",
    "                node_loss.backward(retain_graph=True)\n",
    "                node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            if not edge_early_stopping.early_stop:\n",
    "                edge_loss.backward(retain_graph=True)\n",
    "                edge_optimizer.step()\n",
    "            \n",
    "            # Get items\n",
    "            node_loss_cum += np.array([node_loss.item() for node_loss in node_losses])\n",
    "            edge_loss_cum += edge_loss.item()\n",
    "\n",
    "            del g_batch_t, e_batch_t, pred_epsilon_t, node_loss, edge_loss  # Free up CUDA memory\n",
    "\n",
    "    # Compute the average train loss over n_t_steps\n",
    "    node_loss_cum /= (n_t_steps * len(train_dataset))\n",
    "    edge_loss_cum /= (n_t_steps * len(train_dataset))\n",
    "    \n",
    "    # Append average losses\n",
    "    node_train_losses.append(node_loss_cum)\n",
    "    edge_train_losses.append(edge_loss_cum)\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    node_early_stopping(node_loss_cum.sum(), node_model)\n",
    "    edge_early_stopping(edge_loss_cum,       edge_model)\n",
    "\n",
    "    if node_early_stopping.early_stop and edge_early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "    \n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Epoch: {epoch+1}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:22:22.439379Z",
     "start_time": "2024-04-04T23:51:43.008543Z"
    }
   },
   "id": "da25994c77a75d65",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([6.74429776, 6.74430185, 6.74032131, 6.74216443, 6.74202219,\n       6.74303216, 6.74079545, 6.74345538, 6.73967613, 6.74263092,\n       6.73903452, 6.73871079, 6.74395957, 6.7439481 , 6.73459378,\n       6.73396068, 6.73653012, 6.74248686, 6.74460308, 6.73887691,\n       6.74218325, 6.7413883 , 6.73852902, 6.73594219, 6.73292022,\n       6.73981561, 6.73840433, 6.73948647, 6.74015891, 6.73659438,\n       6.73504234, 6.73576148, 6.740782  , 6.73955946, 6.73725706,\n       6.74079458, 6.73974156, 6.73844525, 6.73978543, 6.73372476,\n       6.73779114, 6.73594474, 6.73331547, 6.73529057, 6.73658778,\n       6.73807648, 6.73387893, 6.73988135, 6.73696578, 6.73722763,\n       6.73222185, 6.73384054, 6.73303777, 6.73656361, 6.73978083,\n       6.736721  , 6.73244132, 6.73703921, 6.73644823, 6.73508793,\n       6.73993975, 6.73905373, 6.73406874, 6.73695335, 6.73497818,\n       6.73532332, 6.73932661, 6.73984232, 6.73181858, 6.73704821,\n       6.73178954, 6.73555867, 6.73649045, 6.73124097, 6.72991036,\n       6.73286339, 6.73537968, 6.73482699, 6.73290282, 6.73353271,\n       6.72754416, 6.73546211, 6.73101203, 6.73241718, 6.72907101,\n       6.73435681, 6.72925686, 6.73282933, 6.73109674, 6.73584475,\n       6.73490185, 6.73218284, 6.73179343, 6.72710992, 6.73148502,\n       6.7368168 , 6.72965422, 6.73266403, 6.72833005, 6.72473282,\n       6.73289969, 6.72921667, 6.72598658, 6.73071488, 6.73161203,\n       6.72896526, 6.72746325, 6.72818105, 6.72952654, 6.73192809,\n       6.72920482, 6.72967361, 6.72918139, 6.72973753, 6.73195609,\n       6.72891138, 6.7288547 , 6.72879543, 6.73085331, 6.73209826,\n       6.72062071, 6.73142037, 6.72917921, 6.72805413, 6.72865174,\n       6.72594   , 6.72736456, 6.72306003, 6.72733917, 6.72738759,\n       6.72916719, 6.73194131, 6.72365171, 6.72918378, 6.72708697,\n       6.72844235, 6.72807189, 6.72819579, 6.72458058, 6.73194641,\n       6.72215555, 6.72723319, 6.73171027, 6.72447033, 6.7231364 ,\n       6.7295784 , 6.72876549, 6.72644852, 6.72821067, 6.73005633,\n       6.72375948, 6.72901753, 6.72351841, 6.72505506, 6.73108612,\n       6.72488464, 6.72837873, 6.72637573, 6.72132122, 6.72343569,\n       6.72252563, 6.72922384, 6.72215584, 6.72485229, 6.72589797,\n       6.72856209, 6.72723561, 6.72245842, 6.72567036, 6.72570526,\n       6.72861844, 6.72763695, 6.72003287, 6.72508713, 6.72271797,\n       6.72886794, 6.72192151, 6.72718892, 6.73082094, 6.72285522,\n       6.72754404, 6.72547882, 6.72884888, 6.72607973, 6.73318636,\n       6.7232563 , 6.72474791, 6.72157108, 6.72948317, 6.72374525,\n       6.72379599, 6.73134777, 6.72340779, 6.72089269, 6.72668658,\n       6.72950456, 6.72689719, 6.72340134, 6.72723047, 6.72604208,\n       6.72528409, 6.7246724 , 6.72585174, 6.72495347, 6.72907588,\n       6.72644478, 6.72799241, 6.73162847, 6.72757495, 6.7270951 ,\n       6.7256594 , 6.72592038, 6.71573183, 6.72675539, 6.72148134,\n       6.72724083, 6.72508054, 6.72662376, 6.72150537, 6.72859084,\n       6.72260239, 6.7266989 , 6.7249038 , 6.72422075, 6.72383635,\n       6.72602103, 6.72360991, 6.72376022, 6.72458362, 6.71899909,\n       6.72839417, 6.72367863, 6.73039842, 6.7308904 , 6.72656808,\n       6.72713906, 6.72782914, 6.72846854, 6.71977439, 6.72452178,\n       6.72710692, 6.72528812, 6.7206572 , 6.72672846, 6.72587526,\n       6.72324022, 6.72742674, 6.72613091, 6.72822617, 6.72435879,\n       6.72425151, 6.72312381, 6.72433169, 6.73032978, 6.7315887 ,\n       6.72391127, 6.71554523, 6.72730554, 6.72516685, 6.73469857,\n       6.72548869, 6.72347917, 6.72369656, 6.72533512, 6.719502  ,\n       6.72826041, 6.72482978, 6.72620186, 6.72196225, 6.72372321,\n       6.72806229, 6.72892581, 6.72193696, 6.72306018, 6.71793044,\n       6.72411234, 6.72103297, 6.72377046, 6.72341575, 6.72755966,\n       6.72916337, 6.7259512 , 6.72902243, 6.72574357, 6.72236348,\n       6.72571314, 6.72376139, 6.72597174, 6.72059049, 6.72784114,\n       6.72713979, 6.72356489, 6.72213572, 6.72802199, 6.72533553,\n       6.72686667, 6.72265667, 6.72562402, 6.72909967, 6.72592195,\n       6.72624146, 6.7254797 , 6.72814963, 6.72784501, 6.72861229,\n       6.72597737, 6.7284543 , 6.72415345, 6.72406824, 6.72494377,\n       6.72360647, 6.7235244 , 6.73156072, 6.72663717, 6.72052436,\n       6.72727189, 6.72497863, 6.71960906, 6.72659252, 6.72444803,\n       6.72782423, 6.72265677, 6.72057709, 6.72646544, 6.72969498,\n       6.7235633 , 6.72386187, 6.72222496, 6.72955701, 6.72877227,\n       6.72598153, 6.72469255, 6.72554364, 6.72511513, 6.72588912,\n       6.72237882, 6.72393425, 6.72697683, 6.72031943, 6.72081046,\n       6.72550641, 6.72344362, 6.72343267, 6.72730957, 6.7221099 ,\n       6.72674522, 6.73018931, 6.72638191, 6.71990084, 6.72458717,\n       6.72775896, 6.7205156 , 6.72147788, 6.72137579, 6.7215388 ,\n       6.72365688, 6.72300146, 6.72704194, 6.72914927, 6.71961014,\n       6.72678841, 6.72133507, 6.72099673, 6.7222918 , 6.72524843,\n       6.72131481, 6.72086395, 6.72238853, 6.72318865, 6.72060788,\n       6.72428428, 6.72981758, 6.72612148, 6.72670923, 6.72046492,\n       6.72200441, 6.73013927, 6.72339225, 6.72573098, 6.72567933,\n       6.73243515, 6.72424211, 6.72621666, 6.72526574, 6.72595384,\n       6.72016797, 6.72548857, 6.7248346 , 6.7242851 , 6.72685681,\n       6.72403347, 6.72718871, 6.72737182, 6.72540464, 6.72908537,\n       6.73028762, 6.72994679, 6.72646917, 6.72956356, 6.72604769,\n       6.72504326, 6.7262127 , 6.7247223 , 6.72207481, 6.72179214,\n       6.72387854, 6.72646785, 6.72166903, 6.72504665, 6.72603888,\n       6.72726206, 6.72443559, 6.72318459, 6.71796925, 6.72831244,\n       6.7235313 , 6.71994992, 6.72057685, 6.72033189, 6.71750036,\n       6.72768849, 6.71632116, 6.72376697, 6.72370216, 6.7290946 ,\n       6.72390013, 6.72343455, 6.72532223, 6.72659976, 6.72700267,\n       6.72142718, 6.72087631, 6.72640698, 6.72279921, 6.71940814,\n       6.72004351, 6.72399902, 6.72393304, 6.72765811, 6.73065303,\n       6.72574878, 6.72061593, 6.72510255, 6.72006216, 6.72958599,\n       6.7266444 , 6.72894359, 6.72056055, 6.72562992, 6.72648026,\n       6.72113282, 6.72093223, 6.72245525, 6.72257619, 6.72809428,\n       6.72238892, 6.72305948, 6.72304884, 6.72428733, 6.72966104,\n       6.72634279, 6.72450525, 6.72853272, 6.7262223 , 6.72216486,\n       6.72154869, 6.72442881, 6.72075002, 6.71993529, 6.73062559,\n       6.7242854 , 6.72406988, 6.72117004, 6.72564293, 6.72562669,\n       6.72072048, 6.73234664, 6.72466862, 6.72341153, 6.72269297,\n       6.72373231, 6.72684456, 6.72418725, 6.73047489, 6.72280734,\n       6.72764387, 6.72444921, 6.72284039, 6.71949108, 6.7243138 ,\n       6.72652598, 6.72430139, 6.72252707, 6.72328203, 6.72166812,\n       6.71928615, 6.72236929, 6.7216815 , 6.72347556, 6.72553016])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaled_edge_train_losses = np.sqrt(edge_train_losses) * dataset_parameters['edge_std'].numpy() + dataset_parameters['edge_mean'].numpy()\n",
    "rescaled_edge_train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:34:02.387852Z",
     "start_time": "2024-04-05T08:34:02.283445Z"
    }
   },
   "id": "599dff947a362ee",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[13.12549471,  6.59604866,  2.77226765, 13.67682476],\n       [12.37526412,  6.22498231,  2.75929114, 13.6526749 ],\n       [12.30913709,  6.20087026,  2.75387054, 13.66417369],\n       ...,\n       [10.55984783,  5.43819106,  2.63954507, 13.34479548],\n       [10.52902058,  5.46300173,  2.64896184, 13.37841676],\n       [10.5824848 ,  5.39335124,  2.64815646, 13.34017282]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_train_losses = np.array(node_train_losses)\n",
    "rescaled_node_loss_cum = np.sqrt(node_train_losses) * dataset_parameters['feat_std'].numpy() + dataset_parameters['feat_mean'].numpy()\n",
    "rescaled_node_loss_cum"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:34:03.187706Z",
     "start_time": "2024-04-05T08:34:02.919869Z"
    }
   },
   "id": "7aa5e76e720d0a5f",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(np.log(edge_train_losses), label='Edge')\n",
    "for i in range(n_node_features-1):\n",
    "    plt.plot(np.log(np.array(node_train_losses)[:, i]), label=f'Node {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ae2de47147f076c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.show()\n",
    "for i in range(n_node_features - 1):\n",
    "    plt.plot(np.log(np.array(node_train_losses)[:100, i]), label=f'Node {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98c6c582d9ab2304"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array(node_train_losses)[:, 0]), label=f'Atomic mass')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 1]), label=f'Charge')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 2]), label=f'Electronegativity')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 3]), label=f'Ionization energy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Losses.eps', dpi=50, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15246a097588b4a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeebf15173f65e29"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, edge loss: 0.0179, node loss: 0.0151 0.0159 0.0139 0.0182\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "edge_test_losses = 0\n",
    "node_test_losses = np.zeros(n_node_features-1, dtype=float)\n",
    "idx = 0\n",
    "for batch_0 in test_loader:\n",
    "    # Move batch data to GPU\n",
    "    batch_0 = batch_0.to(device)\n",
    "    \n",
    "    # Read number of graphs in batch\n",
    "    batch_size = batch_0.num_graphs\n",
    "    \n",
    "    # Diffuse batch\n",
    "    g_batch_t = diffuse(batch_0, n_t_steps, s=alpha_decay)\n",
    "    \n",
    "    # Denoise batch\n",
    "    g_batch_0 = denoise(g_batch_t, n_t_steps, node_model, edge_model, n_graph_features,\n",
    "                        s=alpha_decay, sigma=sigma)\n",
    "    \n",
    "    # Calculate the loss for node features and edge attributes\n",
    "    node_losses, edge_loss = get_graph_losses(batch_0, g_batch_0, batch_size)\n",
    "    \n",
    "    # Get items\n",
    "    edge_loss_cum = edge_loss.item()\n",
    "    node_loss_cum = np.array([node_loss.item() for node_loss in node_losses])\n",
    "    \n",
    "    # Append average losses\n",
    "    edge_test_losses += edge_loss_cum\n",
    "    node_test_losses += node_loss_cum\n",
    "    \n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Batch: {idx}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')\n",
    "    idx += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:06:45.645559Z",
     "start_time": "2024-04-05T08:06:43.545748Z"
    }
   },
   "id": "2fe8468fc69b364a",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "node_test_losses /= len(test_loader)\n",
    "edge_test_losses /= len(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:06:45.689307Z",
     "start_time": "2024-04-05T08:06:45.658787Z"
    }
   },
   "id": "4ea5b88a67ec61d5",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(0.017924724146723747, array([0.0151244 , 0.0159176 , 0.01389554, 0.0182348 ]))"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_test_losses, node_test_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:08:24.586923Z",
     "start_time": "2024-04-05T08:08:24.502253Z"
    }
   },
   "id": "d196836e36b58174",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dde0d0d42345dcb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create and save as a dictionary\n",
    "model_performance = {\n",
    "    'edge_train_losses': edge_train_losses,\n",
    "    'node_train_losses': np.array(node_train_losses).tolist(),\n",
    "    'edge_test_losses':  edge_test_losses,\n",
    "    'node_test_losses':  node_test_losses.tolist()\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_performance.json', 'w') as json_file:\n",
    "    json.dump(model_performance, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:08:03.254749Z",
     "start_time": "2024-04-05T08:08:03.110280Z"
    }
   },
   "id": "a348bc7a734b98db",
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
