{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.558838188Z",
     "start_time": "2024-04-05T11:00:14.617265776Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy             as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torch_geometric.data   import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph, predict_noise, diffuse, denoise, EarlyStopping\n",
    "from libraries.dataset      import standardize_dataset, get_datasets\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.605502551Z",
     "start_time": "2024-04-05T11:00:15.604985966Z"
    }
   },
   "id": "74450972cebeaa56",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ad446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.605803239Z",
     "start_time": "2024-04-05T11:00:15.605086576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs\n",
    "# The models is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property\n",
    "# Although denoising includes noise, I think it is better not to add it when training"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'models/GM_PT_EPA-voronoi/GM_v4'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'GM_PT_EPA-voronoi'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "# Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "i = 0\n",
    "while True:\n",
    "    target_folder = f'{general_folder}/GM_v{i}'\n",
    "    if not os.path.exists(target_folder):\n",
    "        # Copy all data\n",
    "        os.system(f'cp -r {data_folder} {target_folder}')\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "target_folder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.606098917Z",
     "start_time": "2024-04-05T11:00:15.605114759Z"
    }
   },
   "id": "130cc61599976691",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 1000\n",
    "batch_size    = 256\n",
    "learning_rate = 0.0001\n",
    "patience      = 20\n",
    "delta         = 0.2\n",
    "check_labels  = True  # Whether to train-test split attending to labels or not\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 100\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = 0  # Zero for training purposes\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':        data_folder,\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'patience':           patience,\n",
    "    'delta':              delta,\n",
    "    'check_labels':       check_labels,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'sigma':              sigma,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.606366963Z",
     "start_time": "2024-04-05T11:00:15.605144305Z"
    }
   },
   "id": "1be066fddd93b461",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load of graph database for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b69994b030f25e04"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset, already standardized."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb3859057f714368"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "if os.path.exists(dataset_name_std) and os.path.exists(dataset_parameters_name_std) and os.path.exists(labels_name):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset = torch.load(dataset_name_std)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Load the data from the JSON file\n",
    "    with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "        numpy_dict = json.load(json_file)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    dataset_parameters = {}\n",
    "    for key, value in numpy_dict.items():\n",
    "        try:\n",
    "            dataset_parameters[key] = torch.tensor(value)\n",
    "        except:\n",
    "            dataset_parameters[key] = value\n",
    "\n",
    "elif os.path.exists(dataset_name) and os.path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)\n",
    "\n",
    "else:\n",
    "    sys.exit('Error: the database is not available')\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.682226466Z",
     "start_time": "2024-04-05T11:00:15.605166637Z"
    }
   },
   "id": "c40d504f8b3b498c",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split in train, validation and test sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef017add32b19031"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   graphs: 1095\n",
      "Number of validation graphs: 138\n",
      "Number of testing    graphs: 136\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio  = 0.1\n",
    "\n",
    "# Check if data has been already split, else do it randomly\n",
    "path_to_train_labels = f'{target_folder}/train_labels.txt'\n",
    "path_to_val_labels   = f'{target_folder}/validation_labels.txt'\n",
    "path_to_test_labels  = f'{target_folder}/test_labels.txt'\n",
    "\n",
    "# Copy labels\n",
    "material_labels = labels.copy()\n",
    "\n",
    "if os.path.exists(path_to_train_labels) and os.path.exists(path_to_val_labels) and os.path.exists(path_to_test_labels):\n",
    "    # Read labels splitting (which are strings)\n",
    "    train_labels = np.genfromtxt(path_to_train_labels, dtype='str').tolist()\n",
    "    val_labels   = np.genfromtxt(path_to_val_labels,   dtype='str').tolist()\n",
    "    test_labels  = np.genfromtxt(path_to_test_labels,  dtype='str').tolist()\n",
    "else:\n",
    "    if check_labels:\n",
    "        # Splitting into train-test sets considering that Fvs from the same materials must be in the same dataset\n",
    "        material_labels = [label.split()[0] for label in material_labels]\n",
    "        \n",
    "        # Define unique labels\n",
    "        unique_labels = np.unique(material_labels)\n",
    "    else:\n",
    "        # Completely randomly splitting\n",
    "        # Copy material_labels\n",
    "        unique_labels = material_labels.copy()\n",
    "    \n",
    "    # Shuffle the list of unique labels\n",
    "    np.random.shuffle(unique_labels)\n",
    "\n",
    "    # Define the sizes of the train and test sets\n",
    "    # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "    train_size = int(train_ratio * len(unique_labels))\n",
    "    test_size  = int(test_ratio  * len(unique_labels))\n",
    "    \n",
    "    train_labels = unique_labels[:train_size]\n",
    "    val_labels   = unique_labels[train_size:-test_size]\n",
    "    test_labels  = unique_labels[-test_size:]\n",
    "\n",
    "    # Save this splitting for transfer-learning approaches\n",
    "    np.savetxt(path_to_train_labels, train_labels, fmt='%s')\n",
    "    np.savetxt(path_to_val_labels,   val_labels,   fmt='%s')\n",
    "    np.savetxt(path_to_test_labels,  test_labels,  fmt='%s')\n",
    "\n",
    "# Use the computed indexes to generate train and test sets\n",
    "# We iteratively check where labels equals a unique train/test labels and append the index to a list\n",
    "train_dataset = get_datasets(train_labels, material_labels, dataset)\n",
    "val_dataset   = get_datasets(val_labels,   material_labels, dataset)\n",
    "test_dataset  = get_datasets(test_labels,  material_labels, dataset)\n",
    "\n",
    "del dataset  # Free up CUDA memory\n",
    "\n",
    "print(f'Number of training   graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of testing    graphs: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.793684360Z",
     "start_time": "2024-04-05T11:00:15.684017789Z"
    }
   },
   "id": "cd51fcec527c7be6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for graph in train_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for graph in val_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for graph in test_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.803324370Z",
     "start_time": "2024-04-05T11:00:15.793146925Z"
    }
   },
   "id": "4d694649c5074fed",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define data loaders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7ae68deba3180c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(train_dataset[0].y)\n",
    "\n",
    "del train_dataset, val_dataset, test_dataset  # Free up CUDA memory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:15.849130443Z",
     "start_time": "2024-04-05T11:00:15.804737749Z"
    }
   },
   "id": "5a3f12ef8c461bda",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definition of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "837a9d6606b82bda"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(6, 256)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (conv3): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "\n",
    "# Moving models to device\n",
    "node_model = node_model.to(device)\n",
    "edge_model = edge_model.to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "    \n",
    "    # Evaluate model state\n",
    "    node_model.eval()\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:00:16.184759311Z",
     "start_time": "2024-04-05T11:00:15.849034782Z"
    }
   },
   "id": "82337610787fdd55",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "897d37b79cac32c2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, edge loss: 0.9995, node loss: 14842893.2049 29684267.3412 15167154.6060 92627993.6338\n",
      "Epoch: 2, edge loss: 1.0001, node loss: 139593310.1273 28303496.1737 68758301.5064 61220367.0700\n",
      "Epoch: 3, edge loss: 0.9996, node loss: 91558.3239 8836.3923 61220.7463 15962.6104\n",
      "Epoch: 4, edge loss: 0.9998, node loss: 37578.3162 6670.1041 27340.3042 9394.6468\n",
      "Epoch: 5, edge loss: 1.0001, node loss: 13773.7929 2010.1692 8834.4905 3458.8201\n",
      "Epoch: 6, edge loss: 1.0003, node loss: 4919.4524 1254.9406 4415.5198 4312.5236\n",
      "Epoch: 7, edge loss: 0.9999, node loss: 1988.4219 472.9746 3241.9040 2857.2037\n",
      "Epoch: 8, edge loss: 1.0001, node loss: 842.9743 389.7685 1370.0753 917.3833\n",
      "Epoch: 9, edge loss: 0.9999, node loss: 1457.4334 78.4055 964.1626 1161.2530\n",
      "Epoch: 10, edge loss: 1.0003, node loss: 715.1795 87.3067 797.1541 870.4614\n",
      "Epoch: 11, edge loss: 1.0002, node loss: 446.7070 127.2367 647.1093 60.2490\n",
      "Epoch: 12, edge loss: 1.0002, node loss: 671.8185 133.3100 1388.7774 188.4083\n",
      "Epoch: 13, edge loss: 1.0003, node loss: 268.9321 40.9088 124.9851 61.9492\n",
      "Epoch: 14, edge loss: 1.0000, node loss: 293.4876 41.0073 76.6907 162.1293\n",
      "Epoch: 15, edge loss: 0.9999, node loss: 140.8356 83.9128 112.7706 65.6448\n",
      "Epoch: 16, edge loss: 0.9998, node loss: 677.8283 193.2793 161.5573 463.7333\n",
      "Epoch: 17, edge loss: 1.0001, node loss: 281.5480 548.2106 128.2681 243.7062\n",
      "Epoch: 18, edge loss: 0.9994, node loss: 684.2042 179.5200 224.1018 54.2257\n",
      "Epoch: 19, edge loss: 0.9999, node loss: 1677.4541 421.2111 5698.2689 76.0775\n",
      "Epoch: 20, edge loss: 0.9998, node loss: 863.3264 105.1261 120.1446 77.6115\n",
      "Epoch: 21, edge loss: 1.0000, node loss: 118.8693 107.7256 47.0604 84.3692\n",
      "Epoch: 22, edge loss: 0.9998, node loss: 60.0074 133.5568 26.2477 102.1318\n",
      "Epoch: 23, edge loss: 1.0001, node loss: 76.7469 142.3875 64.2563 138.9193\n",
      "Epoch: 24, edge loss: 0.9998, node loss: 1946.7907 1089.9009 1311.6047 3830.5270\n",
      "Epoch: 25, edge loss: 0.9999, node loss: 114.4615 101.4089 127.0437 382.5789\n",
      "Epoch: 26, edge loss: 1.0005, node loss: 38.0569 47.9175 27.5822 446.5396\n",
      "Epoch: 27, edge loss: 0.9999, node loss: 123.3322 119.3452 94.4592 359.9773\n",
      "Epoch: 28, edge loss: 1.0002, node loss: 60.5052 36.2640 21.0236 132.0938\n",
      "Epoch: 29, edge loss: 1.0004, node loss: 462.3115 92.7554 287.6594 183.4542\n",
      "Epoch: 30, edge loss: 1.0000, node loss: 198.3854 207.3076 164.3336 211.9882\n",
      "Epoch: 31, edge loss: 0.9998, node loss: 556.2760 1938.0479 3469.7622 1362.0727\n",
      "Epoch: 32, edge loss: 1.0003, node loss: 92.0352 849.4114 201.0313 375.7459\n",
      "Epoch: 33, edge loss: 0.9998, node loss: 40.2609 197.1920 75.0508 126.4874\n",
      "Epoch: 34, edge loss: 0.9998, node loss: 824.6963 1202.0642 505.2319 618.4360\n",
      "Epoch: 35, edge loss: 0.9999, node loss: 554.8683 1678.8519 2888.0598 2014.3889\n",
      "Epoch: 36, edge loss: 0.9994, node loss: 3181.4254 6587.6339 4775.2539 8630.2660\n",
      "Epoch: 37, edge loss: 0.9999, node loss: 1105.2058 133.2717 776.1973 341.5470\n",
      "Epoch: 38, edge loss: 0.9995, node loss: 2975.3644 4344.6446 35697.4991 1548.8764\n",
      "Epoch: 39, edge loss: 1.0003, node loss: 2004.8130 1520.4379 1105.1805 2460.0940\n",
      "Epoch: 40, edge loss: 0.9997, node loss: 11244.0905 3740.5593 7664.8330 10260.3348\n",
      "Epoch: 41, edge loss: 0.9998, node loss: 13999.3157 20690.6997 26526.0007 7125.8351\n",
      "Epoch: 42, edge loss: 0.9997, node loss: 483.4857 443.5956 1277.5931 292.0805\n",
      "Epoch: 43, edge loss: 1.0000, node loss: 48.8958 607.8535 603.6223 29.1302\n",
      "Epoch: 44, edge loss: 0.9995, node loss: 22.6591 64.0163 54.8411 23.7414\n",
      "Epoch: 45, edge loss: 1.0004, node loss: 79.2972 245.2105 156.9208 15.9832\n",
      "Epoch: 46, edge loss: 0.9996, node loss: 733.8790 72.1597 104.6050 235.9719\n",
      "Epoch: 47, edge loss: 1.0001, node loss: 197.3531 45.5665 51.6577 222.9926\n",
      "Epoch: 48, edge loss: 1.0007, node loss: 671.6807 403.9527 206.9160 390.2083\n",
      "Epoch: 49, edge loss: 1.0002, node loss: 8235.6137 10379.2949 8998.7229 9932.8614\n",
      "Epoch: 50, edge loss: 0.9999, node loss: 2623974.0143 17786831.9904 5741241.4497 6243445.6147\n",
      "Epoch: 51, edge loss: 0.9997, node loss: 2693.7770 1384.0983 3275.7419 1175.1734\n",
      "Epoch: 52, edge loss: 0.9998, node loss: 746.6894 507.9749 861.6387 82.3666\n",
      "Epoch: 53, edge loss: 1.0004, node loss: 211.0770 66.7506 258.7832 111.5694\n",
      "Epoch: 54, edge loss: 0.9996, node loss: 662.6555 106.1519 360.2245 121.4099\n",
      "Epoch: 55, edge loss: 1.0003, node loss: 411.7598 61.2214 255.6341 70.9321\n",
      "Epoch: 56, edge loss: 1.0002, node loss: 124.2749 32.0433 80.2992 31.4658\n",
      "Epoch: 57, edge loss: 1.0004, node loss: 24.9665 9.4136 23.2569 17.8930\n",
      "Epoch: 58, edge loss: 1.0001, node loss: 27.2438 16.9590 19.0515 18.0516\n",
      "Epoch: 59, edge loss: 0.9998, node loss: 52.8499 47.0024 27.3860 39.6297\n",
      "Epoch: 60, edge loss: 1.0000, node loss: 50.7675 93.9045 59.0754 37.4958\n",
      "Epoch: 61, edge loss: 0.9999, node loss: 258.8833 371.9652 276.3463 192.6294\n",
      "Epoch: 62, edge loss: 0.9998, node loss: 5534.8949 2753.0251 4284.7453 6450.3757\n",
      "Epoch: 63, edge loss: 1.0001, node loss: 14550.4477 1842.9829 1878.3887 2138.6360\n",
      "Epoch: 64, edge loss: 1.0000, node loss: 692.5553 41.2383 50.2576 212.5170\n",
      "Epoch: 65, edge loss: 0.9996, node loss: 131.1247 52.4679 48.9941 280.2619\n",
      "Epoch: 66, edge loss: 1.0001, node loss: 110.0872 79.7866 14.5755 112.4121\n",
      "Epoch: 67, edge loss: 0.9997, node loss: 11.7048 238.3111 37.6614 106.8895\n",
      "Epoch: 68, edge loss: 1.0000, node loss: 328.6154 2056.2576 2028.3687 503.5503\n",
      "Epoch: 69, edge loss: 1.0001, node loss: 20.5600 719.5256 145.5794 755.4113\n",
      "Epoch: 70, edge loss: 1.0005, node loss: 12.7886 67.3491 133.7876 146.1027\n",
      "Epoch: 71, edge loss: 0.9998, node loss: 34.6095 32.6511 288.8496 103.2704\n",
      "Epoch: 72, edge loss: 0.9999, node loss: 57.4379 153.3027 307.0773 426.1027\n",
      "Epoch: 73, edge loss: 1.0002, node loss: 212.9251 89.7932 325.4322 900.0679\n",
      "Epoch: 74, edge loss: 1.0004, node loss: 99.9942 168.9123 212.0641 51.9472\n",
      "Epoch: 75, edge loss: 1.0008, node loss: 482.2934 508.5260 1477.5385 160.4296\n",
      "Epoch: 76, edge loss: 0.9995, node loss: 24.8853 223.2480 21.5039 32.9885\n",
      "Epoch: 77, edge loss: 0.9997, node loss: 43.3255 13.4077 4.3520 98.3008\n",
      "Epoch: 78, edge loss: 1.0000, node loss: 100.5197 65.6628 5.2295 164.4456\n",
      "Epoch: 79, edge loss: 1.0003, node loss: 95.2516 62.8405 4.7720 159.0622\n",
      "Epoch: 80, edge loss: 0.9997, node loss: 98.5642 64.7957 4.5142 164.6798\n",
      "Epoch: 81, edge loss: 0.9996, node loss: 93.7508 61.7201 4.4488 156.4706\n",
      "Epoch: 82, edge loss: 0.9999, node loss: 103.7366 68.0189 4.9460 173.4755\n",
      "Epoch: 83, edge loss: 1.0000, node loss: 98.7970 64.9298 4.4649 165.5028\n",
      "Epoch: 84, edge loss: 1.0002, node loss: 95.5247 62.5017 4.7421 159.1536\n",
      "Epoch: 85, edge loss: 0.9998, node loss: 98.2280 64.2279 4.5740 163.9488\n",
      "Epoch: 86, edge loss: 1.0000, node loss: 99.5047 65.0820 4.5886 165.4575\n",
      "Epoch: 87, edge loss: 1.0002, node loss: 314.7763 209.6278 8.9488 543.9546\n",
      "Epoch: 88, edge loss: 1.0002, node loss: 92.6272 60.4802 4.8800 154.4050\n",
      "Epoch: 89, edge loss: 0.9999, node loss: 96.0995 62.5237 4.8444 159.5793\n",
      "Epoch: 90, edge loss: 0.9996, node loss: 95.3535 62.4555 4.4966 159.1523\n",
      "Epoch: 91, edge loss: 0.9997, node loss: 99.0121 65.0741 4.5990 165.6152\n",
      "Epoch: 92, edge loss: 0.9999, node loss: 98.0103 64.4131 5.3584 162.4549\n",
      "Epoch: 93, edge loss: 1.0001, node loss: 96.1227 62.6064 4.7913 158.0580\n",
      "Epoch: 94, edge loss: 0.9999, node loss: 91.9051 60.4826 4.3182 153.4287\n",
      "Epoch: 95, edge loss: 1.0004, node loss: 324.8212 214.4023 8.7872 560.8278\n",
      "Epoch: 96, edge loss: 0.9999, node loss: 94.3898 61.5112 4.7314 156.6963\n",
      "Epoch: 97, edge loss: 1.0001, node loss: 94.8508 62.1182 4.5137 158.7265\n",
      "Epoch: 98, edge loss: 0.9997, node loss: 91.7229 59.9027 4.4366 153.0720\n",
      "Epoch: 99, edge loss: 0.9996, node loss: 318.8637 209.5882 8.8074 549.8171\n",
      "Epoch: 100, edge loss: 0.9996, node loss: 97.6318 63.2938 5.0991 160.4574\n",
      "Epoch: 101, edge loss: 0.9995, node loss: 321.6168 212.9073 9.4087 553.6394\n",
      "Epoch: 102, edge loss: 0.9999, node loss: 95.6306 62.8050 4.4396 158.6964\n",
      "Epoch: 103, edge loss: 1.0005, node loss: 100.3507 65.9439 4.4705 168.2272\n",
      "Epoch: 104, edge loss: 1.0001, node loss: 91.4719 60.0062 4.4391 151.4299\n",
      "Epoch: 105, edge loss: 1.0003, node loss: 92.8268 60.6445 4.4644 154.3243\n",
      "Epoch: 106, edge loss: 1.0003, node loss: 91.1410 59.2204 4.5197 151.2337\n",
      "Epoch: 107, edge loss: 1.0001, node loss: 98.5691 64.8700 4.6508 165.2504\n",
      "Epoch: 108, edge loss: 1.0000, node loss: 96.2952 63.4518 4.4388 160.9114\n",
      "Epoch: 109, edge loss: 1.0006, node loss: 97.7284 63.6565 4.5058 162.7836\n",
      "Epoch: 110, edge loss: 0.9995, node loss: 97.0061 64.1208 4.5944 162.7047\n",
      "Epoch: 111, edge loss: 0.9997, node loss: 326.0152 215.3584 9.2913 561.3795\n",
      "Epoch: 112, edge loss: 1.0000, node loss: 95.2583 63.1419 4.7913 158.3663\n",
      "Epoch: 113, edge loss: 1.0000, node loss: 96.5654 63.7110 4.5785 161.2714\n",
      "Epoch: 114, edge loss: 1.0003, node loss: 97.9810 64.2709 4.5565 163.3388\n",
      "Epoch: 115, edge loss: 0.9999, node loss: 339.8571 224.8102 9.8305 582.5343\n",
      "Epoch: 116, edge loss: 1.0002, node loss: 94.8667 62.1735 4.4515 158.4437\n",
      "Epoch: 117, edge loss: 1.0000, node loss: 93.4209 61.5204 4.8030 155.8768\n",
      "Epoch: 118, edge loss: 1.0004, node loss: 92.6440 60.6995 4.4519 154.7342\n",
      "Epoch: 119, edge loss: 1.0000, node loss: 98.7440 64.7847 4.3954 165.2051\n",
      "Epoch: 120, edge loss: 1.0000, node loss: 97.7108 64.0140 4.6219 163.4342\n",
      "Epoch: 121, edge loss: 0.9997, node loss: 94.0021 61.7761 4.4478 157.0382\n",
      "Epoch: 122, edge loss: 1.0004, node loss: 96.3520 62.7115 4.6261 160.3057\n",
      "Epoch: 123, edge loss: 1.0003, node loss: 95.6968 62.4948 4.6568 159.4021\n",
      "Epoch: 124, edge loss: 1.0003, node loss: 99.6022 66.2568 5.0500 166.0447\n",
      "Epoch: 125, edge loss: 0.9998, node loss: 97.8079 64.4822 4.5053 164.0322\n",
      "Epoch: 126, edge loss: 0.9998, node loss: 97.8650 64.1250 4.8449 163.1540\n",
      "Epoch: 127, edge loss: 0.9997, node loss: 94.4772 62.3418 4.3755 157.5829\n",
      "Epoch: 128, edge loss: 0.9996, node loss: 97.1266 63.5852 4.8908 159.8812\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize early stopping\n",
    "node_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=node_model_name)\n",
    "edge_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=edge_model_name)\n",
    "\n",
    "# Training loop\n",
    "edge_train_losses = []\n",
    "node_train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    edge_loss_cum = 0\n",
    "    node_loss_cum = np.zeros(n_node_features-1, dtype=float)\n",
    "    for batch_0 in train_loader:\n",
    "        #print()\n",
    "        # Clone batch of graphs\n",
    "        g_batch_0 = batch_0.clone()\n",
    "        \n",
    "        # Move batch data to GPU\n",
    "        g_batch_0 = g_batch_0.to(device)\n",
    "        \n",
    "        # Read number of graphs in batch\n",
    "        batch_size_0 = g_batch_0.num_graphs\n",
    "\n",
    "        # Save graph-level embeddings\n",
    "        embedding_batch_0 = []\n",
    "        for idx in range(batch_size_0):\n",
    "            embedding_batch_0.append(g_batch_0[idx].y.detach().to(device))\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        t_steps = np.arange(1, n_t_steps+1)\n",
    "        for t_step in t_steps:\n",
    "            # Read time step, which is added to node-level graph embeddings\n",
    "            t_step_std = torch.tensor([t_step / n_t_steps - 0.5], dtype=torch.float).to(device)  # Standard normalization\n",
    "        \n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            g_batch_t = []\n",
    "            e_batch_t = []\n",
    "            for idx in range(batch_size_0):\n",
    "                # Perform a diffusion step at time step t_step for each graph within the batch\n",
    "                graph_t, epsilon_t = diffusion_step(g_batch_0[idx], t_step, n_t_steps, alpha_decay)\n",
    "                \n",
    "                # Append noisy graphs and noises\n",
    "                g_batch_t.append(graph_t)\n",
    "                e_batch_t.append(epsilon_t)\n",
    "        \n",
    "                # Update diffused graph as next one\n",
    "                g_batch_0[idx] = graph_t.clone()\n",
    "            \n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add embeddings to noisy graphs (t_step information and graph-level embeddings)\n",
    "            for idx in range(batch_size_0):\n",
    "                # Add graph-level embedding to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       embedding_batch_0[idx])  # To match graph.y shape\n",
    "        \n",
    "                # Add t_step information to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       t_step_std)  # To match graph.y shape, which is 1D\n",
    "        \n",
    "            # Generate batch objects\n",
    "            g_batch_t = Batch.from_data_list(g_batch_t)\n",
    "            e_batch_t = Batch.from_data_list(e_batch_t)\n",
    "            \n",
    "            # Move data to device\n",
    "            g_batch_t = g_batch_t.to(device)\n",
    "            e_batch_t = e_batch_t.to(device)\n",
    "            \n",
    "            # Predict batch noise at given time step\n",
    "            pred_epsilon_t = predict_noise(g_batch_t, node_model, edge_model)\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the losses for node features and edge attributes\n",
    "            node_losses, edge_loss = get_graph_losses(e_batch_t, pred_epsilon_t, batch_size_0)\n",
    "            \n",
    "            # Combine losses for each attribute tensors\n",
    "            node_loss = torch.stack(node_losses).sum()\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            if not node_early_stopping.early_stop:\n",
    "                node_loss.backward(retain_graph=True)\n",
    "                node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            if not edge_early_stopping.early_stop:\n",
    "                edge_loss.backward(retain_graph=True)\n",
    "                edge_optimizer.step()\n",
    "            \n",
    "            # Get items\n",
    "            node_loss_cum += np.array([node_loss.item() for node_loss in node_losses])\n",
    "            edge_loss_cum += edge_loss.item()\n",
    "\n",
    "            del g_batch_t, e_batch_t, pred_epsilon_t, node_loss, edge_loss  # Free up CUDA memory\n",
    "\n",
    "    # Compute the average train loss over n_t_steps\n",
    "    node_loss_cum /= (n_t_steps * len(train_loader))\n",
    "    edge_loss_cum /= (n_t_steps * len(train_loader))\n",
    "    \n",
    "    # Append average losses\n",
    "    node_train_losses.append(node_loss_cum)\n",
    "    edge_train_losses.append(edge_loss_cum)\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    node_early_stopping(node_loss_cum.sum(), node_model)\n",
    "    edge_early_stopping(edge_loss_cum,       edge_model)\n",
    "\n",
    "    if node_early_stopping.early_stop and edge_early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "    \n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Epoch: {epoch+1}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:08:27.881116814Z"
    }
   },
   "id": "e9ac110e6708ce90",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rescaled_edge_train_losses = np.sqrt(edge_train_losses) * dataset_parameters['edge_std'].numpy() + dataset_parameters['edge_mean'].numpy()\n",
    "rescaled_edge_train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:05:23.961158622Z"
    }
   },
   "id": "754680a5eda66bbc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "node_train_losses = np.array(node_train_losses)\n",
    "rescaled_node_loss_cum = np.sqrt(node_train_losses) * dataset_parameters['feat_std'].numpy() + dataset_parameters['feat_mean'].numpy()\n",
    "rescaled_node_loss_cum"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T11:05:23.963143434Z",
     "start_time": "2024-04-05T11:05:23.961241778Z"
    }
   },
   "id": "3eff196b7a5827b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(np.log(edge_train_losses), label='Edge')\n",
    "for i in range(n_node_features-1):\n",
    "    plt.plot(np.log(np.array(node_train_losses)[:, i]), label=f'Node {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:05:23.962846474Z"
    }
   },
   "id": "706862a0dcd940f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "792c339357d9b80"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training loop\n",
    "edge_test_losses = 0\n",
    "node_test_losses = np.zeros(n_node_features-1, dtype=float)\n",
    "idx = 0\n",
    "for batch_0 in test_loader:\n",
    "    # Move batch data to GPU\n",
    "    batch_0 = batch_0.to(device)\n",
    "    \n",
    "    # Read number of graphs in batch\n",
    "    batch_size = batch_0.num_graphs\n",
    "    \n",
    "    # Diffuse batch\n",
    "    g_batch_t = diffuse(batch_0, n_t_steps, s=alpha_decay)\n",
    "    \n",
    "    # Denoise batch\n",
    "    g_batch_0 = denoise(g_batch_t, n_t_steps, node_model, edge_model, n_graph_features,\n",
    "                        s=alpha_decay, sigma=sigma)\n",
    "    \n",
    "    # Calculate the loss for node features and edge attributes\n",
    "    node_losses, edge_loss = get_graph_losses(batch_0, g_batch_0, batch_size)\n",
    "    \n",
    "    # Get items\n",
    "    edge_loss_cum = edge_loss.item()\n",
    "    node_loss_cum = np.array([node_loss.item() for node_loss in node_losses])\n",
    "    \n",
    "    # Append average losses\n",
    "    edge_test_losses += edge_loss_cum\n",
    "    node_test_losses += node_loss_cum\n",
    "    \n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Batch: {idx}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')\n",
    "    idx += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:05:23.962882372Z"
    }
   },
   "id": "ebfaa164497b70e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "node_test_losses /= len(test_loader)\n",
    "edge_test_losses /= len(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:05:23.962905335Z"
    }
   },
   "id": "c6287db2b4fcb0e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "edge_test_losses, node_test_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:05:23.962925233Z"
    }
   },
   "id": "7dec1d43c58f6cd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c910d3fb3556314"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create and save as a dictionary\n",
    "model_performance = {\n",
    "    'edge_train_losses': edge_train_losses,\n",
    "    'node_train_losses': np.array(node_train_losses).tolist(),\n",
    "    'edge_test_losses':  edge_test_losses,\n",
    "    'node_test_losses':  node_test_losses.tolist()\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_performance.json', 'w') as json_file:\n",
    "    json.dump(model_performance, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T11:05:23.962943727Z"
    }
   },
   "id": "bc9aff57d49941db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
