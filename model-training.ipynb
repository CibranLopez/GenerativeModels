{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cb4ec9b58cab36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:21:52.337547907Z",
     "start_time": "2024-10-25T18:21:50.860763569Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import torch.nn          as nn\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torch_geometric.data   import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph, predict_noise, diffuse, denoise, EarlyStopping, get_alpha_t\n",
    "from libraries.dataset      import standardize_dataset, get_datasets\n",
    "\n",
    "# Set colormap\n",
    "cmap = plt.get_cmap('plasma')\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bd0d03e0d6c4c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:21:52.340618316Z",
     "start_time": "2024-10-25T18:21:52.338866625Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4cb0f2aacc0afab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:21:53.904185679Z",
     "start_time": "2024-10-25T18:21:53.901986317Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs, he model is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property\n",
    "# Although denoising includes noise, I think it is better not to add it when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31459ad11c33a291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:21:54.246580214Z",
     "start_time": "2024-10-25T18:21:54.071379465Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/Loaded_QM9_gap-sphere-images/GM_v2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'Loaded_QM9_gap-sphere-images'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "target_folder = None\n",
    "if target_folder is None:\n",
    "    # Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "    i = 0\n",
    "    while True:\n",
    "        target_folder = f'{general_folder}/GM_v{i}'\n",
    "        if not os.path.exists(target_folder):\n",
    "            # Copy all data\n",
    "            os.system(f'cp -r {data_folder} {target_folder}')\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6208b4c9c572b6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:21:55.107143182Z",
     "start_time": "2024-10-25T18:21:55.004397445Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 100\n",
    "batch_size    = 256\n",
    "learning_rate = 0.0001\n",
    "patience      = n_epochs\n",
    "delta         = 0.2\n",
    "check_labels  = False  # Whether to train-test split attending to labels or not\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = torch.tensor(1000, dtype=torch.int, device=device)\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = torch.tensor(0, device=device)  # Zero for training purposes\n",
    "\n",
    "# Decay of parameter alpha\n",
    "alpha_decay = torch.tensor(0.0001, device=device)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.1\n",
    "dropout_edge = 0.1\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':   data_folder,\n",
    "    'n_epochs':      n_epochs,\n",
    "    'batch_size':    batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'patience':      patience,\n",
    "    'delta':         delta,\n",
    "    'check_labels':  check_labels,\n",
    "    'n_t_steps':     n_t_steps.cpu().numpy().tolist(),\n",
    "    'sigma':         sigma.cpu().numpy().tolist(),\n",
    "    'alpha_decay':   alpha_decay.cpu().numpy().tolist(),\n",
    "    'dropout_node':  dropout_node,\n",
    "    'dropout_edge':  dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc757ea2296182",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c94c5e9672e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load the dataset, already standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81add81bc5efff72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:22:40.662937118Z",
     "start_time": "2024-10-25T18:21:56.570437488Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "labels_name_std             = f'{target_folder}/standardized_labels.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "if os.path.exists(dataset_name_std) and os.path.exists(dataset_parameters_name_std) and os.path.exists(labels_name_std):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset = torch.load(dataset_name_std, weights_only=False)\n",
    "    labels  = torch.load(labels_name_std,  weights_only=False)\n",
    "    \n",
    "    # Load the data from the JSON file\n",
    "    with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "        numpy_dict = json.load(json_file)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    dataset_parameters = {}\n",
    "    for key, value in numpy_dict.items():\n",
    "        try:\n",
    "            dataset_parameters[key] = torch.tensor(value)\n",
    "        except:\n",
    "            dataset_parameters[key] = value\n",
    "\n",
    "elif os.path.exists(dataset_name) and os.path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name, weights_only=False)\n",
    "    labels  = torch.load(labels_name,  weights_only=False)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, labels, dataset_parameters = standardize_dataset(dataset, labels)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    torch.save(labels,  labels_name_std)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {}\n",
    "    for key, value in dataset_parameters.items():\n",
    "        try:\n",
    "            numpy_dict[key] = value.cpu().numpy().tolist()\n",
    "        except:\n",
    "            numpy_dict[key] = value\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)\n",
    "\n",
    "else:\n",
    "    sys.exit('Error: the database is not available')\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813e0dc-6328-4bb2-b3a3-ae979e84498d",
   "metadata": {},
   "source": [
    "Include features and prepare shape of dataset for training, considerably reducing CPU-GPU communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9141a8aa0c071000",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:22:41.223180138Z",
     "start_time": "2024-10-25T18:22:41.002003720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Determine number of graphs in dataset\n",
    "n_graphs = len(dataset)\n",
    "\n",
    "# Determine number of node-level features in dataset\n",
    "n_node_features = dataset[0].num_node_features\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(dataset[0].y)\n",
    "\n",
    "# Make room for n_graph_features and t_steps in the dataset\n",
    "for idx in range(n_graphs):\n",
    "    dataset[idx] = add_features_to_graph(dataset[idx],\n",
    "                                         torch.tensor([dataset[idx].y, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd0afbf5b67ea7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Split in train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f051cc1bf95d4aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:22:56.683866473Z",
     "start_time": "2024-10-25T18:22:41.225372318Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   graphs: 131207\n",
      "Number of validation graphs: 1340\n",
      "Number of testing    graphs: 1338\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.98\n",
    "test_ratio  = 0.01\n",
    "\n",
    "# Check if data has been already split, else do it randomly\n",
    "path_to_train_labels = f'{target_folder}/train_labels.txt'\n",
    "path_to_val_labels   = f'{target_folder}/validation_labels.txt'\n",
    "path_to_test_labels  = f'{target_folder}/test_labels.txt'\n",
    "\n",
    "# Copy labels\n",
    "material_labels = labels.copy()\n",
    "\n",
    "if check_labels:\n",
    "    if os.path.exists(path_to_train_labels) and os.path.exists(path_to_val_labels) and os.path.exists(path_to_test_labels):\n",
    "        # Read labels splitting (which are strings)\n",
    "        train_labels = np.genfromtxt(path_to_train_labels, dtype='str').tolist()\n",
    "        val_labels   = np.genfromtxt(path_to_val_labels,   dtype='str').tolist()\n",
    "        test_labels  = np.genfromtxt(path_to_test_labels,  dtype='str').tolist()\n",
    "    else:\n",
    "        # Splitting into train-test sets considering that Fvs from the same materials must be in the same dataset\n",
    "        material_labels = [label.split()[0] for label in material_labels]\n",
    "        \n",
    "        # Define unique labels\n",
    "        unique_labels = np.unique(material_labels)\n",
    "        \n",
    "        # Shuffle the list of unique labels\n",
    "        np.random.shuffle(unique_labels)\n",
    "    \n",
    "        # Define the sizes of the train and test sets\n",
    "        # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "        train_size = int(train_ratio * len(unique_labels))\n",
    "        test_size  = int(test_ratio  * len(unique_labels))\n",
    "        \n",
    "        train_labels = unique_labels[:train_size]\n",
    "        val_labels   = unique_labels[train_size:-test_size]\n",
    "        test_labels  = unique_labels[-test_size:]\n",
    "    \n",
    "        # Save this splitting for transfer-learning approaches\n",
    "        np.savetxt(path_to_train_labels, train_labels, fmt='%s')\n",
    "        np.savetxt(path_to_val_labels,   val_labels,   fmt='%s')\n",
    "        np.savetxt(path_to_test_labels,  test_labels,  fmt='%s')\n",
    "\n",
    "    # Use the computed indexes to generate train and test sets\n",
    "    # We iteratively check where labels equals a unique train/test labels and append the index to a list\n",
    "    train_dataset = get_datasets(train_labels, material_labels, dataset)\n",
    "    val_dataset   = get_datasets(val_labels,   material_labels, dataset)\n",
    "    test_dataset  = get_datasets(test_labels,  material_labels, dataset)\n",
    "else:\n",
    "    # Define the sizes of the train and test sets\n",
    "    # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size  = int(test_ratio  * len(dataset))\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "    # Random, fast splitting\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset   = dataset[train_size:-test_size]\n",
    "    test_dataset  = dataset[-test_size:]\n",
    "\n",
    "del dataset  # Free up CUDA memory\n",
    "\n",
    "print(f'Number of training   graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of testing    graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f25e2999d4d5ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86226627fd660ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:22:56.684414643Z",
     "start_time": "2024-10-25T18:22:56.683788707Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "del train_dataset, val_dataset, test_dataset  # Free up CUDA memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db14b7003fd1f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ead2e3b00e44faf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:22:56.698384348Z",
     "start_time": "2024-10-25T18:22:56.683874819Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "DataParallel(\n",
      "  (module): nGCNN(\n",
      "    (conv1): GraphConv(6, 256)\n",
      "    (conv2): GraphConv(256, 512)\n",
      "    (conv3): GraphConv(512, 256)\n",
      "    (conv4): GraphConv(256, 4)\n",
      "    (norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "DataParallel(\n",
      "  (module): eGCNN(\n",
      "    (linear1): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (linear3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (linear4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models for nodes and edges, considering the t_step information; n_graph_features+1 for accounting for the time step\n",
    "node_model = nGCNN(n_node_features, n_graph_features+1, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features+1, dropout_edge).to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load and evaluate model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name, map_location=torch.device(device), weights_only=False))\n",
    "    node_model.eval()\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name, map_location=torch.device(device), weights_only=False))\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Allow data parallelization among multi-GPU\n",
    "node_model= nn.DataParallel(node_model)\n",
    "edge_model= nn.DataParallel(edge_model)\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0f4a07f1c8daa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da315ecb-cda2-4c12-a1af-990c2e50d053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:22:57.625570410Z",
     "start_time": "2024-10-25T18:22:56.703077598Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "node_attributes_list = ['edge_attr', 'atomic_mass', 'charge', 'electronegativity', 'ionization_energy']\n",
    "\n",
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize early stopping\n",
    "node_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=node_model_name)\n",
    "edge_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=edge_model_name)\n",
    "\n",
    "# Training loop\n",
    "edge_train_losses = []\n",
    "node_train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    edge_loss_cum = 0\n",
    "    node_loss_cum = np.zeros(n_node_features, dtype=float)\n",
    "    ground_truth = {k: [] for k in node_attributes_list}\n",
    "    prediction   = {k: [] for k in node_attributes_list}\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(node_attributes_list), figsize=(10, 2))\n",
    "\n",
    "    for batch_idx, batch_0 in enumerate(train_loader):\n",
    "        # Clone batch of graphs and move to device\n",
    "        g_batch_0 = batch_0.clone().to(device)\n",
    "\n",
    "        # Start denoising-diffusing process\n",
    "        for t_step in torch.arange(n_t_steps, device=device):\n",
    "            # Initialize the gradient of the optimizers\n",
    "            node_optimizer.zero_grad()\n",
    "            edge_optimizer.zero_grad()\n",
    "            \n",
    "            # Standard normalization for the time step, which is added to node-level graph embeddings after \n",
    "            t_step_std = t_step / n_t_steps - 0.5 \n",
    "            \n",
    "            # Get amount of noise that is added\n",
    "            alpha_t = get_alpha_t(t_step, n_t_steps, alpha_decay)\n",
    "            \n",
    "            # Diffuse the entire batch of graphs with Gaussian noise\n",
    "            # Vectorized version of diffusion_step for batching\n",
    "            g_batch_t, e_batch_t = diffusion_step(g_batch_0, alpha_t, n_features=n_node_features)\n",
    "            \n",
    "            # Stack time step across batch dimension \n",
    "            g_batch_t.x[:, -1] = t_step_std\n",
    "\n",
    "            # Update diffused batch as next one\n",
    "            g_batch_0 = g_batch_t.clone()\n",
    "            \n",
    "            # Predict batch noise at given time step\n",
    "            pred_epsilon_t = predict_noise(g_batch_t, node_model, edge_model)\n",
    "\n",
    "            # Calculate the losses for node features and edge attributes\n",
    "            node_losses, edge_loss = get_graph_losses(e_batch_t, pred_epsilon_t)\n",
    "            \n",
    "            # Combine losses for each attribute tensors\n",
    "            node_loss = torch.stack(node_losses).sum()\n",
    "            \n",
    "            # Get items\n",
    "            node_loss_cum += np.array([node_loss.item() for node_loss in node_losses])\n",
    "            edge_loss_cum += edge_loss.item()\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            if not node_early_stopping.early_stop:\n",
    "                node_loss.backward(retain_graph=True)\n",
    "                #torch.nn.utils.clip_grad_norm_(node_model.parameters(), max_norm=2.0)\n",
    "                node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            if not edge_early_stopping.early_stop:\n",
    "                edge_loss.backward(retain_graph=True)\n",
    "                #torch.nn.utils.clip_grad_norm_(edge_model.parameters(), max_norm=2.0)\n",
    "                edge_optimizer.step()\n",
    "\n",
    "            # Randomly sample batches to store data\n",
    "            if not t_step%10 and batch_idx == 0:\n",
    "                for key, idx in zip(node_attributes_list, range(n_node_features+1)):\n",
    "                    # In this order: edge_attr, x0, x1, x2, x3, ... to lists\n",
    "                    gt_value   = (     e_batch_t.edge_attr if not idx else      e_batch_t.x[:, idx-1]).cpu().detach().numpy().tolist()\n",
    "                    pred_value = (pred_epsilon_t.edge_attr if not idx else pred_epsilon_t.x[:, idx-1]).cpu().detach().numpy().tolist()\n",
    "\n",
    "                    # Append to dictionaries\n",
    "                    ground_truth[key].append(gt_value)\n",
    "                    prediction[key].append(pred_value)\n",
    "\n",
    "                    ax[idx].plot(gt_value, pred_value, '.', color=cmap(((t_step + 1) / n_t_steps).cpu().detach().numpy()))\n",
    "    \n",
    "    # Plot once per epoch\n",
    "    for idx, item in enumerate(ground_truth):\n",
    "        gt_values   = np.concatenate(ground_truth[item])\n",
    "        pred_values = np.concatenate(prediction[item])\n",
    "        \n",
    "        _min, _max = min(gt_values.min(), pred_values.min()), max(gt_values.max(), pred_values.max())\n",
    "        ax[idx].plot([_min, _max], [_min, _max], '-r')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute the average train loss over n_t_steps\n",
    "    node_loss_cum /= (n_t_steps.cpu().numpy() * len(train_loader))\n",
    "    edge_loss_cum /= (n_t_steps.cpu().numpy() * len(train_loader))\n",
    "\n",
    "    # Append average losses\n",
    "    node_train_losses.append(node_loss_cum)\n",
    "    edge_train_losses.append(edge_loss_cum)\n",
    "\n",
    "    # Check early stopping criteria\n",
    "    node_early_stopping(node_loss_cum.sum(), node_model)\n",
    "    edge_early_stopping(edge_loss_cum,       edge_model)\n",
    "\n",
    "    if node_early_stopping.early_stop and edge_early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Epoch: {epoch+1}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64005213-eece-4e4b-8ee6-2e4913e847c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_edge_train_losses = np.sqrt(edge_train_losses) * dataset_parameters['edge_std'].numpy() + dataset_parameters['edge_mean'].numpy()\n",
    "rescaled_edge_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb8249-2ad7-47cb-84e9-d8549c6717f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:03:31.369806601Z",
     "start_time": "2024-06-30T20:03:31.366148559Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "node_train_losses = np.array(node_train_losses)\n",
    "rescaled_node_loss_cum = np.sqrt(node_train_losses) * dataset_parameters['feat_std'].numpy() + dataset_parameters['feat_mean'].numpy()\n",
    "rescaled_node_loss_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d426ca-ff59-45c3-8cb2-2b0472170e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:03:31.433903110Z",
     "start_time": "2024-06-30T20:03:31.371026869Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(edge_train_losses), label='Edge')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa583ac-d357-497a-8f61-75f49754191f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:03:31.807704623Z",
     "start_time": "2024-06-30T20:03:31.486950285Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array(node_train_losses)[:, 0]), label=f'Atomic mass')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 1]), label=f'Charge')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 2]), label=f'Electronegativity')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 3]), label=f'Ionization energy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Losses.eps', dpi=50, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c339357d9b80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Test of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfaa164497b70e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:07:31.265017721Z",
     "start_time": "2024-06-30T20:03:31.809107806Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, edge loss: nan, node loss: nan nan nan nan\n",
      "Batch: 1, edge loss: nan, node loss: nan nan nan nan\n",
      "Batch: 2, edge loss: nan, node loss: nan nan nan nan\n",
      "Batch: 3, edge loss: nan, node loss: nan nan nan nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m g_batch_t \u001b[38;5;241m=\u001b[39m diffuse(g_batch_0, n_t_steps, alpha_decay)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Denoise batch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m g_batch_0 \u001b[38;5;241m=\u001b[39m \u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_batch_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_t_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_node_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the loss for node features and edge attributes\u001b[39;00m\n\u001b[1;32m     20\u001b[0m node_losses, edge_loss \u001b[38;5;241m=\u001b[39m get_graph_losses(batch_0, g_batch_0)\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/libraries/model.py:261\u001b[0m, in \u001b[0;36mdenoise\u001b[0;34m(batch_t, n_t_steps, node_model, edge_model, alpha_decay, sigma, plot_steps, n_features)\u001b[0m\n\u001b[1;32m    258\u001b[0m batch_0\u001b[38;5;241m.\u001b[39mx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m t_step_std\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Predict batch noise at given time step\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m pred_epsilon_t \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Check if intermediate steps are plotted; then, plot the NetworkX graph\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_steps:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Convert PyTorch graph to NetworkX graph\u001b[39;00m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/libraries/model.py:187\u001b[0m, in \u001b[0;36mpredict_noise\u001b[0;34m(batch_t, node_model, edge_model)\u001b[0m\n\u001b[1;32m    184\u001b[0m batch_0 \u001b[38;5;241m=\u001b[39m batch_t\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Perform a single forward pass for predicting node features\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m out_x \u001b[38;5;241m=\u001b[39m \u001b[43mnode_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Define x_i and x_j as features of every corresponding pair of nodes (same order than attributes)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m x_i \u001b[38;5;241m=\u001b[39m batch_t\u001b[38;5;241m.\u001b[39mx[batch_t\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:191\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/libraries/model.py:318\u001b[0m, in \u001b[0;36mnGCNN.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Apply graph convolution with ReLU activation function\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m    320\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index, edge_attr)\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/nn/conv/graph_conv.py:84\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, x, edge_index, edge_weight, size)\u001b[0m\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_rel(out)\n\u001b[1;32m     88\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.graph_conv_GraphConv_propagate_p2r6io6d.py:249\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_weight, size)\u001b[0m\n\u001b[1;32m    240\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    241\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    242\u001b[0m                 edge_weight\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    246\u001b[0m             )\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:594\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    579\u001b[0m     inputs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/nn/aggr/base.py:131\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/nn/aggr/base.py:185\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# For \"sum\" and \"mean\" reduction, we make use of `scatter_add_`:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:195\u001b[0m, in \u001b[0;36mbroadcast\u001b[0;34m(src, ref, dim)\u001b[0m\n\u001b[1;32m    193\u001b[0m dim \u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m+\u001b[39m dim \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dim\n\u001b[1;32m    194\u001b[0m size \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m dim) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m (ref\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m-\u001b[39m dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "edge_test_losses = 0\n",
    "node_test_losses = np.zeros(n_node_features, dtype=float)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_0 in enumerate(test_loader):\n",
    "        # Move batch to device\n",
    "        batch_0 = batch_0.to(device)\n",
    "        \n",
    "        # Clone batch of graphs and move to device\n",
    "        g_batch_0 = batch_0.clone()\n",
    "        \n",
    "        # Diffuse batch\n",
    "        g_batch_t = diffuse(g_batch_0, n_t_steps, alpha_decay)\n",
    "        \n",
    "        # Denoise batch\n",
    "        g_batch_0 = denoise(g_batch_t, n_t_steps, node_model, edge_model, alpha_decay=alpha_decay,\n",
    "                            sigma=model_parameters['sigma'], n_features=n_node_features)\n",
    "        \n",
    "        # Calculate the loss for node features and edge attributes\n",
    "        node_losses, edge_loss = get_graph_losses(batch_0, g_batch_0)\n",
    "        \n",
    "        # Get items\n",
    "        node_loss_cum = np.array([node_loss.item() for node_loss in node_losses])[:n_node_features]\n",
    "        edge_loss_cum = edge_loss.item()\n",
    "        \n",
    "        # Append average losses\n",
    "        edge_test_losses += edge_loss_cum\n",
    "        node_test_losses += node_loss_cum\n",
    "        \n",
    "        print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "        print(f'Batch: {batch_idx}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec1d43c58f6cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:07:31.272036444Z",
     "start_time": "2024-06-30T20:07:31.267461134Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mean loss per noise step on edges and nodes\n",
    "edge_test_losses /= len(test_loader)\n",
    "node_test_losses /= len(test_loader)\n",
    "edge_test_losses, node_test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c910d3fb3556314",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a35e2-f1e9-4939-b573-757e02e70440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:07:31.279659724Z",
     "start_time": "2024-06-30T20:07:31.270012992Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and save as a dictionary\n",
    "model_performance = {\n",
    "    'edge_train_losses': edge_train_losses,\n",
    "    'node_train_losses': np.array(node_train_losses).tolist(),\n",
    "    'edge_test_losses':  edge_test_losses,\n",
    "    'node_test_losses':  node_test_losses.tolist()\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_performance.json', 'w') as json_file:\n",
    "    json.dump(model_performance, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
