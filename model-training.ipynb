{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:52.555036998Z",
     "start_time": "2024-05-06T12:29:50.354818263Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy             as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torch_geometric.data   import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph, predict_noise, diffuse, denoise, EarlyStopping\n",
    "from libraries.dataset      import standardize_dataset, get_datasets\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74450972cebeaa56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:52.613764926Z",
     "start_time": "2024-05-06T12:29:52.597849736Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ad446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:52.614454153Z",
     "start_time": "2024-05-06T12:29:52.598552278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs\n",
    "# The models is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property\n",
    "# Although denoising includes noise, I think it is better not to add it when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130cc61599976691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:54.021593516Z",
     "start_time": "2024-05-06T12:29:52.598757923Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/QM9-dsC7O2H10nsd/GM_v1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'QM9-dsC7O2H10nsd'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "# Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "i = 0\n",
    "while True:\n",
    "    target_folder = f'{general_folder}/GM_v{i}'\n",
    "    if not os.path.exists(target_folder):\n",
    "        # Copy all data\n",
    "        os.system(f'cp -r {data_folder} {target_folder}')\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be066fddd93b461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:54.053602608Z",
     "start_time": "2024-05-06T12:29:54.029906507Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 1000\n",
    "batch_size    = 64\n",
    "learning_rate = 0.0001\n",
    "patience      = 20\n",
    "delta         = 0.2\n",
    "check_labels  = False  # Whether to train-test split attending to labels or not\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 1000\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = 0  # Zero for training purposes\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':        data_folder,\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'patience':           patience,\n",
    "    'delta':              delta,\n",
    "    'check_labels':       check_labels,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'sigma':              sigma,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69994b030f25e04",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3859057f714368",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load the dataset, already standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40d504f8b3b498c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:54.407106776Z",
     "start_time": "2024-05-06T12:29:54.037451364Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "if os.path.exists(dataset_name_std) and os.path.exists(dataset_parameters_name_std) and os.path.exists(labels_name):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset = torch.load(dataset_name_std)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Load the data from the JSON file\n",
    "    with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "        numpy_dict = json.load(json_file)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    dataset_parameters = {}\n",
    "    for key, value in numpy_dict.items():\n",
    "        try:\n",
    "            dataset_parameters[key] = torch.tensor(value)\n",
    "        except:\n",
    "            dataset_parameters[key] = value\n",
    "\n",
    "elif os.path.exists(dataset_name) and os.path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)\n",
    "\n",
    "else:\n",
    "    sys.exit('Error: the database is not available')\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef017add32b19031",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Split in train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd51fcec527c7be6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:54.624253729Z",
     "start_time": "2024-05-06T12:29:54.415425105Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   graphs: 4876\n",
      "Number of validation graphs: 610\n",
      "Number of testing    graphs: 609\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio  = 0.1\n",
    "\n",
    "# Check if data has been already split, else do it randomly\n",
    "path_to_train_labels = f'{target_folder}/train_labels.txt'\n",
    "path_to_val_labels   = f'{target_folder}/validation_labels.txt'\n",
    "path_to_test_labels  = f'{target_folder}/test_labels.txt'\n",
    "\n",
    "# Copy labels\n",
    "material_labels = labels.copy()\n",
    "\n",
    "if os.path.exists(path_to_train_labels) and os.path.exists(path_to_val_labels) and os.path.exists(path_to_test_labels):\n",
    "    # Read labels splitting (which are strings)\n",
    "    train_labels = np.genfromtxt(path_to_train_labels, dtype='str').tolist()\n",
    "    val_labels   = np.genfromtxt(path_to_val_labels,   dtype='str').tolist()\n",
    "    test_labels  = np.genfromtxt(path_to_test_labels,  dtype='str').tolist()\n",
    "else:\n",
    "    if check_labels:\n",
    "        # Splitting into train-test sets considering that Fvs from the same materials must be in the same dataset\n",
    "        material_labels = [label.split()[0] for label in material_labels]\n",
    "        \n",
    "        # Define unique labels\n",
    "        unique_labels = np.unique(material_labels)\n",
    "    else:\n",
    "        # Completely randomly splitting\n",
    "        # Copy material_labels\n",
    "        unique_labels = material_labels.copy()\n",
    "    \n",
    "    # Shuffle the list of unique labels\n",
    "    np.random.shuffle(unique_labels)\n",
    "\n",
    "    # Define the sizes of the train and test sets\n",
    "    # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "    train_size = int(train_ratio * len(unique_labels))\n",
    "    test_size  = int(test_ratio  * len(unique_labels))\n",
    "    \n",
    "    train_labels = unique_labels[:train_size]\n",
    "    val_labels   = unique_labels[train_size:-test_size]\n",
    "    test_labels  = unique_labels[-test_size:]\n",
    "\n",
    "    # Save this splitting for transfer-learning approaches\n",
    "    np.savetxt(path_to_train_labels, train_labels, fmt='%s')\n",
    "    np.savetxt(path_to_val_labels,   val_labels,   fmt='%s')\n",
    "    np.savetxt(path_to_test_labels,  test_labels,  fmt='%s')\n",
    "\n",
    "# Use the computed indexes to generate train and test sets\n",
    "# We iteratively check where labels equals a unique train/test labels and append the index to a list\n",
    "train_dataset = get_datasets(train_labels, material_labels, dataset)\n",
    "val_dataset   = get_datasets(val_labels,   material_labels, dataset)\n",
    "test_dataset  = get_datasets(test_labels,  material_labels, dataset)\n",
    "\n",
    "del dataset  # Free up CUDA memory\n",
    "\n",
    "print(f'Number of training   graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of testing    graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d694649c5074fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:54.656545006Z",
     "start_time": "2024-05-06T12:29:54.624999119Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for graph in train_dataset:\n",
    "    graph.y = torch.tensor([graph.y], dtype=torch.float)\n",
    "\n",
    "for graph in val_dataset:\n",
    "    graph.y = torch.tensor([graph.y], dtype=torch.float)\n",
    "\n",
    "for graph in test_dataset:\n",
    "    graph.y = torch.tensor([graph.y], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46cb045-0c8e-4b85-9722-e4a6adc885d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in train_dataset:\n",
    "    graph.x *= 100\n",
    "    graph.edge_attr *= 100\n",
    "    graph.y *= 100\n",
    "\n",
    "for graph in val_dataset:\n",
    "    graph.x *= 100\n",
    "    graph.edge_attr *= 100\n",
    "    graph.y *= 100\n",
    "\n",
    "for graph in test_dataset:\n",
    "    graph.x *= 100\n",
    "    graph.edge_attr *= 100\n",
    "    graph.y *= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ae68deba3180c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3f12ef8c461bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:54.693833793Z",
     "start_time": "2024-05-06T12:29:54.646492150Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(train_dataset[0].y)\n",
    "\n",
    "del train_dataset, val_dataset, test_dataset  # Free up CUDA memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a9d6606b82bda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82337610787fdd55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:29:55.415704412Z",
     "start_time": "2024-05-06T12:29:54.693820288Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(6, 256)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (conv3): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "    \n",
    "    # Evaluate model state\n",
    "    node_model.eval()\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d37b79cac32c2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac110e6708ce90",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T12:29:55.422864178Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, edge loss: 13498450.7582, node loss: 197205270830658057404416.0000 126394752173027543220224.0000 160213995987916201918464.0000 251069524177818136608768.0000\n",
      "Epoch: 2, edge loss: 1.0000, node loss: 197433506683932880404480.0000 126776733790780583313408.0000 161550880818566210256896.0000 250029663367963024031744.0000\n",
      "Epoch: 3, edge loss: 0.9999, node loss: 197525570955605104918528.0000 126674382954934938107904.0000 161104628106169809895424.0000 252389265557464308252672.0000\n",
      "Epoch: 4, edge loss: 1.0000, node loss: 198748615895423492554752.0000 126895809526182052888576.0000 161696758220959084707840.0000 250246169390012501590016.0000\n",
      "Epoch: 5, edge loss: 1.0000, node loss: 197639120999326771314688.0000 126939825794617683476480.0000 160522275573463141842944.0000 251774351869490705203200.0000\n",
      "Epoch: 6, edge loss: 1.0000, node loss: 198523597079052397051904.0000 127357073417291280941056.0000 161580708438004486635520.0000 251423197966945921007616.0000\n",
      "Epoch: 7, edge loss: 1.0000, node loss: 197319487638738302926848.0000 126460345875451321778176.0000 160057094270669138427904.0000 251376726862505187475456.0000\n",
      "Epoch: 8, edge loss: 0.9999, node loss: 198396329373718164799488.0000 127109062243196667428864.0000 161305808046122860544000.0000 250419959053381670010880.0000\n",
      "Epoch: 9, edge loss: 1.0000, node loss: 197909962194158775959552.0000 126509650369648615489536.0000 160954449617689465847808.0000 250940386102408684503040.0000\n",
      "Epoch: 10, edge loss: 1.0000, node loss: 197767563244496612229120.0000 126476959298282906976256.0000 160569133044523335680000.0000 251521591342642396921856.0000\n",
      "Epoch: 11, edge loss: 1.0000, node loss: 196972370600769907326976.0000 126605127077714706038784.0000 160321821841874407653376.0000 251900287774203339341824.0000\n",
      "Epoch: 12, edge loss: 1.0000, node loss: 196888580475452492087296.0000 126509741270654424973312.0000 159872092988322183380992.0000 251392115794849445183488.0000\n",
      "Epoch: 13, edge loss: 1.0000, node loss: 197211404025408090275840.0000 126811426905309046636544.0000 160380036372679473758208.0000 250982007365335735336960.0000\n",
      "Epoch: 14, edge loss: 0.9999, node loss: 197031138905531892105216.0000 126342049757128556544000.0000 160341439180630443163648.0000 250841550663198814765056.0000\n",
      "Epoch: 15, edge loss: 1.0000, node loss: 196838424664062969249792.0000 126114562311630427783168.0000 159863399315636326760448.0000 251372702403298662023168.0000\n",
      "Epoch: 16, edge loss: 1.0000, node loss: 196685520565270506110976.0000 126214858631470324383744.0000 159646710691279713337344.0000 250100283884415660064768.0000\n",
      "Epoch: 17, edge loss: 0.9999, node loss: 196453816548410966147072.0000 126907076796783811100672.0000 159500192775191675273216.0000 251019922629464766283776.0000\n",
      "Epoch: 18, edge loss: 1.0000, node loss: 198493952223875346464768.0000 126316444734926476017664.0000 161311212869376987365376.0000 250811304726733703348224.0000\n",
      "Epoch: 19, edge loss: 0.9999, node loss: 196935386808314388021248.0000 126282701274682377633792.0000 159988863798629934039040.0000 250952293912732906815488.0000\n",
      "Epoch: 20, edge loss: 1.0000, node loss: 197876313208088145428480.0000 126942897384320615317504.0000 161271549411279975219200.0000 252066306238952945745920.0000\n",
      "Epoch: 21, edge loss: 0.9999, node loss: 197262622974540028837888.0000 126669275395108045324288.0000 160703884266487301537792.0000 251186543443996865200128.0000\n",
      "Epoch: 22, edge loss: 0.9999, node loss: 197260255990077397663744.0000 126430819802539003740160.0000 160031115390858729357312.0000 251129780721642064838656.0000\n",
      "Epoch: 23, edge loss: 1.0000, node loss: 196648315405803309760512.0000 126045354641057577435136.0000 159930787616299841224704.0000 250916675280640587857920.0000\n",
      "Epoch: 24, edge loss: 1.0000, node loss: 196706337168741084692480.0000 126634162531758977843200.0000 159871550431720665776128.0000 250993282072959652986880.0000\n",
      "Epoch: 25, edge loss: 0.9999, node loss: 197425889695700120240128.0000 126528245360173458128896.0000 160196280317030231441408.0000 250079724127665529028608.0000\n",
      "Epoch: 26, edge loss: 1.0001, node loss: 196678533995124752384000.0000 126361947467303949959168.0000 160572105162811861106688.0000 251141608063987657736192.0000\n",
      "Epoch: 27, edge loss: 1.0000, node loss: 197406510871908473372672.0000 126744247663260912844800.0000 160528429959228585672704.0000 251747365362371076816896.0000\n",
      "Epoch: 28, edge loss: 1.0000, node loss: 196731512373243595980800.0000 126269890391331148136448.0000 160096206036408115134464.0000 250753242756828215902208.0000\n",
      "Epoch: 29, edge loss: 1.0000, node loss: 197600822044886252388352.0000 126468801449141647441920.0000 160675575733545877897216.0000 249951634159673433653248.0000\n",
      "Epoch: 30, edge loss: 1.0000, node loss: 196508078270758908854272.0000 126340757931860992983040.0000 159668130279657314451456.0000 250504966751891333578752.0000\n",
      "Epoch: 31, edge loss: 1.0000, node loss: 196934633300298876059648.0000 126042772469399278321664.0000 159925890230460365144064.0000 250427001140979200688128.0000\n",
      "Epoch: 32, edge loss: 1.0000, node loss: 197143881340022140436480.0000 126358377074581152202752.0000 160329323871457018642432.0000 250852152804190197907456.0000\n",
      "Epoch: 33, edge loss: 1.0000, node loss: 197408715803872135741440.0000 127413203491399940440064.0000 160085246068868442488832.0000 251042667310732453871616.0000\n",
      "Epoch: 34, edge loss: 1.0000, node loss: 197195598539342464679936.0000 127019850070988496568320.0000 159961915489872573890560.0000 251074793380959829884928.0000\n",
      "Epoch: 35, edge loss: 1.0000, node loss: 197722107102149721718784.0000 126658780947115356979200.0000 160712188080099933814784.0000 251054926081077742141440.0000\n",
      "Epoch: 36, edge loss: 1.0000, node loss: 199209720701613420576768.0000 126560403624119565811712.0000 162391403275862805577728.0000 251114238828104256585728.0000\n",
      "Epoch: 37, edge loss: 1.0000, node loss: 196842612992532281294848.0000 126547705476628843855872.0000 159838669352316914630656.0000 250664778017104189194240.0000\n",
      "Epoch: 38, edge loss: 1.0001, node loss: 196472215556551197327360.0000 126177872448480736182272.0000 159465739019262944608256.0000 251271831721442398961664.0000\n",
      "Epoch: 39, edge loss: 1.0001, node loss: 197731658593587970441216.0000 126853059540351535022080.0000 160425561093151028936704.0000 250759074190283205771264.0000\n",
      "Epoch: 40, edge loss: 0.9999, node loss: 199233792093850261520384.0000 128196298074739768819712.0000 161977507664728390893568.0000 251171027448847375794176.0000\n",
      "Epoch: 41, edge loss: 1.0000, node loss: 197222210955485458202624.0000 127284710147686899122176.0000 160369939192473871974400.0000 251270674601881064439808.0000\n",
      "Epoch: 42, edge loss: 1.0000, node loss: 198340699624027231092736.0000 126648377933541840781312.0000 161691073645091905077248.0000 250178826648581664931840.0000\n",
      "Epoch: 43, edge loss: 1.0000, node loss: 197187561388308862861312.0000 126522984776102609682432.0000 160516827394347810947072.0000 250843114462017635745792.0000\n",
      "Epoch: 44, edge loss: 1.0000, node loss: 197563962283085153697792.0000 126633924766146760802304.0000 160359395308058651394048.0000 251003171462108455895040.0000\n",
      "Epoch: 45, edge loss: 1.0000, node loss: 198036893608105323003904.0000 127695796015770246316032.0000 160570388985927904002048.0000 250756834443739810234368.0000\n",
      "Epoch: 46, edge loss: 1.0000, node loss: 196575591301012139278336.0000 126431996884978289868800.0000 159532779839433285304320.0000 250650147121057140572160.0000\n",
      "Epoch: 47, edge loss: 0.9999, node loss: 197887291581079140433920.0000 126571004362327402217472.0000 160818315111941581832192.0000 250839839190997270528000.0000\n",
      "Epoch: 48, edge loss: 1.0000, node loss: 198052589284831380111360.0000 126793996952492484591616.0000 160876848112396278956032.0000 250776959812514011414528.0000\n",
      "Epoch: 49, edge loss: 1.0001, node loss: 197931381454124554911744.0000 127062666107429983354880.0000 160725966314191634563072.0000 251617310780340272365568.0000\n",
      "Epoch: 50, edge loss: 1.0001, node loss: 197123069964353408073728.0000 126709121165585660510208.0000 160804287996301003456512.0000 251051039833015650877440.0000\n",
      "Epoch: 51, edge loss: 1.0000, node loss: 198182641713439670534144.0000 126714972787566503264256.0000 160593116166324444200960.0000 250915415703836818407424.0000\n",
      "Epoch: 52, edge loss: 1.0000, node loss: 196619488297752908005376.0000 126592995635009731690496.0000 159549919999884938706944.0000 250609515001613658357760.0000\n",
      "Epoch: 53, edge loss: 1.0001, node loss: 198110299901908326809600.0000 127373510889281459060736.0000 160670000987723163238400.0000 252032609246966789963776.0000\n",
      "Epoch: 54, edge loss: 1.0000, node loss: 196643624352088324046848.0000 126245613640800131874816.0000 159872685708706830090240.0000 251142761902822855278592.0000\n",
      "Epoch: 55, edge loss: 1.0001, node loss: 197454385433335071506432.0000 126388329248963057156096.0000 160430858305173108817920.0000 252005990783600776183808.0000\n",
      "Epoch: 56, edge loss: 0.9999, node loss: 197893167413124877254656.0000 126449505594171635793920.0000 160987176455356677095424.0000 251264164212296925577216.0000\n",
      "Epoch: 57, edge loss: 1.0000, node loss: 197312302300292886560768.0000 126547098980259354116096.0000 160565109648868220338176.0000 252055475024764541599744.0000\n",
      "Epoch: 58, edge loss: 1.0000, node loss: 196646288011351864377344.0000 126221374171885697236992.0000 159838074011422195449856.0000 251172139689161059729408.0000\n",
      "Epoch: 59, edge loss: 1.0000, node loss: 197612702820043398840320.0000 126957208226620730507264.0000 160418787321012104986624.0000 250861826182452525137920.0000\n",
      "Epoch: 60, edge loss: 1.0000, node loss: 198804157887972114432000.0000 127213658758873513721856.0000 162777730581248430047232.0000 252321426658322718654464.0000\n",
      "Epoch: 61, edge loss: 1.0000, node loss: 196540113037463941808128.0000 126361835126590416093184.0000 159885935471000121507840.0000 251224317245967405416448.0000\n",
      "Epoch: 62, edge loss: 1.0000, node loss: 196899539790496605929472.0000 126420175933182064984064.0000 159862536014423981555712.0000 251005148368517612961792.0000\n",
      "Epoch: 63, edge loss: 1.0000, node loss: 197028548217215773573120.0000 126362443758484972371968.0000 160831701495082236510208.0000 250041703890361042075648.0000\n",
      "Epoch: 64, edge loss: 1.0000, node loss: 197945616023461377867776.0000 126633492421635320315904.0000 160853951519098243383296.0000 252098165853532882731008.0000\n",
      "Epoch: 65, edge loss: 1.0000, node loss: 198075182948004053123072.0000 126605528515250155945984.0000 160693773983965144678400.0000 251093236132621724942336.0000\n",
      "Epoch: 66, edge loss: 1.0001, node loss: 197214934837338990182400.0000 127545082887314660655104.0000 160335613628759603675136.0000 250556615397301179908096.0000\n",
      "Epoch: 67, edge loss: 1.0001, node loss: 197724538124640670711808.0000 126780540703197662019584.0000 160880760979071667535872.0000 250294811215436466618368.0000\n",
      "Epoch: 68, edge loss: 1.0000, node loss: 197998104434622813175808.0000 126469184519711064326144.0000 160947273601227396481024.0000 250468873802090008281088.0000\n",
      "Epoch: 69, edge loss: 0.9999, node loss: 197511181761316338532352.0000 127026653661285094260736.0000 160837020173365726412800.0000 251472299130989571997696.0000\n",
      "Epoch: 70, edge loss: 1.0001, node loss: 197222061631679712198656.0000 126360300485479296401408.0000 160458377322357810266112.0000 250274630923334656720896.0000\n",
      "Epoch: 71, edge loss: 0.9999, node loss: 197176604826309358518272.0000 126730745910999178543104.0000 160716718812569821773824.0000 250797259704547049209856.0000\n",
      "Epoch: 72, edge loss: 0.9999, node loss: 197378017172006159515648.0000 127095261645165041811456.0000 160331203199952241033216.0000 251017031447646178377728.0000\n",
      "Epoch: 73, edge loss: 0.9999, node loss: 196820250523810542911488.0000 126403237797191078641664.0000 160634435879801116950528.0000 250384809264024968495104.0000\n",
      "Epoch: 74, edge loss: 1.0000, node loss: 197520046591492898160640.0000 126436749482312314388480.0000 160524319222516919828480.0000 250791082903956837367808.0000\n",
      "Epoch: 75, edge loss: 1.0000, node loss: 197342425096061670391808.0000 127507759194742961733632.0000 160398279534280982921216.0000 251004188243890519670784.0000\n",
      "Epoch: 76, edge loss: 1.0000, node loss: 197913571327547434598400.0000 126371279875581964976128.0000 160793031776713884827648.0000 251499937842154681925632.0000\n",
      "Epoch: 77, edge loss: 1.0000, node loss: 196410466793769985376256.0000 126157009980702463098880.0000 159672291541960724840448.0000 250898426812628962639872.0000\n",
      "Epoch: 78, edge loss: 1.0000, node loss: 196606488646666657202176.0000 126476927473508409147392.0000 159819118315320631099392.0000 250470314721655214047232.0000\n",
      "Epoch: 79, edge loss: 1.0000, node loss: 197500952724369913675776.0000 126327896727861006958592.0000 160377844374118147817472.0000 250906439758861622575104.0000\n",
      "Epoch: 80, edge loss: 1.0000, node loss: 198509728462980125818880.0000 127362462750502232784896.0000 161038617026275336781824.0000 250988895047078543097856.0000\n",
      "Epoch: 81, edge loss: 1.0000, node loss: 197308649796070096764928.0000 126524995318903084154880.0000 160276769567252133445632.0000 250830156195619663773696.0000\n",
      "Epoch: 82, edge loss: 0.9999, node loss: 197755233416941303496704.0000 126726453285479251968000.0000 160678294071187962068992.0000 250347285144191249678336.0000\n",
      "Epoch: 83, edge loss: 1.0000, node loss: 197371396051423708839936.0000 126537277359289142345728.0000 160708455569137300668416.0000 250790291316427311808512.0000\n",
      "Epoch: 84, edge loss: 1.0001, node loss: 197120956497105868816384.0000 126408881945238713663488.0000 160010749214729428795392.0000 250505782748462964539392.0000\n",
      "Epoch: 85, edge loss: 1.0000, node loss: 197175501163890735054848.0000 127205751063635766018048.0000 160048765155014967558144.0000 251303712112238697381888.0000\n",
      "Epoch: 86, edge loss: 1.0001, node loss: 196859438857059907403776.0000 126506077582414542012416.0000 159678650760327453474816.0000 251143154775564606242816.0000\n",
      "Epoch: 87, edge loss: 0.9999, node loss: 197349951383084644433920.0000 126353839486462857314304.0000 160055591239212519653376.0000 251534837461348132061184.0000\n",
      "Epoch: 88, edge loss: 0.9999, node loss: 197548556481543440171008.0000 126457963476219951316992.0000 160325768860106133340160.0000 250624188470129506385920.0000\n",
      "Epoch: 89, edge loss: 1.0000, node loss: 197787407712181358166016.0000 126729727952315410808832.0000 160831087258604157272064.0000 251628944740691367428096.0000\n",
      "Epoch: 90, edge loss: 1.0001, node loss: 196959920447092548435968.0000 126143146665308472737792.0000 159655366790082956623872.0000 250645145198949837045760.0000\n",
      "Epoch: 91, edge loss: 1.0000, node loss: 196574258716647257800704.0000 126200062199711651594240.0000 159574826954143291146240.0000 250993976014260936900608.0000\n",
      "Epoch: 92, edge loss: 1.0001, node loss: 198152308372165762220032.0000 126675458437921281933312.0000 161362218546161948557312.0000 251390927640222723211264.0000\n",
      "Epoch: 93, edge loss: 1.0000, node loss: 197534444649216653393920.0000 126639535738456993955840.0000 160525844898891280941056.0000 252111066737520928620544.0000\n",
      "Epoch: 94, edge loss: 1.0000, node loss: 196672252438701643137024.0000 126263986383362438725632.0000 159991472659029051834368.0000 249838429680660450902016.0000\n",
      "Epoch: 95, edge loss: 1.0000, node loss: 197314756612944648208384.0000 127106454824651306565632.0000 160712501859104346603520.0000 251855377995929764233216.0000\n",
      "Epoch: 96, edge loss: 0.9999, node loss: 199067438881687308599296.0000 127549780934944425508864.0000 162139536914272403587072.0000 250008238259400523907072.0000\n",
      "Epoch: 97, edge loss: 1.0000, node loss: 197511867156657120215040.0000 126517665602423163453440.0000 161028416758206345248768.0000 250982155767248796516352.0000\n",
      "Epoch: 98, edge loss: 0.9999, node loss: 196591548656411608088576.0000 126438030180881354719232.0000 160112787217911474290688.0000 250449027136414644436992.0000\n",
      "Epoch: 99, edge loss: 1.0000, node loss: 197016532044786612305920.0000 125995199065902345617408.0000 160168323535339654217728.0000 250731481210423336763392.0000\n",
      "Epoch: 100, edge loss: 0.9999, node loss: 197556795810744958451712.0000 126577984503321477513216.0000 160792567372655907307520.0000 250234515398586101923840.0000\n",
      "Epoch: 101, edge loss: 1.0000, node loss: 197424902079229259153408.0000 126711730558462307860480.0000 160710419855173551652864.0000 250598894513074359762944.0000\n",
      "Epoch: 102, edge loss: 1.0000, node loss: 198476900583253546631168.0000 126893100760151361585152.0000 161705326145305927745536.0000 251226194539303481114624.0000\n",
      "Epoch: 103, edge loss: 1.0001, node loss: 196950639952099876536320.0000 126506869612466610896896.0000 160086256009975093526528.0000 250546344914253779042304.0000\n",
      "Epoch: 104, edge loss: 1.0000, node loss: 196975070854529366884352.0000 126205144930550441574400.0000 159984203788402632425472.0000 249846887830994636767232.0000\n",
      "Epoch: 105, edge loss: 1.0000, node loss: 197555430377358371061760.0000 127489084917001429712896.0000 161197288652093175889920.0000 251443823579657928704000.0000\n",
      "Epoch: 106, edge loss: 1.0000, node loss: 197919056524497035198464.0000 126811230176028716433408.0000 160796410143201107640320.0000 250702290003655632355328.0000\n",
      "Epoch: 107, edge loss: 1.0000, node loss: 199695357066620384051200.0000 128028645787917587841024.0000 162576433319132780298240.0000 251014449559012802297856.0000\n",
      "Epoch: 108, edge loss: 0.9999, node loss: 196916189961297418256384.0000 126462541244810758455296.0000 159894308078091373641728.0000 250307855662716941238272.0000\n",
      "Epoch: 109, edge loss: 1.0000, node loss: 196879640564421263097856.0000 126395219899923224854528.0000 159889984615781429673984.0000 251433645182940354707456.0000\n",
      "Epoch: 110, edge loss: 1.0000, node loss: 196655508200806007439360.0000 126502507533602996092928.0000 159471080240986984546304.0000 250438955408565544681472.0000\n",
      "Epoch: 111, edge loss: 1.0000, node loss: 196964333353698521513984.0000 126521724454391758454784.0000 159837419480828832907264.0000 251205957800838628900864.0000\n",
      "Epoch: 112, edge loss: 1.0000, node loss: 197993865278294172106752.0000 126863560295957185691648.0000 161026223414647925506048.0000 250977534721463590846464.0000\n",
      "Epoch: 113, edge loss: 1.0000, node loss: 197656277459416777752576.0000 126480098117604102635520.0000 160669853155720150646784.0000 250671446184276961263616.0000\n",
      "Epoch: 114, edge loss: 1.0000, node loss: 197625439697087206785024.0000 126095953228465017192448.0000 160755670346667757928448.0000 250938465412854482731008.0000\n",
      "Epoch: 115, edge loss: 1.0000, node loss: 197494623870909524475904.0000 126659996237041082826752.0000 160539058040018013519872.0000 250308092525620615970816.0000\n",
      "Epoch: 116, edge loss: 1.0001, node loss: 197642246218478969159680.0000 126452950402556598681600.0000 160104074258589411180544.0000 251181071879267858513920.0000\n",
      "Epoch: 117, edge loss: 1.0001, node loss: 196865234851280019521536.0000 125957145249291152916480.0000 160105503224080522805248.0000 250076860855531165712384.0000\n",
      "Epoch: 118, edge loss: 1.0001, node loss: 198016371343295734874112.0000 126365932536156487942144.0000 160523449805416301592576.0000 251303069093196902432768.0000\n",
      "Epoch: 119, edge loss: 1.0000, node loss: 196859380751968415776768.0000 126459253683993402408960.0000 160256635391542600663040.0000 250829904141898974494720.0000\n",
      "Epoch: 120, edge loss: 0.9999, node loss: 197763493950610573623296.0000 126744517308158715101184.0000 160438798702159039823872.0000 250147445683822652293120.0000\n",
      "Epoch: 121, edge loss: 0.9999, node loss: 198217808756379006009344.0000 126687102666245557714944.0000 161020516133386072358912.0000 251436142382377057910784.0000\n",
      "Epoch: 122, edge loss: 1.0001, node loss: 197089244158773027667968.0000 126584348436079677800448.0000 159774249924776708538368.0000 250942341598354297847808.0000\n",
      "Epoch: 123, edge loss: 1.0001, node loss: 197703471841840702947328.0000 127766812334176000278528.0000 161056194388926285217792.0000 251371117836685809811456.0000\n",
      "Epoch: 124, edge loss: 1.0000, node loss: 196988523837603214000128.0000 126956337296193653571584.0000 159829733933187701145600.0000 251614509732277908406272.0000\n",
      "Epoch: 125, edge loss: 1.0000, node loss: 197949074572738213969920.0000 126361396523128294735872.0000 161047296834659021750272.0000 250746004753756681404416.0000\n",
      "Epoch: 126, edge loss: 1.0000, node loss: 199121244704411012300800.0000 126683459362701323534336.0000 162598406276084832141312.0000 249840091538284085248000.0000\n",
      "Epoch: 127, edge loss: 1.0000, node loss: 198200489958951926890496.0000 127031100200539136196608.0000 160817209916778382098432.0000 251286955112896351174656.0000\n",
      "Epoch: 128, edge loss: 1.0001, node loss: 197731544275969676148736.0000 126023205337298282479616.0000 161089668092091978219520.0000 250278488868671107104768.0000\n",
      "Epoch: 129, edge loss: 1.0000, node loss: 198132369256597260599296.0000 126513316012918638116864.0000 161302353499785709748224.0000 250726320483906684452864.0000\n",
      "Epoch: 130, edge loss: 1.0000, node loss: 197855898003550723112960.0000 126292474203786209918976.0000 160770041132112882434048.0000 250180222146127761244160.0000\n",
      "Epoch: 131, edge loss: 1.0001, node loss: 197881884172407910957056.0000 126881342684220820029440.0000 160665168324810072129536.0000 250698856566216332410880.0000\n",
      "Epoch: 132, edge loss: 1.0000, node loss: 197968995796616696823808.0000 126599656097751804936192.0000 160720165086905724043264.0000 251889463887897915555840.0000\n",
      "Epoch: 133, edge loss: 1.0000, node loss: 198564796596485003149312.0000 128115061368287469240320.0000 161334672644476080488448.0000 251411313209202972819456.0000\n",
      "Epoch: 134, edge loss: 1.0001, node loss: 197407923617773251461120.0000 126526742233848813191168.0000 160124363860891601469440.0000 249827017354027730993152.0000\n",
      "Epoch: 135, edge loss: 1.0000, node loss: 197198334681293055328256.0000 126444327940437099675648.0000 159972406430204495396864.0000 250338414876590723825664.0000\n",
      "Epoch: 136, edge loss: 1.0000, node loss: 197303147306701627064320.0000 127055189741931535007744.0000 160656022963307119902720.0000 252228836324687334080512.0000\n",
      "Epoch: 137, edge loss: 0.9999, node loss: 198009393649803335827456.0000 127603686154217480781824.0000 160761924745683284262912.0000 250428055497521191976960.0000\n",
      "Epoch: 138, edge loss: 1.0001, node loss: 196694472256665541410816.0000 126195354924615670431744.0000 159528759787554628173824.0000 251471123175386009042944.0000\n",
      "Epoch: 139, edge loss: 1.0000, node loss: 197908906524203370414080.0000 126530762773987841802240.0000 160899994333904902291456.0000 249863741945491444727808.0000\n",
      "Epoch: 140, edge loss: 1.0000, node loss: 198862779617410835021824.0000 127091796502501356732416.0000 161307292943864798642176.0000 250595408676896810991616.0000\n",
      "Epoch: 141, edge loss: 1.0000, node loss: 197477489348145760436224.0000 126226155243316101251072.0000 160969496311788988268544.0000 250975215646059838570496.0000\n",
      "Epoch: 142, edge loss: 1.0000, node loss: 198700848887091592953856.0000 126589798725804047204352.0000 162127001039547645558784.0000 250136437271120276619264.0000\n",
      "Epoch: 143, edge loss: 1.0000, node loss: 198079546653778395529216.0000 126511928963540547796992.0000 160563498135171662610432.0000 251106919933832712945664.0000\n",
      "Epoch: 144, edge loss: 1.0000, node loss: 197271913142328875810816.0000 126759520873097738584064.0000 160061276871495524548608.0000 251306472359560092844032.0000\n",
      "Epoch: 145, edge loss: 1.0001, node loss: 198158601113190818381824.0000 126846013695816103886848.0000 161008768457261356941312.0000 251203819114836885766144.0000\n",
      "Epoch: 146, edge loss: 1.0000, node loss: 197001094436133873385472.0000 126050378594306345664512.0000 159693534079858287575040.0000 250457254455861206581248.0000\n",
      "Epoch: 147, edge loss: 1.0000, node loss: 197571365690620571025408.0000 126743671822484655570944.0000 160462260072000327254016.0000 250705122154629841289216.0000\n",
      "Epoch: 148, edge loss: 1.0000, node loss: 197232577854518083977216.0000 126990862247598776909824.0000 160211697377804954894336.0000 251426416379611559493632.0000\n",
      "Epoch: 149, edge loss: 0.9999, node loss: 197080655692280037900288.0000 126613139474157293010944.0000 160327074859430000459776.0000 251073457178859202936832.0000\n",
      "Epoch: 150, edge loss: 1.0000, node loss: 196601480929186974531584.0000 126054034012358127386624.0000 159465240304969855270912.0000 251197880255440811458560.0000\n",
      "Epoch: 151, edge loss: 1.0000, node loss: 197235753287870287183872.0000 126399517307798051356672.0000 160114967677899613667328.0000 251616446358492006055936.0000\n",
      "Epoch: 152, edge loss: 1.0000, node loss: 196508860768620673761280.0000 126176903117209226706944.0000 159495198711222336749568.0000 250619993811353799229440.0000\n",
      "Epoch: 153, edge loss: 1.0000, node loss: 198811557092069881151488.0000 128812405941869614202880.0000 161365523628204402671616.0000 252043315487786221436928.0000\n",
      "Epoch: 154, edge loss: 0.9999, node loss: 198194065513255639449600.0000 126953092520859502903296.0000 161509872617788991864832.0000 251146100681850528727040.0000\n",
      "Epoch: 155, edge loss: 1.0001, node loss: 198123391076549907185664.0000 126746614989412234166272.0000 161168096661231208759296.0000 251145320667753184493568.0000\n",
      "Epoch: 156, edge loss: 1.0000, node loss: 197429456655861181054976.0000 126965813217932041256960.0000 160244405879154013110272.0000 251891595631571563970560.0000\n",
      "Epoch: 157, edge loss: 1.0000, node loss: 197740060208773852037120.0000 126502572579033928695808.0000 160357529531772036972544.0000 250869682005697684832256.0000\n",
      "Epoch: 158, edge loss: 1.0000, node loss: 197221572383192654020608.0000 126868094066777636470784.0000 159960890847028108591104.0000 251145565556890247626752.0000\n",
      "Epoch: 159, edge loss: 0.9999, node loss: 197586780123515662106624.0000 126932521893589642379264.0000 160255338127096459296768.0000 251222399602016288505856.0000\n",
      "Epoch: 160, edge loss: 0.9999, node loss: 197162747324419119513600.0000 126524012053494342090752.0000 160533555111285395292160.0000 251346290825473506148352.0000\n",
      "Epoch: 161, edge loss: 1.0000, node loss: 197583073430578416058368.0000 126743888598983075954688.0000 160720121210381640138752.0000 251310125264778746134528.0000\n",
      "Epoch: 162, edge loss: 1.0000, node loss: 197082656252530255724544.0000 128000013296783815868416.0000 160668980901819671642112.0000 252132294974442565533696.0000\n",
      "Epoch: 163, edge loss: 1.0001, node loss: 197701124300528926326784.0000 126614909138012995584000.0000 160577850611203248750592.0000 251373194673408626917376.0000\n",
      "Epoch: 164, edge loss: 0.9999, node loss: 197253390378897160273920.0000 127204034743923342573568.0000 160471477610062509244416.0000 252203099408545836171264.0000\n",
      "Epoch: 165, edge loss: 1.0000, node loss: 197728418126385526079488.0000 126527597700552442511360.0000 160690794007655624998912.0000 250651675708745454714880.0000\n",
      "Epoch: 166, edge loss: 1.0000, node loss: 196320376122396886695936.0000 126541215773656992448512.0000 159600423571692665700352.0000 250207602131694050607104.0000\n",
      "Epoch: 167, edge loss: 1.0000, node loss: 197007717688615146356736.0000 126112081225839257059328.0000 160129236603852766052352.0000 249716769650533638078464.0000\n",
      "Epoch: 168, edge loss: 1.0000, node loss: 199671091571813087444992.0000 128446305477206640427008.0000 162329531222070269050880.0000 251602827683074865627136.0000\n",
      "Epoch: 169, edge loss: 1.0001, node loss: 196918149908788057997312.0000 126514275611267970367488.0000 159737598966114105163776.0000 251135589664471449075712.0000\n",
      "Epoch: 170, edge loss: 1.0000, node loss: 197517499162415278850048.0000 126548856720103921680384.0000 160497365056389562499072.0000 250182020655948496896000.0000\n",
      "Epoch: 171, edge loss: 0.9999, node loss: 197457152097408608370688.0000 126335024158905431228416.0000 160444892398936609783808.0000 250648224234448154001408.0000\n",
      "Epoch: 172, edge loss: 1.0000, node loss: 198369810925590836936704.0000 127548509724933401608192.0000 161803519114562250997760.0000 251886378677204135444480.0000\n",
      "Epoch: 173, edge loss: 1.0000, node loss: 198229570396937888202752.0000 126775773799857734549504.0000 161785610559183313174528.0000 250017082477946855227392.0000\n",
      "Epoch: 174, edge loss: 1.0000, node loss: 197007360134973273145344.0000 126040634240951574855680.0000 160029998551024571252736.0000 250386799893194643341312.0000\n",
      "Epoch: 175, edge loss: 1.0000, node loss: 197273322668763276902400.0000 126634209063184279535616.0000 160022129724542081302528.0000 251423077277728442941440.0000\n",
      "Epoch: 176, edge loss: 1.0000, node loss: 197438482903353727647744.0000 126877758896155075280896.0000 160733007849542572310528.0000 250342858896753383440384.0000\n",
      "Epoch: 177, edge loss: 1.0000, node loss: 197171957225195185373184.0000 126943109047420303966208.0000 160654270960580561993728.0000 251886677711306379755520.0000\n",
      "Epoch: 178, edge loss: 1.0000, node loss: 197276755551499463950336.0000 126828042923149822525440.0000 160490625992615629160448.0000 251179596487864363253760.0000\n",
      "Epoch: 179, edge loss: 1.0000, node loss: 197306945415575744020480.0000 126703318496796938862592.0000 159955985844841320808448.0000 252042578917633451622400.0000\n",
      "Epoch: 180, edge loss: 0.9999, node loss: 198292473221628691480576.0000 127836689629302581362688.0000 161978877556210735775744.0000 251046089450804366802944.0000\n",
      "Epoch: 181, edge loss: 1.0000, node loss: 198203409936084679786496.0000 126789835310716919218176.0000 160326883668992326631424.0000 251759715725638805487616.0000\n",
      "Epoch: 182, edge loss: 0.9999, node loss: 198560092250132489175040.0000 126922801798611067731968.0000 162408800362111126470656.0000 251681813216123252899840.0000\n",
      "Epoch: 183, edge loss: 1.0001, node loss: 198419317047698091147264.0000 126700151327842499559424.0000 161510939725951647350784.0000 251111215298944924909568.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize early stopping\n",
    "node_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=node_model_name)\n",
    "edge_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=edge_model_name)\n",
    "\n",
    "# Training loop\n",
    "edge_train_losses = []\n",
    "node_train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    edge_loss_cum = 0\n",
    "    node_loss_cum = np.zeros(n_node_features-1, dtype=float)\n",
    "    for batch_0 in train_loader:\n",
    "        #print()\n",
    "        # Clone batch of graphs\n",
    "        g_batch_0 = batch_0.clone()\n",
    "        \n",
    "        # Move batch data to GPU\n",
    "        g_batch_0 = g_batch_0.to(device)\n",
    "        \n",
    "        # Read number of graphs in batch\n",
    "        batch_size_0 = g_batch_0.num_graphs\n",
    "\n",
    "        # Save graph-level embeddings\n",
    "        embedding_batch_0 = []\n",
    "        for idx in range(batch_size_0):\n",
    "            embedding_batch_0.append(g_batch_0[idx].y.detach().to(device))\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        t_steps = np.arange(1, n_t_steps+1)\n",
    "        for t_step in t_steps:\n",
    "            # Read time step, which is added to node-level graph embeddings\n",
    "            t_step_std = torch.tensor([t_step / n_t_steps - 0.5], dtype=torch.float).to(device)  # Standard normalization\n",
    "        \n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            g_batch_t = []\n",
    "            e_batch_t = []\n",
    "            for idx in range(batch_size_0):\n",
    "                # Perform a diffusion step at time step t_step for each graph within the batch\n",
    "                graph_t, epsilon_t = diffusion_step(g_batch_0[idx], t_step, n_t_steps, alpha_decay)\n",
    "                \n",
    "                # Append noisy graphs and noises\n",
    "                g_batch_t.append(graph_t)\n",
    "                e_batch_t.append(epsilon_t)\n",
    "        \n",
    "                # Update diffused graph as next one\n",
    "                g_batch_0[idx] = graph_t.clone()\n",
    "            \n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add embeddings to noisy graphs (t_step information and graph-level embeddings)\n",
    "            for idx in range(batch_size_0):\n",
    "                # Add t_step information to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       t_step_std)  # To match graph.y shape, which is 1D\n",
    "                \n",
    "                # Add graph-level embedding to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       embedding_batch_0[idx])  # To match graph.y shape\n",
    "        \n",
    "            # Generate batch objects\n",
    "            g_batch_t = Batch.from_data_list(g_batch_t)\n",
    "            e_batch_t = Batch.from_data_list(e_batch_t)\n",
    "            \n",
    "            # Move data to device\n",
    "            g_batch_t = g_batch_t.to(device)\n",
    "            e_batch_t = e_batch_t.to(device)\n",
    "            \n",
    "            # Predict batch noise at given time step\n",
    "            pred_epsilon_t = predict_noise(g_batch_t, node_model, edge_model)\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the losses for node features and edge attributes\n",
    "            node_losses, edge_loss = get_graph_losses(e_batch_t, pred_epsilon_t, batch_size_0)\n",
    "            \n",
    "            # Combine losses for each attribute tensors\n",
    "            node_loss = torch.stack(node_losses).sum()\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            if not node_early_stopping.early_stop:\n",
    "                node_loss.backward(retain_graph=True)\n",
    "                node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            if not edge_early_stopping.early_stop:\n",
    "                edge_loss.backward(retain_graph=True)\n",
    "                edge_optimizer.step()\n",
    "            \n",
    "            # Get items\n",
    "            node_loss_cum += np.array([node_loss.item() for node_loss in node_losses])\n",
    "            edge_loss_cum += edge_loss.item()\n",
    "\n",
    "    # Compute the average train loss over n_t_steps\n",
    "    node_loss_cum /= (n_t_steps * len(train_loader))\n",
    "    edge_loss_cum /= (n_t_steps * len(train_loader))\n",
    "    \n",
    "    # Append average losses\n",
    "    node_train_losses.append(node_loss_cum)\n",
    "    edge_train_losses.append(edge_loss_cum)\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    node_early_stopping(node_loss_cum.sum(), node_model)\n",
    "    edge_early_stopping(edge_loss_cum,       edge_model)\n",
    "\n",
    "    if node_early_stopping.early_stop and edge_early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "    \n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Epoch: {epoch+1}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754680a5eda66bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:17:02.970608266Z",
     "start_time": "2024-05-06T13:17:02.969908611Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rescaled_edge_train_losses = np.sqrt(edge_train_losses) * dataset_parameters['edge_std'].numpy() + dataset_parameters['edge_mean'].numpy()\n",
    "rescaled_edge_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff196b7a5827b2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:17:02.970233758Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "node_train_losses = np.array(node_train_losses)\n",
    "rescaled_node_loss_cum = np.sqrt(node_train_losses) * dataset_parameters['feat_std'].numpy() + dataset_parameters['feat_mean'].numpy()\n",
    "rescaled_node_loss_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706862a0dcd940f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:17:02.970594059Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(edge_train_losses), label='Edge')\n",
    "for i in range(n_node_features-1):\n",
    "    plt.plot(np.log(np.array(node_train_losses)[:, i]), label=f'Node {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ea3aa52ce9eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:42:01.864410Z",
     "start_time": "2024-04-07T04:42:01.359900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.show()\n",
    "for i in range(n_node_features - 1):\n",
    "    plt.plot(np.log(np.array(node_train_losses)[:100, i]), label=f'Node {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0bb75cd4ddc33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:44:18.226988Z",
     "start_time": "2024-04-07T04:44:14.065652Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array(node_train_losses)[:, 0]), label=f'Atomic mass')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 1]), label=f'Charge')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 2]), label=f'Electronegativity')\n",
    "plt.plot(np.log(np.array(node_train_losses)[:, 3]), label=f'Ionization energy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Losses.eps', dpi=50, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c339357d9b80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Test of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfaa164497b70e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:17:02.971830153Z",
     "start_time": "2024-05-06T13:17:02.970747615Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, edge loss: 4.7859, node loss: 37.6537 37.6574 37.9346 37.8924\n",
      "Batch: 1, edge loss: 4.8326, node loss: 38.0566 38.0359 38.1299 37.8916\n",
      "Batch: 2, edge loss: 4.7907, node loss: 37.6465 37.9317 37.8782 38.0290\n",
      "Batch: 3, edge loss: 4.7789, node loss: 38.1181 37.5084 37.9452 38.0633\n",
      "Batch: 4, edge loss: 4.8557, node loss: 38.0005 37.5569 37.7483 37.4881\n",
      "Batch: 5, edge loss: 4.7783, node loss: 38.1335 38.0199 37.6441 37.9165\n",
      "Batch: 6, edge loss: 4.8262, node loss: 38.2474 37.7961 38.0659 37.6615\n",
      "Batch: 7, edge loss: 4.7893, node loss: 38.4973 37.5501 37.6360 38.1982\n",
      "Batch: 8, edge loss: 4.7810, node loss: 37.6647 38.7877 37.9740 38.0135\n",
      "Batch: 9, edge loss: 4.7747, node loss: 37.5929 37.9465 37.4057 37.0555\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "edge_test_losses = 0\n",
    "node_test_losses = np.zeros(n_node_features-1, dtype=float)\n",
    "idx = 0\n",
    "for batch_0 in test_loader:\n",
    "    # Move batch data to GPU\n",
    "    batch_0 = batch_0.to(device)\n",
    "    \n",
    "    # Read number of graphs in batch\n",
    "    batch_size = batch_0.num_graphs\n",
    "    \n",
    "    # Diffuse batch\n",
    "    g_batch_t = diffuse(batch_0, n_t_steps, s=alpha_decay)\n",
    "    \n",
    "    # Denoise batch\n",
    "    g_batch_0 = denoise(g_batch_t, n_t_steps, node_model, edge_model, n_graph_features,\n",
    "                        s=alpha_decay, sigma=sigma)\n",
    "    \n",
    "    # Calculate the loss for node features and edge attributes\n",
    "    node_losses, edge_loss = get_graph_losses(batch_0, g_batch_0, batch_size)\n",
    "    \n",
    "    # Get items\n",
    "    edge_loss_cum = edge_loss.item()\n",
    "    node_loss_cum = np.array([node_loss.item() for node_loss in node_losses])\n",
    "    \n",
    "    # Append average losses\n",
    "    edge_test_losses += edge_loss_cum\n",
    "    node_test_losses += node_loss_cum\n",
    "    \n",
    "    print_node_loss = ' '.join([f'{node_loss:.4f}' for node_loss in node_loss_cum])\n",
    "    print(f'Batch: {idx}, edge loss: {edge_loss_cum:.4f}, node loss: {print_node_loss}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6287db2b4fcb0e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:17:02.971161787Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "node_test_losses /= len(test_loader)\n",
    "edge_test_losses /= len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dec1d43c58f6cd3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:17:02.971237338Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.799348020553589,\n",
       " array([37.96112747, 37.87905006, 37.83618431, 37.82095413]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_test_losses, node_test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c910d3fb3556314",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9aff57d49941db",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:17:02.971303512Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and save as a dictionary\n",
    "model_performance = {\n",
    "    'edge_train_losses': edge_train_losses,\n",
    "    'node_train_losses': np.array(node_train_losses).tolist(),\n",
    "    'edge_test_losses':  edge_test_losses,\n",
    "    'node_test_losses':  node_test_losses.tolist()\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_performance.json', 'w') as json_file:\n",
    "    json.dump(model_performance, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
