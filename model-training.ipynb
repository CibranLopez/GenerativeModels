{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.630526408Z",
     "start_time": "2024-04-02T09:54:55.173444354Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy             as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torch_geometric.data   import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph, predict_noise, diffuse, denoise, EarlyStopping\n",
    "from libraries.dataset      import standardize_dataset, get_datasets\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.651302235Z",
     "start_time": "2024-04-02T09:54:56.629491400Z"
    }
   },
   "id": "74450972cebeaa56",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ad446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.667397586Z",
     "start_time": "2024-04-02T09:54:56.629876915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs\n",
    "# The models is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property\n",
    "# Although denoising includes noise, I think it is better not to add it when training"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'models/GM_PT_EPA-voronoi/GM_v0'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'GM_PT_EPA-voronoi'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "# Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "i = 0\n",
    "while True:\n",
    "    target_folder = f'{general_folder}/GM_v{i}'\n",
    "    if not os.path.exists(target_folder):\n",
    "        # Copy all data\n",
    "        os.system(f'cp -r {data_folder} {target_folder}')\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'\n",
    "target_folder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.699930009Z",
     "start_time": "2024-04-02T09:54:56.630062174Z"
    }
   },
   "id": "4d31022ebdd0dbf4",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 1000\n",
    "batch_size    = 256\n",
    "learning_rate = 0.0001\n",
    "patience      = 20\n",
    "delta         = 0.2\n",
    "check_labels  = True  # Whether to train-test split attending to labels or not\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 100\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = 0  # Zero for training purposes\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':        data_folder,\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'patience':           patience,\n",
    "    'delta':              delta,\n",
    "    'check_labels':       check_labels,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'sigma':              sigma,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.730249617Z",
     "start_time": "2024-04-02T09:54:56.630264876Z"
    }
   },
   "id": "ffa9ac369c3a9006",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load of graph database for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeb3c2d70eb68bdb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset, already standardized."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4b0adac6b66bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "if os.path.exists(dataset_name_std) and os.path.exists(dataset_parameters_name_std) and os.path.exists(labels_name):\n",
    "    # Load the standardized dataset, with corresponding labels and parameters\n",
    "    dataset = torch.load(dataset_name_std)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Load the data from the JSON file\n",
    "    with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "        numpy_dict = json.load(json_file)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    dataset_parameters = {}\n",
    "    for key, value in numpy_dict.items():\n",
    "        try:\n",
    "            dataset_parameters[key] = torch.tensor(value)\n",
    "        except:\n",
    "            dataset_parameters[key] = value\n",
    "\n",
    "elif os.path.exists(dataset_name) and os.path.exists(labels_name):\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(dataset_name)\n",
    "    labels  = torch.load(labels_name)\n",
    "    \n",
    "    # Standardize dataset\n",
    "    dataset, dataset_parameters = standardize_dataset(dataset)\n",
    "    \n",
    "    # Save standardized dataset\n",
    "    torch.save(dataset, dataset_name_std)\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    numpy_dict = {key: value.cpu().numpy().tolist() for key, value in dataset_parameters.items()}\n",
    "\n",
    "    # Dump the dictionary with numpy arrays to a JSON file\n",
    "    with open(dataset_parameters_name_std, 'w') as json_file:\n",
    "        json.dump(numpy_dict, json_file)\n",
    "\n",
    "else:\n",
    "    sys.exit('Error: the database is not available')\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.778982979Z",
     "start_time": "2024-04-02T09:54:56.630420869Z"
    }
   },
   "id": "bf186ea431a29ccc",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split in train, validation and test sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4295b379d810af37"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   graphs: 1095\n",
      "Number of validation graphs: 138\n",
      "Number of testing    graphs: 136\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio  = 0.1\n",
    "\n",
    "# Check if data has been already split, else do it randomly\n",
    "path_to_train_labels = f'{target_folder}/train_labels.txt'\n",
    "path_to_val_labels   = f'{target_folder}/validation_labels.txt'\n",
    "path_to_test_labels  = f'{target_folder}/test_labels.txt'\n",
    "\n",
    "# Copy labels\n",
    "material_labels = labels.copy()\n",
    "\n",
    "if os.path.exists(path_to_train_labels) and os.path.exists(path_to_val_labels) and os.path.exists(path_to_test_labels):\n",
    "    # Read labels splitting (which are strings)\n",
    "    train_labels = np.genfromtxt(path_to_train_labels, dtype='str').tolist()\n",
    "    val_labels   = np.genfromtxt(path_to_val_labels,   dtype='str').tolist()\n",
    "    test_labels  = np.genfromtxt(path_to_test_labels,  dtype='str').tolist()\n",
    "else:\n",
    "    if check_labels:\n",
    "        # Splitting into train-test sets considering that Fvs from the same materials must be in the same dataset\n",
    "        material_labels = [label.split()[0] for label in material_labels]\n",
    "        \n",
    "        # Define unique labels\n",
    "        unique_labels = np.unique(material_labels)\n",
    "    else:\n",
    "        # Completely randomly splitting\n",
    "        # Copy material_labels\n",
    "        unique_labels = material_labels.copy()\n",
    "    \n",
    "    # Shuffle the list of unique labels\n",
    "    np.random.shuffle(unique_labels)\n",
    "\n",
    "    # Define the sizes of the train and test sets\n",
    "    # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "    train_size = int(train_ratio * len(unique_labels))\n",
    "    test_size  = int(test_ratio  * len(unique_labels))\n",
    "    \n",
    "    train_labels = unique_labels[:train_size]\n",
    "    val_labels   = unique_labels[train_size:-test_size]\n",
    "    test_labels  = unique_labels[-test_size:]\n",
    "\n",
    "    # Save this splitting for transfer-learning approaches\n",
    "    np.savetxt(path_to_train_labels, train_labels, fmt='%s')\n",
    "    np.savetxt(path_to_val_labels,   val_labels,   fmt='%s')\n",
    "    np.savetxt(path_to_test_labels,  test_labels,  fmt='%s')\n",
    "\n",
    "# Use the computed indexes to generate train and test sets\n",
    "# We iteratively check where labels equals a unique train/test labels and append the index to a list\n",
    "train_dataset = get_datasets(train_labels, material_labels, dataset)\n",
    "val_dataset   = get_datasets(val_labels,   material_labels, dataset)\n",
    "test_dataset  = get_datasets(test_labels,  material_labels, dataset)\n",
    "\n",
    "del dataset  # Free up CUDA memory\n",
    "\n",
    "print(f'Number of training   graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of testing    graphs: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.829475512Z",
     "start_time": "2024-04-02T09:54:56.711710068Z"
    }
   },
   "id": "55178ef4803e580c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for graph in train_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for graph in val_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for graph in test_dataset:\n",
    "    graph.y = torch.tensor([0], dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.845662223Z",
     "start_time": "2024-04-02T09:54:56.827977783Z"
    }
   },
   "id": "e435f7665da018fd",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define data loaders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "549a2953188746d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(train_dataset[0].y)\n",
    "\n",
    "del train_dataset, val_dataset, test_dataset  # Free up CUDA memory"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:56.861912184Z",
     "start_time": "2024-04-02T09:54:56.838771255Z"
    }
   },
   "id": "86402c04-4b6c-4dfb-b112-6574fc89380d",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definition of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28881dd9d41fdf8e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1591eccef168173b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:54:57.270675509Z",
     "start_time": "2024-04-02T09:54:56.841788864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(6, 256)\n",
      "  (conv2): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "\n",
    "# Moving models to device\n",
    "node_model = node_model.to(device)\n",
    "edge_model = edge_model.to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "    \n",
    "    # Evaluate model state\n",
    "    node_model.eval()\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training of the model"
   ],
   "metadata": {},
   "id": "31a76fc0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, total loss: 0.4932, edge loss: 0.0324, node loss: 0.4608\n",
      "Epoch: 2, total loss: 0.8471, edge loss: 0.0298, node loss: 0.8173\n",
      "Epoch: 3, total loss: 0.7504, edge loss: 0.0298, node loss: 0.7206\n",
      "Epoch: 4, total loss: 0.3424, edge loss: 0.0297, node loss: 0.3127\n",
      "Epoch: 5, total loss: 0.2499, edge loss: 0.0297, node loss: 0.2201\n",
      "Epoch: 6, total loss: 0.4498, edge loss: 0.0297, node loss: 0.4201\n",
      "Epoch: 7, total loss: 0.1873, edge loss: 0.0298, node loss: 0.1576\n",
      "Epoch: 8, total loss: 0.1091, edge loss: 0.0297, node loss: 0.0794\n",
      "Epoch: 9, total loss: 0.2353, edge loss: 0.0298, node loss: 0.2056\n",
      "Epoch: 10, total loss: 0.3503, edge loss: 0.0297, node loss: 0.3206\n",
      "Epoch: 11, total loss: 0.2312, edge loss: 0.0297, node loss: 0.2015\n",
      "Epoch: 12, total loss: 0.0842, edge loss: 0.0297, node loss: 0.0545\n",
      "Epoch: 13, total loss: 0.0850, edge loss: 0.0297, node loss: 0.0553\n",
      "Epoch: 14, total loss: 0.5089, edge loss: 0.0297, node loss: 0.4792\n",
      "Epoch: 15, total loss: 0.1590, edge loss: 0.0297, node loss: 0.1293\n",
      "Epoch: 16, total loss: 0.1190, edge loss: 0.0297, node loss: 0.0893\n",
      "Epoch: 17, total loss: 0.0814, edge loss: 0.0297, node loss: 0.0517\n",
      "Epoch: 18, total loss: 0.1078, edge loss: 0.0297, node loss: 0.0781\n",
      "Epoch: 19, total loss: 0.0967, edge loss: 0.0297, node loss: 0.0670\n",
      "Epoch: 20, total loss: 0.0827, edge loss: 0.0297, node loss: 0.0530\n",
      "Epoch: 21, total loss: 0.0814, edge loss: 0.0297, node loss: 0.0517\n",
      "Epoch: 22, total loss: 0.0745, edge loss: 0.0297, node loss: 0.0448\n",
      "Epoch: 23, total loss: 0.0763, edge loss: 0.0297, node loss: 0.0466\n",
      "Epoch: 24, total loss: 0.0911, edge loss: 0.0297, node loss: 0.0615\n",
      "Epoch: 25, total loss: 0.0842, edge loss: 0.0297, node loss: 0.0545\n",
      "Epoch: 26, total loss: 0.1344, edge loss: 0.0297, node loss: 0.1047\n",
      "Epoch: 27, total loss: 0.1112, edge loss: 0.0297, node loss: 0.0815\n",
      "Epoch: 28, total loss: 0.0671, edge loss: 0.0297, node loss: 0.0374\n",
      "Epoch: 29, total loss: 0.0621, edge loss: 0.0297, node loss: 0.0323\n",
      "Epoch: 30, total loss: 0.0733, edge loss: 0.0297, node loss: 0.0436\n",
      "Epoch: 31, total loss: 0.0700, edge loss: 0.0297, node loss: 0.0403\n",
      "Epoch: 32, total loss: 0.0630, edge loss: 0.0297, node loss: 0.0333\n",
      "Epoch: 33, total loss: 0.0634, edge loss: 0.0297, node loss: 0.0337\n",
      "Epoch: 34, total loss: 0.0610, edge loss: 0.0297, node loss: 0.0313\n",
      "Epoch: 35, total loss: 0.0617, edge loss: 0.0297, node loss: 0.0320\n",
      "Epoch: 36, total loss: 0.0672, edge loss: 0.0297, node loss: 0.0375\n",
      "Epoch: 37, total loss: 0.0660, edge loss: 0.0297, node loss: 0.0363\n",
      "Epoch: 38, total loss: 0.0617, edge loss: 0.0297, node loss: 0.0320\n",
      "Epoch: 39, total loss: 0.0621, edge loss: 0.0297, node loss: 0.0324\n",
      "Epoch: 40, total loss: 0.0605, edge loss: 0.0297, node loss: 0.0308\n",
      "Epoch: 41, total loss: 0.0623, edge loss: 0.0297, node loss: 0.0327\n",
      "Epoch: 42, total loss: 0.0624, edge loss: 0.0297, node loss: 0.0327\n",
      "Epoch: 43, total loss: 0.0596, edge loss: 0.0297, node loss: 0.0299\n",
      "Epoch: 44, total loss: 0.0591, edge loss: 0.0297, node loss: 0.0294\n",
      "Epoch: 45, total loss: 0.0607, edge loss: 0.0297, node loss: 0.0310\n",
      "Epoch: 46, total loss: 0.0600, edge loss: 0.0297, node loss: 0.0303\n",
      "Epoch: 47, total loss: 0.0614, edge loss: 0.0297, node loss: 0.0317\n",
      "Epoch: 48, total loss: 0.0628, edge loss: 0.0297, node loss: 0.0331\n",
      "Epoch: 49, total loss: 0.0625, edge loss: 0.0297, node loss: 0.0327\n",
      "Epoch: 50, total loss: 0.0591, edge loss: 0.0297, node loss: 0.0293\n",
      "Epoch: 51, total loss: 0.0588, edge loss: 0.0297, node loss: 0.0291\n",
      "Epoch: 52, total loss: 0.0581, edge loss: 0.0297, node loss: 0.0284\n",
      "Epoch: 53, total loss: 0.0580, edge loss: 0.0297, node loss: 0.0283\n",
      "Epoch: 54, total loss: 0.0579, edge loss: 0.0297, node loss: 0.0282\n",
      "Epoch: 55, total loss: 0.0579, edge loss: 0.0297, node loss: 0.0282\n",
      "Epoch: 56, total loss: 0.0584, edge loss: 0.0297, node loss: 0.0286\n",
      "Epoch: 57, total loss: 0.0575, edge loss: 0.0297, node loss: 0.0278\n",
      "Epoch: 58, total loss: 0.0579, edge loss: 0.0297, node loss: 0.0282\n",
      "Epoch: 59, total loss: 0.0595, edge loss: 0.0297, node loss: 0.0298\n",
      "Epoch: 60, total loss: 0.0594, edge loss: 0.0297, node loss: 0.0296\n",
      "Epoch: 61, total loss: 0.0577, edge loss: 0.0297, node loss: 0.0279\n",
      "Epoch: 62, total loss: 0.0576, edge loss: 0.0297, node loss: 0.0279\n",
      "Epoch: 63, total loss: 0.0570, edge loss: 0.0297, node loss: 0.0273\n",
      "Epoch: 64, total loss: 0.0573, edge loss: 0.0297, node loss: 0.0276\n",
      "Epoch: 65, total loss: 0.0569, edge loss: 0.0297, node loss: 0.0272\n",
      "Epoch: 66, total loss: 0.0573, edge loss: 0.0297, node loss: 0.0276\n",
      "Epoch: 67, total loss: 0.0569, edge loss: 0.0297, node loss: 0.0272\n",
      "Epoch: 68, total loss: 0.0564, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 69, total loss: 0.0573, edge loss: 0.0297, node loss: 0.0275\n",
      "Epoch: 70, total loss: 0.0570, edge loss: 0.0297, node loss: 0.0273\n",
      "Epoch: 71, total loss: 0.0563, edge loss: 0.0297, node loss: 0.0266\n",
      "Epoch: 72, total loss: 0.0572, edge loss: 0.0297, node loss: 0.0275\n",
      "Epoch: 73, total loss: 0.0564, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 74, total loss: 0.0565, edge loss: 0.0297, node loss: 0.0268\n",
      "Epoch: 75, total loss: 0.0565, edge loss: 0.0297, node loss: 0.0268\n",
      "Epoch: 76, total loss: 0.0571, edge loss: 0.0297, node loss: 0.0274\n",
      "Epoch: 77, total loss: 0.0576, edge loss: 0.0297, node loss: 0.0279\n",
      "Epoch: 78, total loss: 0.0562, edge loss: 0.0297, node loss: 0.0265\n",
      "Epoch: 79, total loss: 0.0559, edge loss: 0.0297, node loss: 0.0261\n",
      "Epoch: 80, total loss: 0.0567, edge loss: 0.0297, node loss: 0.0270\n",
      "Epoch: 81, total loss: 0.0562, edge loss: 0.0297, node loss: 0.0265\n",
      "Epoch: 82, total loss: 0.0558, edge loss: 0.0297, node loss: 0.0261\n",
      "Epoch: 83, total loss: 0.0554, edge loss: 0.0297, node loss: 0.0257\n",
      "Epoch: 84, total loss: 0.0564, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 85, total loss: 0.0570, edge loss: 0.0297, node loss: 0.0272\n",
      "Epoch: 86, total loss: 0.0560, edge loss: 0.0297, node loss: 0.0263\n",
      "Epoch: 87, total loss: 0.0557, edge loss: 0.0297, node loss: 0.0260\n",
      "Epoch: 88, total loss: 0.0552, edge loss: 0.0297, node loss: 0.0255\n",
      "Epoch: 89, total loss: 0.0559, edge loss: 0.0297, node loss: 0.0262\n",
      "Epoch: 90, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 91, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0258\n",
      "Epoch: 92, total loss: 0.0557, edge loss: 0.0297, node loss: 0.0260\n",
      "Epoch: 93, total loss: 0.0566, edge loss: 0.0297, node loss: 0.0269\n",
      "Epoch: 94, total loss: 0.0560, edge loss: 0.0297, node loss: 0.0263\n",
      "Epoch: 95, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 96, total loss: 0.0553, edge loss: 0.0297, node loss: 0.0256\n",
      "Epoch: 97, total loss: 0.0563, edge loss: 0.0297, node loss: 0.0265\n",
      "Epoch: 98, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 99, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 100, total loss: 0.0562, edge loss: 0.0297, node loss: 0.0265\n",
      "Epoch: 101, total loss: 0.0557, edge loss: 0.0297, node loss: 0.0260\n",
      "Epoch: 102, total loss: 0.0568, edge loss: 0.0297, node loss: 0.0271\n",
      "Epoch: 103, total loss: 0.0580, edge loss: 0.0297, node loss: 0.0283\n",
      "Epoch: 104, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 105, total loss: 0.0552, edge loss: 0.0297, node loss: 0.0255\n",
      "Epoch: 106, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 107, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 108, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 109, total loss: 0.0565, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 110, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 111, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 112, total loss: 0.0562, edge loss: 0.0297, node loss: 0.0265\n",
      "Epoch: 113, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 114, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 115, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 116, total loss: 0.0554, edge loss: 0.0297, node loss: 0.0257\n",
      "Epoch: 117, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 118, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 119, total loss: 0.0564, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 120, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 121, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0258\n",
      "Epoch: 122, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 123, total loss: 0.0549, edge loss: 0.0297, node loss: 0.0252\n",
      "Epoch: 124, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 125, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 126, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 127, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 128, total loss: 0.0550, edge loss: 0.0297, node loss: 0.0253\n",
      "Epoch: 129, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 130, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0258\n",
      "Epoch: 131, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 132, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 133, total loss: 0.0564, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 134, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 135, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 136, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 137, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 138, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 139, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 140, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 141, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0258\n",
      "Epoch: 142, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 143, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 144, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 145, total loss: 0.0552, edge loss: 0.0297, node loss: 0.0255\n",
      "Epoch: 146, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0258\n",
      "Epoch: 147, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 148, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 149, total loss: 0.0550, edge loss: 0.0297, node loss: 0.0253\n",
      "Epoch: 150, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 151, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 152, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 153, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 154, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 155, total loss: 0.0558, edge loss: 0.0297, node loss: 0.0261\n",
      "Epoch: 156, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 157, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 158, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 159, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 160, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 161, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 162, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 163, total loss: 0.0552, edge loss: 0.0297, node loss: 0.0255\n",
      "Epoch: 164, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 165, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 166, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 167, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 168, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 169, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 170, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 171, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 172, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 173, total loss: 0.0549, edge loss: 0.0297, node loss: 0.0252\n",
      "Epoch: 174, total loss: 0.0552, edge loss: 0.0297, node loss: 0.0255\n",
      "Epoch: 175, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 176, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 177, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 178, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 179, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 180, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 181, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 182, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 183, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 184, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 185, total loss: 0.0549, edge loss: 0.0297, node loss: 0.0252\n",
      "Epoch: 186, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 187, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 188, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 189, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 190, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 191, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 192, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 193, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 194, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 195, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 196, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 197, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 198, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 199, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 200, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 201, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 202, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 203, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 204, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 205, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 206, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 207, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 208, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 209, total loss: 0.0550, edge loss: 0.0297, node loss: 0.0253\n",
      "Epoch: 210, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 211, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 212, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 213, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 214, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 215, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 216, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 217, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 218, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 219, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 220, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 221, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 222, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 223, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 224, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 225, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 226, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 227, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 228, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 229, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 230, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 231, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 232, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 233, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 234, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 235, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 236, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 237, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 238, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 239, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 240, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 241, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 242, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 243, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 244, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 245, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 246, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 247, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 248, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 249, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 250, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 251, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 252, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 253, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 254, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 255, total loss: 0.0553, edge loss: 0.0297, node loss: 0.0256\n",
      "Epoch: 256, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 257, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 258, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 259, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 260, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 261, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 262, total loss: 0.0554, edge loss: 0.0297, node loss: 0.0257\n",
      "Epoch: 263, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 264, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 265, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 266, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 267, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 268, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 269, total loss: 0.0553, edge loss: 0.0297, node loss: 0.0256\n",
      "Epoch: 270, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 271, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 272, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 273, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 274, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 275, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 276, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 277, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 278, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 279, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 280, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 281, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 282, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 283, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 284, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 285, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 286, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 287, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 288, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 289, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 290, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 291, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 292, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 293, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 294, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 295, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 296, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 297, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 298, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 299, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 300, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 301, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 302, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 303, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 304, total loss: 0.0546, edge loss: 0.0297, node loss: 0.0249\n",
      "Epoch: 305, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 306, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 307, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 308, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 309, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 310, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 311, total loss: 0.0564, edge loss: 0.0297, node loss: 0.0267\n",
      "Epoch: 312, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 313, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 314, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 315, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 316, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 317, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 318, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 319, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 320, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 321, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 322, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 323, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 324, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 325, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 326, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 327, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 328, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 329, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 330, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 331, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 332, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 333, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 334, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 335, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 336, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 337, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 338, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 339, total loss: 0.0547, edge loss: 0.0297, node loss: 0.0250\n",
      "Epoch: 340, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 341, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 342, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 343, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 344, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 345, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 346, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 347, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 348, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 349, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 350, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 351, total loss: 0.0556, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 352, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 353, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 354, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 355, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 356, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 357, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 358, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 359, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 360, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 361, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 362, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 363, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 364, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 365, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 366, total loss: 0.0544, edge loss: 0.0297, node loss: 0.0247\n",
      "Epoch: 367, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 368, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 369, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 370, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 371, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 372, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 373, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 374, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 375, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 376, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 377, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 378, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 379, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 380, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 381, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 382, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 383, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 384, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 385, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 386, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 387, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 388, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 389, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 390, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 391, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0259\n",
      "Epoch: 392, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 393, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 394, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 395, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 396, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 397, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 398, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 399, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 400, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 401, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 402, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 403, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 404, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 405, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 406, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 407, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 408, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 409, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 410, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 411, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 412, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 413, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 414, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 415, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 416, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 417, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 418, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 419, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 420, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 421, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 422, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 423, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 424, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 425, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 426, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 427, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 428, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 429, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 430, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 431, total loss: 0.0555, edge loss: 0.0297, node loss: 0.0258\n",
      "Epoch: 432, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 433, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 434, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 435, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 436, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 437, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 438, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 439, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 440, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 441, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 442, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 443, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 444, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 445, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 446, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 447, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 448, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 449, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 450, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 451, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 452, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 453, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 454, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 455, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 456, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 457, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 458, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 459, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 460, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 461, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 462, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 463, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 464, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 465, total loss: 0.0545, edge loss: 0.0297, node loss: 0.0248\n",
      "Epoch: 466, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 467, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 468, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 469, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 470, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 471, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 472, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 473, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 474, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 475, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 476, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 477, total loss: 0.0551, edge loss: 0.0297, node loss: 0.0254\n",
      "Epoch: 478, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 479, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 480, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 481, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 482, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 483, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 484, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 485, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 486, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 487, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 488, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 489, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 490, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 491, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 492, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 493, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 494, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 495, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 496, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 497, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 498, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 499, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 500, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 501, total loss: 0.0543, edge loss: 0.0297, node loss: 0.0246\n",
      "Epoch: 502, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 503, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 504, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 505, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 506, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 507, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 508, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 509, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 510, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 511, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 512, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 513, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 514, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 515, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 516, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 517, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 518, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 519, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 520, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 521, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 522, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 523, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 524, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 525, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 526, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 527, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 528, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 529, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 530, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 531, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 532, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 533, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 534, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 535, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 536, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 537, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 538, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 539, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 540, total loss: 0.0537, edge loss: 0.0297, node loss: 0.0241\n",
      "Epoch: 541, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 542, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 543, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 544, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 545, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 546, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 547, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 548, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 549, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 550, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 551, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 552, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 553, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 554, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 555, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 556, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 557, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 558, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 559, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 560, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 561, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 562, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 563, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 564, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 565, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 566, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 567, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 568, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 569, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 570, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 571, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 572, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 573, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 574, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 575, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 576, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 577, total loss: 0.0542, edge loss: 0.0297, node loss: 0.0245\n",
      "Epoch: 578, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 579, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 580, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 581, total loss: 0.0538, edge loss: 0.0297, node loss: 0.0240\n",
      "Epoch: 582, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 583, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 584, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 585, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 586, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 587, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 588, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 589, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 590, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 591, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 592, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 593, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 594, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 595, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 596, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 597, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 598, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 599, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 600, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 601, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 602, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 603, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 604, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 605, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 606, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 607, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 608, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 609, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 610, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 611, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 612, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 613, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 614, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 615, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 616, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 617, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 618, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 619, total loss: 0.0534, edge loss: 0.0297, node loss: 0.0237\n",
      "Epoch: 620, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 621, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 622, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 623, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 624, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 625, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 626, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 627, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 628, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 629, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 630, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 631, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 632, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 633, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 634, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 635, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 636, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 637, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 638, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 639, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 640, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 641, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 642, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 643, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 644, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 645, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 646, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 647, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 648, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 649, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 650, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 651, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 652, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 653, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 654, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 655, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 656, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 657, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 658, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 659, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 660, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 661, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 662, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 663, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 664, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 665, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 666, total loss: 0.0539, edge loss: 0.0297, node loss: 0.0242\n",
      "Epoch: 667, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 668, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 669, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 670, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 671, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 672, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 673, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 674, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 675, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 676, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 677, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 678, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 679, total loss: 0.0496, edge loss: 0.0297, node loss: 0.0199\n",
      "Epoch: 680, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 681, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 682, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 683, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 684, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 685, total loss: 0.0496, edge loss: 0.0297, node loss: 0.0199\n",
      "Epoch: 686, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 687, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 688, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 689, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 690, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 691, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 692, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 693, total loss: 0.0540, edge loss: 0.0297, node loss: 0.0243\n",
      "Epoch: 694, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 695, total loss: 0.0499, edge loss: 0.0297, node loss: 0.0202\n",
      "Epoch: 696, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 697, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 698, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 699, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 700, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 701, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 702, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 703, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 704, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 705, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 706, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 707, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 708, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 709, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 710, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 711, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 712, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 713, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 714, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 715, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 716, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 717, total loss: 0.0499, edge loss: 0.0297, node loss: 0.0202\n",
      "Epoch: 718, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 719, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 720, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 721, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 722, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 723, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 724, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 725, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 726, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 727, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 728, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 729, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 730, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 731, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 732, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 733, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 734, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 735, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 736, total loss: 0.0498, edge loss: 0.0297, node loss: 0.0201\n",
      "Epoch: 737, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 738, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 739, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 740, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 741, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 742, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 743, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 744, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 745, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 746, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 747, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 748, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 749, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 750, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 751, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 752, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 753, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 754, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 755, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 756, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0204\n",
      "Epoch: 757, total loss: 0.0533, edge loss: 0.0297, node loss: 0.0236\n",
      "Epoch: 758, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 759, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 760, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 761, total loss: 0.0498, edge loss: 0.0297, node loss: 0.0201\n",
      "Epoch: 762, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 763, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 764, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 765, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 766, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 767, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 768, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 769, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 770, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 771, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 772, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 773, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 774, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 775, total loss: 0.0531, edge loss: 0.0297, node loss: 0.0234\n",
      "Epoch: 776, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 777, total loss: 0.0497, edge loss: 0.0297, node loss: 0.0200\n",
      "Epoch: 778, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 779, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 780, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 781, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 782, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 783, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 784, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 785, total loss: 0.0499, edge loss: 0.0297, node loss: 0.0202\n",
      "Epoch: 786, total loss: 0.0498, edge loss: 0.0297, node loss: 0.0201\n",
      "Epoch: 787, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 788, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 789, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 790, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 791, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 792, total loss: 0.0532, edge loss: 0.0297, node loss: 0.0235\n",
      "Epoch: 793, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 794, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 795, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 796, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 797, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 798, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 799, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 800, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 801, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 802, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 803, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0204\n",
      "Epoch: 804, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 805, total loss: 0.0548, edge loss: 0.0297, node loss: 0.0251\n",
      "Epoch: 806, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 807, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 808, total loss: 0.0497, edge loss: 0.0297, node loss: 0.0200\n",
      "Epoch: 809, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 810, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 811, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 812, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 813, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 814, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 815, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 816, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 817, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 818, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 819, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 820, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 821, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 822, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 823, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 824, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 825, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 826, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 827, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 828, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 829, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 830, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 831, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 832, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 833, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 834, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 835, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 836, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 837, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 838, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 839, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0204\n",
      "Epoch: 840, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 841, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 842, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 843, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 844, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 845, total loss: 0.0527, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 846, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 847, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 848, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 849, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 850, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 851, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 852, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 853, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 854, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 855, total loss: 0.0528, edge loss: 0.0298, node loss: 0.0230\n",
      "Epoch: 856, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 857, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0226\n",
      "Epoch: 858, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 859, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 860, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 861, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 862, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 863, total loss: 0.0530, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 864, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 865, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 866, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 867, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 868, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 869, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 870, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 871, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 872, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 873, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 874, total loss: 0.0496, edge loss: 0.0297, node loss: 0.0199\n",
      "Epoch: 875, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0232\n",
      "Epoch: 876, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 877, total loss: 0.0529, edge loss: 0.0297, node loss: 0.0233\n",
      "Epoch: 878, total loss: 0.0499, edge loss: 0.0297, node loss: 0.0201\n",
      "Epoch: 879, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 880, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 881, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 882, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 883, total loss: 0.0536, edge loss: 0.0297, node loss: 0.0239\n",
      "Epoch: 884, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 885, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 886, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 887, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 888, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 889, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 890, total loss: 0.0535, edge loss: 0.0297, node loss: 0.0238\n",
      "Epoch: 891, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 892, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 893, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 894, total loss: 0.0495, edge loss: 0.0297, node loss: 0.0198\n",
      "Epoch: 895, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0231\n",
      "Epoch: 896, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 897, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 898, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 899, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 900, total loss: 0.0528, edge loss: 0.0297, node loss: 0.0230\n",
      "Epoch: 901, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 902, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 903, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 904, total loss: 0.0495, edge loss: 0.0297, node loss: 0.0198\n",
      "Epoch: 905, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0204\n",
      "Epoch: 906, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 907, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 908, total loss: 0.0541, edge loss: 0.0297, node loss: 0.0244\n",
      "Epoch: 909, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 910, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 911, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 912, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 913, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 914, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 915, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 916, total loss: 0.0498, edge loss: 0.0297, node loss: 0.0201\n",
      "Epoch: 917, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 918, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 919, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 920, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 921, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 922, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 923, total loss: 0.0522, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 924, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 925, total loss: 0.0518, edge loss: 0.0297, node loss: 0.0221\n",
      "Epoch: 926, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 927, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 928, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 929, total loss: 0.0519, edge loss: 0.0297, node loss: 0.0222\n",
      "Epoch: 930, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 931, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 932, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 933, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 934, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 935, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 936, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 937, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0204\n",
      "Epoch: 938, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 939, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 940, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 941, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 942, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 943, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 944, total loss: 0.0494, edge loss: 0.0297, node loss: 0.0197\n",
      "Epoch: 945, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 946, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 947, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 948, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0217\n",
      "Epoch: 949, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 950, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 951, total loss: 0.0498, edge loss: 0.0297, node loss: 0.0200\n",
      "Epoch: 952, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 953, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 954, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 955, total loss: 0.0525, edge loss: 0.0297, node loss: 0.0228\n",
      "Epoch: 956, total loss: 0.0499, edge loss: 0.0297, node loss: 0.0202\n",
      "Epoch: 957, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 958, total loss: 0.0501, edge loss: 0.0297, node loss: 0.0204\n",
      "Epoch: 959, total loss: 0.0511, edge loss: 0.0297, node loss: 0.0214\n",
      "Epoch: 960, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 961, total loss: 0.0504, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 962, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0216\n",
      "Epoch: 963, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 964, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 965, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 966, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 967, total loss: 0.0500, edge loss: 0.0297, node loss: 0.0203\n",
      "Epoch: 968, total loss: 0.0523, edge loss: 0.0297, node loss: 0.0225\n",
      "Epoch: 969, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 970, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 971, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 972, total loss: 0.0507, edge loss: 0.0297, node loss: 0.0210\n",
      "Epoch: 973, total loss: 0.0508, edge loss: 0.0297, node loss: 0.0211\n",
      "Epoch: 974, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 975, total loss: 0.0497, edge loss: 0.0297, node loss: 0.0200\n",
      "Epoch: 976, total loss: 0.0521, edge loss: 0.0297, node loss: 0.0224\n",
      "Epoch: 977, total loss: 0.0517, edge loss: 0.0297, node loss: 0.0220\n",
      "Epoch: 978, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 979, total loss: 0.0512, edge loss: 0.0297, node loss: 0.0215\n",
      "Epoch: 980, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 981, total loss: 0.0510, edge loss: 0.0297, node loss: 0.0213\n",
      "Epoch: 982, total loss: 0.0524, edge loss: 0.0297, node loss: 0.0227\n",
      "Epoch: 983, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 984, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 985, total loss: 0.0509, edge loss: 0.0297, node loss: 0.0212\n",
      "Epoch: 986, total loss: 0.0514, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 987, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 988, total loss: 0.0506, edge loss: 0.0297, node loss: 0.0209\n",
      "Epoch: 989, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0207\n",
      "Epoch: 990, total loss: 0.0516, edge loss: 0.0297, node loss: 0.0219\n",
      "Epoch: 991, total loss: 0.0515, edge loss: 0.0297, node loss: 0.0218\n",
      "Epoch: 992, total loss: 0.0503, edge loss: 0.0297, node loss: 0.0206\n",
      "Epoch: 993, total loss: 0.0498, edge loss: 0.0297, node loss: 0.0201\n",
      "Epoch: 994, total loss: 0.0499, edge loss: 0.0297, node loss: 0.0202\n",
      "Epoch: 995, total loss: 0.0520, edge loss: 0.0297, node loss: 0.0223\n",
      "Epoch: 996, total loss: 0.0526, edge loss: 0.0297, node loss: 0.0229\n",
      "Epoch: 997, total loss: 0.0502, edge loss: 0.0297, node loss: 0.0205\n",
      "Epoch: 998, total loss: 0.0505, edge loss: 0.0297, node loss: 0.0208\n",
      "Epoch: 999, total loss: 0.0497, edge loss: 0.0297, node loss: 0.0200\n",
      "Epoch: 1000, total loss: 0.0513, edge loss: 0.0297, node loss: 0.0215\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize early stopping\n",
    "node_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=node_model_name)\n",
    "edge_early_stopping = EarlyStopping(patience=patience, delta=delta, model_name=edge_model_name)\n",
    "\n",
    "# Training loop\n",
    "total_train_losses = []\n",
    "edge_train_losses  = []\n",
    "node_train_losses  = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    total_loss_cum = 0\n",
    "    edge_loss_cum  = 0\n",
    "    node_loss_cum  = 0\n",
    "    for batch_0 in train_loader:\n",
    "        #print()\n",
    "        # Clone batch of graphs\n",
    "        g_batch_0 = batch_0.clone()\n",
    "        \n",
    "        # Move batch data to GPU\n",
    "        g_batch_0 = g_batch_0.to(device)\n",
    "        \n",
    "        # Read number of graphs in batch\n",
    "        batch_size_0 = g_batch_0.num_graphs\n",
    "\n",
    "        # Save graph-level embeddings\n",
    "        embedding_batch_0 = []\n",
    "        for idx in range(batch_size_0):\n",
    "            embedding_batch_0.append(g_batch_0[idx].y.detach().to(device))\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        t_steps = np.arange(1, n_t_steps+1)\n",
    "        for t_step in t_steps:\n",
    "            # Read time step, which is added to node-level graph embeddings\n",
    "            t_step_std = torch.tensor([t_step / n_t_steps - 0.5], dtype=torch.float).to(device)  # Standard normalization\n",
    "        \n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            g_batch_t = []\n",
    "            e_batch_t = []\n",
    "            for idx in range(batch_size_0):\n",
    "                # Perform a diffusion step at time step t_step for each graph within the batch\n",
    "                graph_t, epsilon_t = diffusion_step(g_batch_0[idx], t_step, n_t_steps, alpha_decay)\n",
    "                \n",
    "                # Append noisy graphs and noises\n",
    "                g_batch_t.append(graph_t)\n",
    "                e_batch_t.append(epsilon_t)\n",
    "        \n",
    "                # Update diffused graph as next one\n",
    "                g_batch_0[idx] = graph_t.clone()\n",
    "            \n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add embeddings to noisy graphs (t_step information and graph-level embeddings)\n",
    "            for idx in range(batch_size_0):\n",
    "                # Add graph-level embedding to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       embedding_batch_0[idx])  # To match graph.y shape\n",
    "        \n",
    "                # Add t_step information to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       t_step_std)  # To match graph.y shape, which is 1D\n",
    "        \n",
    "            # Generate batch objects\n",
    "            g_batch_t = Batch.from_data_list(g_batch_t)\n",
    "            e_batch_t = Batch.from_data_list(e_batch_t)\n",
    "            \n",
    "            # Move data to device\n",
    "            g_batch_t = g_batch_t.to(device)\n",
    "            e_batch_t = e_batch_t.to(device)\n",
    "            \n",
    "            # Predict batch noise at given time step\n",
    "            pred_epsilon_t = predict_noise(g_batch_t, node_model, edge_model)\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the loss for node features and edge attributes\n",
    "            node_loss, edge_loss = get_graph_losses(e_batch_t, pred_epsilon_t, batch_size_0)\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            if not node_early_stopping.early_stop:\n",
    "                node_loss.backward(retain_graph=True)\n",
    "                node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            if not edge_early_stopping.early_stop:\n",
    "                edge_loss.backward(retain_graph=True)\n",
    "                edge_optimizer.step()\n",
    "\n",
    "            # Accumulate the total training loss\n",
    "            loss = node_loss + edge_loss\n",
    "            \n",
    "            # Get items\n",
    "            total_loss_cum += loss.item()\n",
    "            node_loss_cum  += node_loss.item()\n",
    "            edge_loss_cum  += edge_loss.item()\n",
    "\n",
    "            del g_batch_t, e_batch_t, pred_epsilon_t, node_loss, edge_loss  # Free up CUDA memory\n",
    "\n",
    "    # Compute the average train loss over n_t_steps\n",
    "    total_loss_cum /= n_t_steps\n",
    "    node_loss_cum  /= n_t_steps\n",
    "    edge_loss_cum  /= n_t_steps\n",
    "    \n",
    "    # Append average losses\n",
    "    total_train_losses.append(total_loss_cum)\n",
    "    node_train_losses.append(node_loss_cum)\n",
    "    edge_train_losses.append(edge_loss_cum)\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    node_early_stopping(node_loss_cum, node_model)\n",
    "    edge_early_stopping(edge_loss_cum, edge_model)\n",
    "\n",
    "    if node_early_stopping.early_stop and edge_early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, total loss: {total_loss_cum:.4f}, edge loss: {edge_loss_cum:.4f}, node loss: {node_loss_cum:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:58:50.344731953Z",
     "start_time": "2024-04-02T09:54:57.354466747Z"
    }
   },
   "id": "5f6c3be11e55a08c",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log(total_train_losses), label='Total')\n",
    "plt.plot(np.log(edge_train_losses),  label='Edge')\n",
    "plt.plot(np.log(node_train_losses),  label='Node')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:58:50.372632504Z",
     "start_time": "2024-04-03T02:58:50.344641062Z"
    }
   },
   "id": "921aa5cf0aef59a3",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeebf15173f65e29"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, total loss: 0.0013, edge loss: 0.0007, node loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "total_test_losses = 0\n",
    "edge_test_losses  = 0\n",
    "node_test_losses  = 0\n",
    "idx = 0\n",
    "for batch_0 in test_loader:\n",
    "    # Move batch data to GPU\n",
    "    batch_0 = batch_0.to(device)\n",
    "    \n",
    "    # Read number of graphs in batch\n",
    "    batch_size = batch_0.num_graphs\n",
    "    \n",
    "    # Diffuse batch\n",
    "    g_batch_t = diffuse(batch_0, n_t_steps, s=alpha_decay)\n",
    "    \n",
    "    # Denoise batch\n",
    "    g_batch_0 = denoise(g_batch_t, n_t_steps, node_model, edge_model, n_graph_features,\n",
    "                        s=alpha_decay, sigma=sigma)\n",
    "    \n",
    "    # Calculate the loss for node features and edge attributes\n",
    "    node_loss, edge_loss = get_graph_losses(batch_0, g_batch_0, batch_size)\n",
    "    \n",
    "    # Accumulate the total training loss\n",
    "    loss = node_loss + edge_loss\n",
    "    \n",
    "    # Get items\n",
    "    total_loss_cum = loss.item()\n",
    "    edge_loss_cum  = edge_loss.item()\n",
    "    node_loss_cum  = node_loss.item()\n",
    "    \n",
    "    # Append average losses\n",
    "    total_test_losses += total_loss_cum\n",
    "    edge_test_losses  += edge_loss_cum\n",
    "    node_test_losses  += node_loss_cum\n",
    "    \n",
    "    print(f'Batch: {idx}, total loss: {total_loss_cum:.4f}, edge loss: {edge_loss_cum:.4f}, node loss: {node_loss_cum:.4f}')\n",
    "    idx += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:58:55.487213025Z",
     "start_time": "2024-04-03T02:58:50.374185643Z"
    }
   },
   "id": "2fe8468fc69b364a",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0012733193580061197, 0.0006530781392939389, 0.0006202412187121809)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_losses, edge_test_losses, node_test_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:58:55.488405413Z",
     "start_time": "2024-04-03T02:58:55.486496484Z"
    }
   },
   "id": "d196836e36b58174",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dde0d0d42345dcb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create and save as a dictionary\n",
    "model_performance = {\n",
    "    'total_train_losses': total_train_losses,\n",
    "    'edge_train_losses':  edge_train_losses,\n",
    "    'node_train_losses':  node_train_losses,\n",
    "    'total_test_losses':  total_test_losses,\n",
    "    'edge_test_losses':   edge_test_losses,\n",
    "    'node_test_losses':   node_test_losses\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_performance.json', 'w') as json_file:\n",
    "    json.dump(model_performance, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:58:55.491898901Z",
     "start_time": "2024-04-03T02:58:55.488496916Z"
    }
   },
   "id": "a348bc7a734b98db",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
