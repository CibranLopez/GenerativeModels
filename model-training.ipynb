{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:30.757682330Z",
     "start_time": "2024-02-27T16:19:29.556093830Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy             as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from torch_geometric.data   import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph, predict_noise, diffuse, denoise\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:30.765917536Z",
     "start_time": "2024-02-27T16:19:30.762136832Z"
    }
   },
   "id": "74450972cebeaa56",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ad446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:30.766414722Z",
     "start_time": "2024-02-27T16:19:30.765136136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs\n",
    "# The models is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property\n",
    "# Although denoising includes noise, I think it is better not to add it when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a85832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:30.937566288Z",
     "start_time": "2024-02-27T16:19:30.767062561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'GM_BiSI'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder   = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "# Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "i = 0\n",
    "while True:\n",
    "    target_folder = f'{general_folder}/GM_v{i}'\n",
    "    if not os.path.exists(target_folder):\n",
    "        # Copy all data\n",
    "        os.system(f'cp -r {data_folder} {target_folder}')\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75aa6001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:30.940568137Z",
     "start_time": "2024-02-27T16:19:30.937012937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 3000\n",
    "batch_size    = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 100\n",
    "\n",
    "# Amount of noise for the generative process\n",
    "sigma = 0  # Zero for training purposes\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':        data_folder,\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'sigma':              sigma,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Load of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the dataset, already standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96173a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:31.760687854Z",
     "start_time": "2024-02-27T16:19:30.940839327Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_name_std      = f'{target_folder}/train_dataset.pt'\n",
    "test_dataset_name_std       = f'{target_folder}/test_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset, with corresponding labels and parameters\n",
    "train_dataset = torch.load(train_dataset_name_std)\n",
    "test_dataset  = torch.load(test_dataset_name_std)\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert NumPy arrays back to PyTorch tensors\n",
    "dataset_parameters = {key: torch.tensor(value) for key, value in numpy_dict.items()}\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i in range(500):\n",
    "    test_dataset.append(train_dataset[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:31.774131433Z",
     "start_time": "2024-02-27T16:19:31.760846703Z"
    }
   },
   "id": "70945317432384ff",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:31.776932704Z",
     "start_time": "2024-02-27T16:19:31.775140942Z"
    }
   },
   "id": "86402c04-4b6c-4dfb-b112-6574fc89380d",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definition of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28881dd9d41fdf8e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1591eccef168173b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:32.340557029Z",
     "start_time": "2024-02-27T16:19:31.777657297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(6, 256)\n",
      "  (conv2): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(train_dataset[0].y)\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "\n",
    "# Moving models to device\n",
    "node_model = node_model.to(device)\n",
    "edge_model = edge_model.to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "    \n",
    "    # Evaluate model state\n",
    "    node_model.eval()\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training of the model"
   ],
   "metadata": {},
   "id": "31a76fc0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loss factor for normalization\n",
    "loss_factor = len(train_dataset) * n_t_steps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:32.342918493Z",
     "start_time": "2024-02-27T16:19:32.341563793Z"
    }
   },
   "id": "f22a15487c86d8c6",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_train_losses = []\n",
    "edge_train_losses  = []\n",
    "node_train_losses  = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    total_loss_cum = 0\n",
    "    edge_loss_cum  = 0\n",
    "    node_loss_cum  = 0\n",
    "    for batch_0 in train_loader:\n",
    "        #print()\n",
    "        # Clone batch of graphs\n",
    "        g_batch_0 = batch_0.clone()\n",
    "        \n",
    "        # Move batch data to GPU\n",
    "        g_batch_0 = g_batch_0.to(device)\n",
    "        \n",
    "        # Read number of graphs in batch\n",
    "        batch_size_0 = g_batch_0.num_graphs\n",
    "        \n",
    "        # Save graph-level embeddings\n",
    "        embedding_batch_0 = []\n",
    "        for idx in range(batch_size_0):\n",
    "            embedding_batch_0.append(g_batch_0[idx].y.detach().to(device))\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        t_steps = np.arange(1, n_t_steps+1)\n",
    "        for t_step in t_steps:\n",
    "            # Read time step, which is added to node-level graph embeddings\n",
    "            t_step_std = torch.tensor([t_step / n_t_steps - 0.5], dtype=torch.float).to(device)  # Standard normalization\n",
    "        \n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            g_batch_t = []\n",
    "            e_batch_t = []\n",
    "            for idx in range(batch_size_0):\n",
    "                # Perform a diffusion step at time step t_step for each graph within the batch\n",
    "                graph_t, epsilon_t = diffusion_step(g_batch_0[idx], t_step, n_t_steps, alpha_decay)\n",
    "                \n",
    "                # Append noisy graphs and noises\n",
    "                g_batch_t.append(graph_t)\n",
    "                e_batch_t.append(epsilon_t)\n",
    "        \n",
    "                # Update diffused graph as next one\n",
    "                g_batch_0[idx] = graph_t.clone()\n",
    "            \n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add embeddings to noisy graphs (t_step information and graph-level embeddings)\n",
    "            for idx in range(batch_size_0):\n",
    "                # Add graph-level embedding to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       embedding_batch_0[idx])  # To match graph.y shape\n",
    "        \n",
    "                # Add t_step information to graph_t as node embeddings\n",
    "                g_batch_t[idx] = add_features_to_graph(g_batch_t[idx],\n",
    "                                                       t_step_std)  # To match graph.y shape, which is 1D\n",
    "        \n",
    "            # Generate batch objects\n",
    "            g_batch_t = Batch.from_data_list(g_batch_t)\n",
    "            e_batch_t = Batch.from_data_list(e_batch_t)\n",
    "            \n",
    "            # Move data to device\n",
    "            g_batch_t = g_batch_t.to(device)\n",
    "            e_batch_t = e_batch_t.to(device)\n",
    "            \n",
    "            # Predict batch noise at given time step\n",
    "            pred_epsilon_t = predict_noise(g_batch_t, node_model, edge_model)\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the loss for node features and edge attributes\n",
    "            node_loss, edge_loss = get_graph_losses(e_batch_t, pred_epsilon_t, batch_size_0)\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            node_loss.backward(retain_graph=True)\n",
    "            node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            edge_loss.backward(retain_graph=True)\n",
    "            edge_optimizer.step()\n",
    "\n",
    "            # Accumulate the total training loss\n",
    "            loss = node_loss + edge_loss\n",
    "            \n",
    "            # Get items\n",
    "            total_loss_cum += loss.item()\n",
    "            edge_loss_cum  += edge_loss.item()\n",
    "            node_loss_cum  += node_loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    total_loss_cum /= loss_factor\n",
    "    edge_loss_cum  /= loss_factor\n",
    "    node_loss_cum  /= loss_factor\n",
    "    \n",
    "    # Append average losses\n",
    "    total_train_losses.append(total_loss_cum)\n",
    "    edge_train_losses.append(edge_loss_cum)\n",
    "    node_train_losses.append(node_loss_cum)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, total loss: {total_loss_cum:.4f}, edge loss: {edge_loss_cum:.4f}, node loss: {node_loss_cum:.4f}')\n",
    "    \n",
    "    # Save some checkpoints\n",
    "    if (epoch % 20) == 0:\n",
    "        torch.save(node_model.state_dict(), node_model_name)\n",
    "        torch.save(edge_model.state_dict(), edge_model_name)\n",
    "\n",
    "torch.save(node_model.state_dict(), node_model_name)\n",
    "torch.save(edge_model.state_dict(), edge_model_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357e5c8aa11c1c20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "plt.plot(np.log(total_train_losses), label='Total')\n",
    "plt.plot(np.log(edge_train_losses),  label='Edge')\n",
    "plt.plot(np.log(node_train_losses),  label='Node')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3364deaff7c2059"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeebf15173f65e29"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[2304, 4], edge_index=[2, 30679], edge_attr=[30679], y=[32], batch=[2304], ptr=[33]) DataBatch(x=[2304, 4], edge_index=[2, 30679], edge_attr=[30679], y=[32], batch=[2304], ptr=[33])\n",
      "Batch: 0, total loss: 0.0000, edge loss: 0.0000, node loss: 0.0000\n",
      "DataBatch(x=[2304, 4], edge_index=[2, 30452], edge_attr=[30452], y=[32], batch=[2304], ptr=[33]) DataBatch(x=[2304, 4], edge_index=[2, 30452], edge_attr=[30452], y=[32], batch=[2304], ptr=[33])\n",
      "Batch: 1, total loss: 0.0000, edge loss: 0.0000, node loss: 0.0000\n",
      "DataBatch(x=[2304, 4], edge_index=[2, 30599], edge_attr=[30599], y=[32], batch=[2304], ptr=[33]) DataBatch(x=[2304, 4], edge_index=[2, 30599], edge_attr=[30599], y=[32], batch=[2304], ptr=[33])\n",
      "Batch: 2, total loss: 0.0000, edge loss: 0.0000, node loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m g_batch_t, _ \u001B[38;5;241m=\u001B[39m diffuse(g_batch_0, n_t_steps, s\u001B[38;5;241m=\u001B[39malpha_decay)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Denoise batch\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m g_batch_0, _ \u001B[38;5;241m=\u001B[39m \u001B[43mdenoise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg_batch_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_t_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha_decay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msigma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Calculate the loss for node features and edge attributes\u001B[39;00m\n\u001B[1;32m     23\u001B[0m node_loss, edge_loss \u001B[38;5;241m=\u001B[39m get_graph_losses(g_batch_t, g_batch_0, batch_size)\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/libraries/model.py:230\u001B[0m, in \u001B[0;36mdenoise\u001B[0;34m(batch_t, n_t_steps, node_model, edge_model, s, sigma, plot_steps)\u001B[0m\n\u001B[1;32m    227\u001B[0m g_batch_0 \u001B[38;5;241m=\u001B[39m g_batch_0\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    229\u001B[0m \u001B[38;5;66;03m# Predict batch noise at given time step\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m pred_epsilon_t \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_noise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg_batch_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;66;03m# Check if intermediate steps are plotted; then, plot the NetworkX graph\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m plot_steps:\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;66;03m# Convert PyTorch graph to NetworkX graph\u001B[39;00m\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/libraries/model.py:300\u001B[0m, in \u001B[0;36mpredict_noise\u001B[0;34m(g_batch_t, node_model, edge_model)\u001B[0m\n\u001B[1;32m    296\u001B[0m idx_x_f    \u001B[38;5;241m=\u001B[39m idx_x_i    \u001B[38;5;241m+\u001B[39m g_batch_t[idx]\u001B[38;5;241m.\u001B[39mnum_nodes\n\u001B[1;32m    297\u001B[0m idx_attr_f \u001B[38;5;241m=\u001B[39m idx_attr_i \u001B[38;5;241m+\u001B[39m g_batch_t[idx]\u001B[38;5;241m.\u001B[39mnum_edges\n\u001B[1;32m    299\u001B[0m temp_pred \u001B[38;5;241m=\u001B[39m Data(x\u001B[38;5;241m=\u001B[39mout_x[idx_x_i:idx_x_f],\n\u001B[0;32m--> 300\u001B[0m                  edge_index\u001B[38;5;241m=\u001B[39m\u001B[43mg_batch_t\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39medge_index,\n\u001B[1;32m    301\u001B[0m                  edge_attr\u001B[38;5;241m=\u001B[39mout_attr[idx_attr_i:idx_attr_f])\n\u001B[1;32m    303\u001B[0m \u001B[38;5;66;03m# Update indexes\u001B[39;00m\n\u001B[1;32m    304\u001B[0m idx_x_i \u001B[38;5;241m=\u001B[39m idx_x_f\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch_geometric/data/batch.py:154\u001B[0m, in \u001B[0;36mBatch.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx: Union[\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39minteger, \u001B[38;5;28mstr\u001B[39m, IndexType]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, (\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39minteger))\n\u001B[1;32m    152\u001B[0m             \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, Tensor) \u001B[38;5;129;01mand\u001B[39;00m idx\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    153\u001B[0m             \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(idx))):\n\u001B[0;32m--> 154\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_example\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mtuple\u001B[39m)\n\u001B[1;32m    156\u001B[0m                                   \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m)):\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;66;03m# Accessing attributes or node/edge types:\u001B[39;00m\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch_geometric/data/batch.py:103\u001B[0m, in \u001B[0;36mBatch.get_example\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_slice_dict\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    100\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot reconstruct \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object from \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatch\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m because \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatch\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m was not created via \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatch.from_data_list()\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 103\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mseparate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__bases__\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mslice_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_slice_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43minc_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inc_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecrement\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch_geometric/data/separate.py:37\u001B[0m, in \u001B[0;36mseparate\u001B[0;34m(cls, batch, idx, slice_dict, inc_dict, decrement)\u001B[0m\n\u001B[1;32m     35\u001B[0m         slices \u001B[38;5;241m=\u001B[39m slice_dict[attr]\n\u001B[1;32m     36\u001B[0m         incs \u001B[38;5;241m=\u001B[39m inc_dict[attr] \u001B[38;5;28;01mif\u001B[39;00m decrement \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m     data_store[attr] \u001B[38;5;241m=\u001B[39m \u001B[43m_separate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_store\u001B[49m\u001B[43m[\u001B[49m\u001B[43mattr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mincs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_store\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecrement\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we cannot infer\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# the real number of nodes from the total number of nodes alone:\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(batch_store, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_num_nodes\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch_geometric/data/separate.py:67\u001B[0m, in \u001B[0;36m_separate\u001B[0;34m(key, value, idx, slices, incs, batch, store, decrement)\u001B[0m\n\u001B[1;32m     65\u001B[0m value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mnarrow(cat_dim \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m, start, end \u001B[38;5;241m-\u001B[39m start)\n\u001B[1;32m     66\u001B[0m value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m cat_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m value\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decrement \u001B[38;5;129;01mand\u001B[39;00m (incs\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mincs\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m     68\u001B[0m     value \u001B[38;5;241m=\u001B[39m value \u001B[38;5;241m-\u001B[39m incs[idx]\u001B[38;5;241m.\u001B[39mto(value\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "total_test_losses = []\n",
    "edge_test_losses  = []\n",
    "node_test_losses  = []\n",
    "idx = 0\n",
    "for batch_0 in test_loader:\n",
    "    # Clone batch of graphs\n",
    "    g_batch_0 = batch_0.clone()\n",
    "    \n",
    "    # Move batch data to GPU\n",
    "    g_batch_0 = g_batch_0.to(device)\n",
    "    \n",
    "    # Read number of graphs in batch\n",
    "    batch_size = g_batch_0.num_graphs\n",
    "    \n",
    "    # Diffuse batch\n",
    "    g_batch_t, _ = diffuse(g_batch_0, n_t_steps, s=alpha_decay)\n",
    "    \n",
    "    # Denoise batch\n",
    "    g_batch_0, _ = denoise(g_batch_t, n_t_steps, node_model, edge_model, s=alpha_decay, sigma=sigma)\n",
    "    \n",
    "    # Calculate the loss for node features and edge attributes\n",
    "    node_loss, edge_loss = get_graph_losses(g_batch_t, g_batch_0, batch_size)\n",
    "    print(g_batch_t, g_batch_0)\n",
    "    \n",
    "    # Accumulate the total training loss\n",
    "    loss = node_loss + edge_loss\n",
    "    \n",
    "    # Get items\n",
    "    total_loss_cum = loss.item()\n",
    "    edge_loss_cum  = edge_loss.item()\n",
    "    node_loss_cum  = node_loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    total_loss_cum /= loss_factor\n",
    "    edge_loss_cum  /= loss_factor\n",
    "    node_loss_cum  /= loss_factor\n",
    "    \n",
    "    # Append average losses\n",
    "    total_test_losses.append(total_loss_cum)\n",
    "    edge_test_losses.append(edge_loss_cum)\n",
    "    node_test_losses.append(node_loss_cum)\n",
    "    \n",
    "    print(f'Batch: {idx}, total loss: {total_loss_cum:.4f}, edge loss: {edge_loss_cum:.4f}, node loss: {node_loss_cum:.4f}')\n",
    "    idx += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:44.502762186Z",
     "start_time": "2024-02-27T16:19:32.344538922Z"
    }
   },
   "id": "2fe8468fc69b364a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(np.log(total_test_losses), label='Total')\n",
    "plt.plot(np.log(edge_test_losses),  label='Edge')\n",
    "plt.plot(np.log(node_test_losses),  label='Node')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T16:19:44.506544212Z",
     "start_time": "2024-02-27T16:19:44.503370641Z"
    }
   },
   "id": "d196836e36b58174"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
