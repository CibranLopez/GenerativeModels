{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T15:25:43.556635556Z",
     "start_time": "2024-02-09T15:25:41.022476838Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from libraries.model        import nGCNN, eGCNN, diffusion_step, get_graph_losses, add_features_to_graph\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686ad446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T15:28:12.091054143Z",
     "start_time": "2024-02-09T15:28:12.043669208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on adding and removing noise to graphs\n",
    "# The models is able to learn hidden patterns\n",
    "# It can be conditionally trained with respect to some target property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a85832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T15:28:12.745495800Z",
     "start_time": "2024-02-09T15:28:12.477735022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define name of data folder where reference dataset are contained\n",
    "# It shall be consistent with data_folder and data will be moved to models folder\n",
    "data_name = 'GM_BiSI'\n",
    "\n",
    "# Define folder in which data is stored\n",
    "data_folder   = f'data/{data_name}'\n",
    "\n",
    "# The folder is named as target_folder_vi (eg, target_folder_v0)\n",
    "general_folder = f'models/{data_name}'\n",
    "if not os.path.exists(general_folder):\n",
    "    # Generate new folder\n",
    "    os.system(f'mkdir {general_folder}')\n",
    "\n",
    "# Each new run generates a new folder, with different generations and training most likely (as data might vary as well)\n",
    "i = 0\n",
    "while True:\n",
    "    target_folder = f'{general_folder}/GM_v{i}'\n",
    "    if not os.path.exists(target_folder):\n",
    "        # Copy all data\n",
    "        os.system(f'cp -r {data_folder} {target_folder}')\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "edge_model_name = f'{target_folder}/edge_model.pt'\n",
    "node_model_name = f'{target_folder}/node_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75aa6001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T15:28:13.648458979Z",
     "start_time": "2024-02-09T15:28:13.630547571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Machine-learning parameters\n",
    "n_epochs      = 3000\n",
    "batch_size    = 1  # 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Number of diffusing and denoising steps\n",
    "n_t_steps = 5\n",
    "\n",
    "# Decay of parameter alpha\n",
    "noise_contribution = 0.05\n",
    "alpha_decay = 0.5 * (1 - noise_contribution**2)\n",
    "\n",
    "# Dropouts for node and edge models (independent of each other)\n",
    "dropout_node = 0.2\n",
    "dropout_edge = 0.2\n",
    "\n",
    "# Create and save as a dictionary\n",
    "model_parameters = {\n",
    "    'data_folder':        data_folder,\n",
    "    'n_epochs':           n_epochs,\n",
    "    'batch_size':         batch_size,\n",
    "    'learning_rate':      learning_rate,\n",
    "    'n_t_steps':          n_t_steps,\n",
    "    'noise_contribution': noise_contribution,\n",
    "    'dropout_node':       dropout_node,\n",
    "    'dropout_edge':       dropout_edge\n",
    "}\n",
    "\n",
    "# Write the dictionary to the file in JSON format\n",
    "with open(f'{target_folder}/model_parameters.json', 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b2e8",
   "metadata": {},
   "source": [
    "# Load of graph database for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbe57",
   "metadata": {},
   "source": [
    "Load the dataset, already standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96173a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T15:28:22.087691510Z",
     "start_time": "2024-02-09T15:28:20.785917798Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_name_std      = f'{target_folder}/train_dataset.pt'\n",
    "test_dataset_name_std       = f'{target_folder}/test_dataset.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset, with corresponding labels and parameters\n",
    "train_dataset = torch.load(train_dataset_name_std)\n",
    "test_dataset  = torch.load(test_dataset_name_std)\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert NumPy arrays back to PyTorch tensors\n",
    "dataset_parameters = {key: torch.tensor(value) for key, value in numpy_dict.items()}\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a76fc0",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86402c04-4b6c-4dfb-b112-6574fc89380d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T15:28:32.897855146Z",
     "start_time": "2024-02-09T15:28:32.895336015Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "#test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1591eccef168173b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T15:28:56.111781837Z",
     "start_time": "2024-02-09T15:28:56.086565832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node GCNN:\n",
      "nGCNN(\n",
      "  (conv1): GraphConv(6, 256)\n",
      "  (conv2): GraphConv(256, 5)\n",
      ")\n",
      "\n",
      "Edge GCNN:\n",
      "eGCNN(\n",
      "  (linear1): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features + 1\n",
    "\n",
    "# Determine the number of graph-level features to be predicted\n",
    "n_graph_features = len(train_dataset[0].y)\n",
    "\n",
    "# Instantiate the models for nodes and edges\n",
    "node_model = nGCNN(n_node_features, n_graph_features, dropout_node).to(device)\n",
    "edge_model = eGCNN(n_node_features, n_graph_features, dropout_edge).to(device)\n",
    "\n",
    "# Moving models to device\n",
    "node_model = node_model.to(device)\n",
    "edge_model = edge_model.to(device)\n",
    "\n",
    "# Load previous model if available\n",
    "try:\n",
    "    # Load model state\n",
    "    node_model.load_state_dict(torch.load(node_model_name))\n",
    "    edge_model.load_state_dict(torch.load(edge_model_name))\n",
    "    \n",
    "    # Evaluate model state\n",
    "    node_model.eval()\n",
    "    edge_model.eval()\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('\\nNode GCNN:')\n",
    "print(node_model)\n",
    "print('\\nEdge GCNN:')\n",
    "print(edge_model)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 70\u001B[0m\n\u001B[1;32m     66\u001B[0m x_j \u001B[38;5;241m=\u001B[39m graph_t\u001B[38;5;241m.\u001B[39mx[graph_t\u001B[38;5;241m.\u001B[39medge_index[\u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Perform a single forward pass for predicting edge attributes\u001B[39;00m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Introduce previous edge attributes as features as well\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m out_attr \u001B[38;5;241m=\u001B[39m \u001B[43medge_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_j\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Moving data to device\u001B[39;00m\n\u001B[1;32m     73\u001B[0m out_x    \u001B[38;5;241m=\u001B[39m    out_x\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/libraries/model.py:329\u001B[0m, in \u001B[0;36meGCNN.forward\u001B[0;34m(self, x_i, x_j, previous_attr)\u001B[0m\n\u001B[1;32m    326\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x_ij, previous_attr), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    328\u001B[0m \u001B[38;5;66;03m# Apply linear convolution with ReLU activation function\u001B[39;00m\n\u001B[0;32m--> 329\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# Dropout layer (only for training)\u001B[39;00m\n\u001B[1;32m    332\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(x, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m~/cibran/Work/UPC/GenerativeModels/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1598\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_backward_pre_hooks\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m:\n\u001B[1;32m   1599\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[0;32m-> 1601\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tensor, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModule\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m:\n\u001B[1;32m   1603\u001B[0m         _parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Loss factor for normalization\n",
    "loss_factor = len(train_dataset) * n_t_steps\n",
    "\n",
    "# Initialize the optimizers\n",
    "node_optimizer = torch.optim.Adam(node_model.parameters(), lr=learning_rate)\n",
    "edge_optimizer = torch.optim.Adam(edge_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_losses = []\n",
    "edge_losses  = []\n",
    "node_losses  = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize train loss variable\n",
    "    total_loss_cum = 0\n",
    "    edge_loss_cum  = 0\n",
    "    node_loss_cum  = 0\n",
    "    for graph in train_dataset:  # train_loader\n",
    "        #print()\n",
    "        # Clone existing graph\n",
    "        graph_0 = graph.clone()\n",
    "        \n",
    "        # Save graph-level embedding\n",
    "        graph_embedding = graph_0.y.detach()\n",
    "        \n",
    "        # Initialize the gradient of the optimizers\n",
    "        node_optimizer.zero_grad()\n",
    "        edge_optimizer.zero_grad()\n",
    "        \n",
    "        # Start denoising-diffusing process\n",
    "        for t_step in np.arange(1, n_t_steps+1):\n",
    "            # Diffuse the graph with some noise\n",
    "            #print()\n",
    "            #print(f'Step: {t_step}')\n",
    "            #print('Diffusing...')\n",
    "            \n",
    "            graph_t, epsilon_t = diffusion_step(graph_0, t_step, n_t_steps, alpha_decay)\n",
    "            \n",
    "            # Update diffused graph as next one\n",
    "            graph_0 = graph_t.clone()\n",
    "\n",
    "            # Denoise the diffused graph\n",
    "            #print(f'Denoising...')\n",
    "            \n",
    "            # Add graph-level embedding to graph_t as node embeddings\n",
    "            graph_t = add_features_to_graph(graph_t,\n",
    "                                            graph_embedding)  # To match graph.y shape\n",
    "            \n",
    "            # Add t_step information to graph_t as node embeddings\n",
    "            t_step_std = torch.tensor([t_step/n_t_steps - 0.5], dtype=torch.float)  # Standard normalization\n",
    "            graph_t = add_features_to_graph(graph_t,\n",
    "                                            t_step_std)  # To match graph.y shape, which is 1D\n",
    "            \n",
    "            # Moving data to device\n",
    "            graph_t = graph_t.to(device)\n",
    "            \n",
    "            # Perform a single forward pass for predicting node features\n",
    "            out_x = node_model(graph_t.x,\n",
    "                               graph_t.edge_index,\n",
    "                               graph_t.edge_attr)\n",
    "            \n",
    "            # Remove t_step information\n",
    "            out_x = out_x[:, :-1]\n",
    "\n",
    "            # Define x_i and x_j as features of every corresponding pair of nodes (same order than attributes)\n",
    "            x_i = graph_t.x[graph_t.edge_index[0]]\n",
    "            x_j = graph_t.x[graph_t.edge_index[1]]\n",
    "\n",
    "            # Perform a single forward pass for predicting edge attributes\n",
    "            # Introduce previous edge attributes as features as well\n",
    "            out_attr = edge_model(x_i, x_j, graph_t.edge_attr)\n",
    "\n",
    "            # Moving data to device\n",
    "            out_x    =    out_x.to(device)\n",
    "            out_attr = out_attr.to(device)\n",
    "            \n",
    "            # Construct noise graph with predicted node features and edge attributes, and previous edge indexes\n",
    "            pred_epsilon_t = Data(x=out_x,\n",
    "                                  edge_index=graph_t.edge_index,\n",
    "                                  edge_attr=out_attr.ravel())\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            #print('Backpropagating...')\n",
    "\n",
    "            # Calculate the loss for node features and edge attributes\n",
    "            node_loss, edge_loss = get_graph_losses(epsilon_t, pred_epsilon_t, batch_size)\n",
    "            \n",
    "            # Backpropagate and optimize node loss\n",
    "            node_loss.backward(retain_graph=True)\n",
    "            node_optimizer.step()\n",
    "\n",
    "            # Backpropagate and optimize edge loss\n",
    "            edge_loss.backward(retain_graph=True)\n",
    "            edge_optimizer.step()\n",
    "\n",
    "            # Accumulate the total training loss\n",
    "            loss = node_loss + edge_loss\n",
    "            \n",
    "            # Get items\n",
    "            total_loss_cum += loss.item()\n",
    "            edge_loss_cum  += edge_loss.item()\n",
    "            node_loss_cum  += node_loss.item()\n",
    "    \n",
    "    # Compute the average train loss\n",
    "    total_loss_cum /= loss_factor\n",
    "    edge_loss_cum  /= loss_factor\n",
    "    node_loss_cum  /= loss_factor\n",
    "    \n",
    "    # Append average losses\n",
    "    total_losses.append(total_loss_cum)\n",
    "    edge_losses.append(edge_loss_cum)\n",
    "    node_losses.append(node_loss_cum)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, total loss: {total_loss_cum:.4f}, edge loss: {edge_loss_cum:.4f}, node loss: {node_loss_cum:.4f}')\n",
    "    \n",
    "    # Save some checkpoints\n",
    "    if (epoch % 20) == 0:\n",
    "        torch.save(node_model.state_dict(), node_model_name)\n",
    "        torch.save(edge_model.state_dict(), edge_model_name)\n",
    "\n",
    "torch.save(node_model.state_dict(), node_model_name)\n",
    "torch.save(edge_model.state_dict(), edge_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T15:29:28.216877601Z",
     "start_time": "2024-02-09T15:29:17.666622548Z"
    }
   },
   "id": "518aeb7557ad53a3",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeffa13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-09T11:07:20.122676Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.log(total_losses), label='Total')\n",
    "plt.plot(np.log(edge_losses),  label='Edge')\n",
    "plt.plot(np.log(node_losses),  label='Node')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
